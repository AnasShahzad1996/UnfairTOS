{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------# Import libraries and datasets #------#\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import datasets as dts\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import gc\n",
    "import random\n",
    "import spacy\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "from transformers import BertModel,AutoModel\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac083e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dts.load_dataset('lex_glue','unfair_tos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33257be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.DataFrame.from_dict(dataset[\"train\"])\n",
    "val_dataset = pd.DataFrame.from_dict(dataset[\"validation\"])\n",
    "test_dataset = pd.DataFrame.from_dict(dataset[\"test\"])\n",
    "\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = {\"Limitation of liability\": \"this clause stipulates that the duty to pay damages is limited or excluded, for certain kind of losses, under certain conditions. \"\n",
    "               , \"Unilateral termination\": \"this clause gives provider the right to suspend and/or terminate the service and/or the contract, and sometimes details the circumstances under which the provider claims to have a right to do so.\"\n",
    "               , \"Unilateral change\": \"this clause specifies the conditions under which the service provider could amend and modify the terms of service and/or the service itself.\"\n",
    "               , \"Content removal\": \"this clause gives the provider a right to modify/delete userâ€™s content, including in-app purchases, and sometimes specifies the conditions under which the service provider may do so.\"\n",
    "               , \"Contract by using\": \"this clause stipulates that the consumer is bound by the terms of use of a specific service, simply by using the service, without even being required to mark that he or she has read and accepted them.\"\n",
    "               , \"Choice of law\": \"this clause specifies what law will govern the contract, meaning also what law will be applied in potential adjudication of a dispute arising under the contract.\"\n",
    "               , \"Jurisdiction\": \"this selection clause requires or allows the parties to resolve their disputes through an arbitration process, before the case could go to court.\"\n",
    "               , \"Arbitration\": \"this forum selection clause requires or allows the parties to resolve their disputes through an arbitration process, before the case could go to court however, such a clause may or may not specify that arbitration should occur within a specific jurisdiction. \"}\n",
    "label_to_def = {\n",
    "    0 : \"Limitation of liability\",\n",
    "    1 : \"Unilateral termination\",\n",
    "    2 : \"Unilateral change\",\n",
    "    3 : \"Content removal\",\n",
    "    4 : \"Contract by using\",\n",
    "    5 : \"Choice of law\",\n",
    "    6 : \"Jurisdiction\",\n",
    "    7 : \"Arbitration\"\n",
    "}\n",
    "\n",
    "entail_con = [\"entails  that\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_entailment(dataset,defs,lab2def,ent_con,remove_unseen):\n",
    "    new_dataset = {\"text\":[],\"labels\":[],\"str_labels\":[]}\n",
    "    num_text = len(dataset[\"text\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    max_len = 0\n",
    "    \n",
    "    iteri = 0\n",
    "    print (\"garmi garmi : \",num_text)\n",
    "    for i in range(0,num_text):\n",
    "        if remove_unseen :\n",
    "            if len(dataset[\"labels\"][i]) > 0:\n",
    "                iteri +=1\n",
    "                print (iteri,\"/\",num_text)\n",
    "                old_string = dataset[\"text\"][i]\n",
    "                random_number = random.randint(0, len(ent_con)) - 1\n",
    "                doc = nlp(old_string)\n",
    "                old_string = (\" \").join([\" \".join([token.text for token in sent if not token.is_punct]) for sent in doc.sents])[:-1]\n",
    "                for j in range(0,8):\n",
    "                    new_string = old_string + \" \" + ent_con[random_number] + \" \" + definitions[label_to_def[j]]\n",
    "                    new_dataset[\"text\"].append(new_string)\n",
    "                    new_dataset[\"str_labels\"].append(str(dataset[\"labels\"][i]))\n",
    "                    if j in dataset[\"labels\"][i]:\n",
    "                        new_dataset[\"labels\"].append([0,1])\n",
    "                    else:\n",
    "                        new_dataset[\"labels\"].append([1,0])\n",
    "\n",
    "                    doc2 = nlp(new_string)\n",
    "                    if len(doc2) > max_len :\n",
    "                        max_len = len(doc2)\n",
    "        else:\n",
    "            iteri +=1\n",
    "            print (iteri,\"/\",num_text)\n",
    "            print (i,\"/\",num_text)\n",
    "            old_string = dataset[\"text\"][i]\n",
    "            random_number = random.randint(0, len(ent_con)) - 1\n",
    "            doc = nlp(old_string)\n",
    "            old_string = (\" \").join([\" \".join([token.text for token in sent if not token.is_punct]) for sent in doc.sents])[:-1]\n",
    "            for j in range(0,8):\n",
    "                new_string = old_string + \" \" + ent_con[random_number] + \" \" + definitions[label_to_def[j]]\n",
    "                new_dataset[\"text\"].append(new_string)\n",
    "                new_dataset[\"str_labels\"].append(str(dataset[\"labels\"][i]))\n",
    "                if j in dataset[\"labels\"][i]:\n",
    "                    new_dataset[\"labels\"].append([0,1])\n",
    "                else:\n",
    "                    new_dataset[\"labels\"].append([1,0])\n",
    "\n",
    "                doc2 = nlp(new_string)\n",
    "                if len(doc2) > max_len :\n",
    "                    max_len = len(doc2)\n",
    "            \n",
    "    \n",
    "    return new_dataset,max_len\n",
    "\n",
    "train_dataset_ent, train_max = convert_to_entailment(train_dataset,definitions,label_to_def,entail_con,True)\n",
    "val_dataset_ent, val_max = convert_to_entailment(val_dataset,definitions,label_to_def,entail_con,False)\n",
    "test_dataset_ent, test_max = convert_to_entailment(test_dataset,definitions,label_to_def,entail_con,False)\n",
    "\n",
    "print (train_max, val_max, test_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset,num_classes,tokenizer):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.texts = self.dataset[\"text\"]\n",
    "        self.labels = self.dataset[\"labels\"]\n",
    "        self.num_classes = num_classes\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "    \n",
    "        multi_label = torch.tensor(label,dtype=torch.float)\n",
    "        #multi_label = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        #multi_label[label] = 1\n",
    "        \n",
    "        return {'input_ids':input_ids, 'attention_mask':attention_mask, 'multi_label':multi_label}\n",
    "    \n",
    "\n",
    "def list_it(curr):\n",
    "    return [curr]\n",
    "\n",
    "def delist_it(curr):\n",
    "    new_list = []\n",
    "    for i in curr:\n",
    "        new_list.append(i[0][0])\n",
    "    return new_list\n",
    "\n",
    "def str2list(curr):\n",
    "    if curr==\"[]\":\n",
    "        return []\n",
    "    else:\n",
    "        return [int(x) for x in curr[1:-1].split(',')]\n",
    "\n",
    "batch_size = 16\n",
    "num_classes = 2\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "train_custom = CustomDataset(train_dataset_ent, num_classes,tokenizer)\n",
    "train_dataloader = DataLoader(train_custom, batch_size=batch_size, shuffle=True)\n",
    "valid_custom = CustomDataset(val_dataset_ent, num_classes,tokenizer)\n",
    "val_dataloader = DataLoader(valid_custom, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c45c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(train_dataset_ent)[\"labels\"].value_counts().plot.pie(autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9814757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "        #self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "        #self.sig = torch.nn.GELU()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _ , pooled_output = self.bert(input_ids=input_ids, attention_mask =attention_mask,return_dict=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        #prediction = self.sig(logits)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "learning_rate = 3e-5\n",
    "base_model = BERTClassifier(num_classes)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(base_model.parameters(), lr=learning_rate)\n",
    "print (base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba66062",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model.to(device)\n",
    "\n",
    "\n",
    "def train(base_model,train_dataloader,optimizer,loss_function):\n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    valid_interval = 20  # Perform validation and save model every 10 iterations\n",
    "    iteration = 0\n",
    "    max_f1 = 0\n",
    "    stop_criterion = 2000000\n",
    "\n",
    "    running_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        base_model.train()  # Set the model to training mode\n",
    "        for curr_batch in train_dataloader:\n",
    "            \n",
    "            if iteration > stop_criterion:\n",
    "                break\n",
    "            \n",
    "            input_ids = curr_batch['input_ids'].to(device)\n",
    "            attention_mask = curr_batch['attention_mask'].to(device)\n",
    "            targets = curr_batch['multi_label'].to(device)\n",
    "\n",
    "\n",
    "\n",
    "            outputs = base_model(input_ids,attention_mask)\n",
    "            loss = loss_function(outputs.to(device), targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #print (iteration)\n",
    "            running_loss.append(loss.item())\n",
    "            if len(running_loss) > 20:\n",
    "                running_loss.pop(0)\n",
    "            print (f\"Epoch : {epoch} ,Iteration : {iteration}, training loss: {loss:.4f} , running loss:{sum(running_loss)/len(running_loss)}\",find_metrics(targets,outputs))\n",
    "            \n",
    "            # freeing up excess memory\n",
    "            del loss, outputs\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \n",
    "            # Validation and model saving\n",
    "            if iteration % valid_interval == 0:\n",
    "                base_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    total_loss = []\n",
    "                    f1_micro = []\n",
    "                    f1_macro = []\n",
    "                    f1_avg = []\n",
    "                    for val_batch in val_dataloader:\n",
    "                        val_input_ids = val_batch['input_ids'].to(device)\n",
    "                        val_attention_mask = val_batch['attention_mask'].to(device)\n",
    "                        val_targets = val_batch['multi_label'].to(device)\n",
    "\n",
    "\n",
    "                        outputs = base_model(val_input_ids,val_attention_mask)\n",
    "                        loss = loss_function(outputs.to(device), val_targets)\n",
    "                        \n",
    "                        total_loss.append(loss.item())\n",
    "                        val_out = find_metrics(val_targets,outputs)\n",
    "                        f1_micro.append(val_out[0])\n",
    "                        f1_macro.append(val_out[1])\n",
    "                        f1_avg.append(val_out[2])\n",
    "                        \n",
    "                        # emptying memory\n",
    "                        del val_out, loss, outputs\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                    avg_acc = sum(f1_avg)/len(f1_avg)\n",
    "                    avg_f1mic = sum(f1_micro)/len(f1_micro)\n",
    "                    avg_f1mac = sum(f1_macro)/len(f1_macro)\n",
    "                    avg_loss = sum(total_loss)/len(total_loss)\n",
    "                    \n",
    "                    print (f\"Validation loss : {sum(total_loss)/len(total_loss)} \", ' ,acc : ',avg_acc,\" ,f1-micro : \",avg_f1mic,\" ,f1-macro : \",avg_f1mac)\n",
    "                    if avg_f1mac > max_f1 :\n",
    "                        max_f1 = avg_f1mic\n",
    "                        torch.save(base_model.state_dict(),f\"model_trained_ent/model_{iteration}.pth\")\n",
    "                    \n",
    "                    del total_loss, f1_micro, f1_macro, f1_avg\n",
    "\n",
    "                base_model.train()  # Set the model back to training mode\n",
    "            \n",
    "            iteration += 1\n",
    "    return base_model, train_dataloader, optimizer, loss_function\n",
    "\n",
    "base_model, train_dataloader, optimizer, loss_function = train(base_model,train_dataloader,optimizer,loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_metrics1(targets,prediction):\n",
    "    #final_pred = ((torch.sigmoid(prediction) >= 0.5) * 1.0) \n",
    "    final_pred = prediction\n",
    "    np_tar = targets.cpu().detach().numpy()\n",
    "    np_pred = final_pred.cpu().detach().numpy()\n",
    "    \n",
    "    avg_f1_mic = f1_score(np_tar.flatten(), np_pred.flatten(), average='micro',zero_division=0)\n",
    "    avg_f1_mac = f1_score(np_tar, np_pred, average='macro',zero_division=1)\n",
    "    avg_acc = accuracy_score(np_tar, np_pred)\n",
    "    del np_tar\n",
    "    del np_pred\n",
    "    del final_pred\n",
    "    return avg_f1_mic, avg_f1_mac, avg_acc\n",
    "\n",
    "def test_accuracies(base_model,test_data,tokenizer_private):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    total_acc = []\n",
    "    \n",
    "    global_pred = []\n",
    "    global_tar = []\n",
    "    for i in range(0,len(test_data)):\n",
    "        \n",
    "        old_str = test_data[\"text\"][i]\n",
    "        doc = nlp(old_str)\n",
    "        old_str = (\" \").join([\" \".join([token.text for token in sent if not token.is_punct]) for sent in doc.sents])[:-1]\n",
    "        prediction = []\n",
    "        for j in range(0,8):\n",
    "            new_string = old_str[:-1] + \" \" + definitions[label_to_def[j]]  \n",
    "            inputs = tokenizer_private.encode_plus(\n",
    "                new_string,\n",
    "                add_special_tokens=True,\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            outputs = torch.sigmoid(base_model(inputs[\"input_ids\"].to(device),inputs[\"attention_mask\"].to(device))) \n",
    "            #print (outputs)\n",
    "            if outputs[0][1] > outputs[0][0] :\n",
    "                prediction.append(1)\n",
    "            else:\n",
    "                prediction.append(0)\n",
    "        \n",
    "        targets = torch.zeros(8, dtype=torch.float32)\n",
    "        targets[test_data[\"labels\"][i]] = 1\n",
    "        \n",
    "        global_pred.append(prediction)\n",
    "        global_tar.append(targets.tolist())\n",
    "        print (\"prediction : \",prediction)\n",
    "        print (\"targets : \",targets)\n",
    "        print (\"metrics : \",find_metrics1(torch.tensor(targets),torch.tensor(prediction)))\n",
    "        total_acc.append(list(find_metrics1(torch.tensor(targets),torch.tensor(prediction) )))\n",
    "        \n",
    "    global_pred = np.array(global_pred)\n",
    "    global_tar = np.array(global_tar)\n",
    "    incorrect_predictions = np.where(global_pred != global_tar)\n",
    "\n",
    "    # Compute label frequencies for incorrect predictions\n",
    "    incorrect_label_frequencies = {}\n",
    "    for label_idx, instance_idx in zip(*incorrect_predictions):\n",
    "        if label_idx not in incorrect_label_frequencies:\n",
    "            incorrect_label_frequencies[label_idx] = 1\n",
    "        else:\n",
    "            incorrect_label_frequencies[label_idx] += 1\n",
    "\n",
    "    # Create a table to display the incorrect predictions and frequencies\n",
    "    table_data = {\"Incorrect Label\": list(incorrect_label_frequencies.keys()),\n",
    "                  \"Frequency\": list(incorrect_label_frequencies.values())}\n",
    "    df = pd.DataFrame(table_data)\n",
    "    print (\"incorrect frequencies table: \\n\",df)\n",
    "    print (\"test statistics : \",np.sum(np.array(total_acc),axis=0)/len(np.array(total_acc)))\n",
    "    return global_pred,global_tar\n",
    "test_all_pred, test_all_tar = test_accuracies(base_model,test_dataset,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947f5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infreq(test_all_pred,test_all_tar):\n",
    "    for i in range(0,test_all_pred.shape[0]):\n",
    "        print (test_all_pred[i])\n",
    "        print (test_all_tar[i])\n",
    "infreq(test_all_pred,test_all_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df74abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"train_1_lab\" : [0,1,4,5,6,7],\n",
    "    \"test_1_lab\" : [2,3],\n",
    "    \"train_2_lab\" : [0,1,4,5],\n",
    "    \"test_2_lab\" : [2,3,6,7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c9bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new_show, test_new_show, val_new_show = {}, {}, {}\n",
    "\n",
    "def compare_list(list1,list2):\n",
    "    for i in range(0,len(list1)):\n",
    "        if list1[i] in list2:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "def convert_to_hidden(train_data,config,train_new_show,val_new_show):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    train_new_show[\"1_lab\"] = {\"text\":[],\"labels\":[]}\n",
    "    train_new_show[\"2_lab\"] = {\"text\":[],\"labels\":[]}\n",
    "    val_new_show[\"1_lab\"] = {\"text\":[],\"labels\":[]}\n",
    "    val_new_show[\"2_lab\"] = {\"text\":[],\"labels\":[]}\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(0,len(train_data[\"text\"])):\n",
    "        print (i,\"|\",len(train_data[\"text\"]))\n",
    "        # 1 lab\n",
    "        if compare_list(train_data[\"labels\"][i],config[\"train_1_lab\"]):\n",
    "            if len(train_dataset[\"labels\"][i]) > 0:\n",
    "                old_string = train_dataset[\"text\"][i]\n",
    "                random_number = random.randint(0, len(entail_con)) - 1\n",
    "                doc = nlp(old_string)\n",
    "                old_string = (\" \").join([\" \".join([token.text for token in sent if not token.is_punct]) for sent in doc.sents])[:-1]\n",
    "                for j in range(0,8):\n",
    "                    new_string = old_string + \" \" + entail_con[random_number] + \" \" + definitions[label_to_def[j]]\n",
    "                    train_new_show[\"1_lab\"][\"text\"].append(new_string)\n",
    "                    #train_new_show[\"1_lab\"][\"str_labels\"].append(str(train_dataset[\"labels\"][i]))\n",
    "                    if j in train_dataset[\"labels\"][i]:\n",
    "                        train_new_show[\"1_lab\"][\"labels\"].append([0,1])\n",
    "                    else:\n",
    "                        train_new_show[\"1_lab\"][\"labels\"].append([1,0])\n",
    "\n",
    "        else :\n",
    "            if len(train_dataset[\"labels\"][i]) > 0:\n",
    "                old_string = train_dataset[\"text\"][i]\n",
    "                random_number = random.randint(0, len(entail_con)) - 1\n",
    "                doc = nlp(old_string)\n",
    "                old_string = (\" \").join([\" \".join([token.text for token in sent if not token.is_punct]) for sent in doc.sents])[:-1]\n",
    "                for j in range(0,8):\n",
    "                    new_string = old_string + \" \" + entail_con[random_number] + \" \" + definitions[label_to_def[j]]\n",
    "                    val_new_show[\"1_lab\"][\"text\"].append(new_string)\n",
    "                    #val_new_show[\"1_lab\"][\"str_labels\"].append(str(train_dataset[\"labels\"][i]))\n",
    "                    if j in train_dataset[\"labels\"][i]:\n",
    "                        val_new_show[\"1_lab\"][\"labels\"].append([0,1])\n",
    "                    else:\n",
    "                        val_new_show[\"1_lab\"][\"labels\"].append([1,0])\n",
    "\n",
    "        # 1 lab\n",
    "        if compare_list(train_data[\"labels\"][i],config[\"train_1_lab\"]):\n",
    "            if len(train_dataset[\"labels\"][i]) > 0:\n",
    "                old_string = train_dataset[\"text\"][i]\n",
    "                random_number = random.randint(0, len(entail_con)) - 1\n",
    "                doc = nlp(old_string)\n",
    "                old_string = (\" \").join([\" \".join([token.text for token in sent if not token.is_punct]) for sent in doc.sents])[:-1]\n",
    "                for j in range(0,8):\n",
    "                    new_string = old_string + \" \" + entail_con[random_number] + \" \" + definitions[label_to_def[j]]\n",
    "                    train_new_show[\"2_lab\"][\"text\"].append(new_string)\n",
    "                    #train_new_show[\"2_lab\"][\"str_labels\"].append(str(train_dataset[\"labels\"][i]))\n",
    "                    if j in train_dataset[\"labels\"][i]:\n",
    "                        train_new_show[\"2_lab\"][\"labels\"].append([0,1])\n",
    "                    else:\n",
    "                        train_new_show[\"2_lab\"][\"labels\"].append([1,0])\n",
    "\n",
    "        else :\n",
    "            if len(train_dataset[\"labels\"][i]) > 0:\n",
    "                old_string = train_dataset[\"text\"][i]\n",
    "                random_number = random.randint(0, len(entail_con)) - 1\n",
    "                doc = nlp(old_string)\n",
    "                old_string = (\" \").join([\" \".join([token.text for token in sent if not token.is_punct]) for sent in doc.sents])[:-1]\n",
    "                for j in range(0,8):\n",
    "                    new_string = old_string + \" \" + entail_con[random_number] + \" \" + definitions[label_to_def[j]]\n",
    "                    val_new_show[\"2_lab\"][\"text\"].append(new_string)\n",
    "                    #val_new_show[\"2_lab\"][\"str_labels\"].append(str(train_dataset[\"labels\"][i]))\n",
    "                    if j in train_dataset[\"labels\"][i]:\n",
    "                        val_new_show[\"2_lab\"][\"labels\"].append([0,1])\n",
    "                    else:\n",
    "                        val_new_show[\"2_lab\"][\"labels\"].append([1,0])\n",
    "    \n",
    "    # add validation data from validation\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return train_new_show,val_new_show\n",
    "\n",
    "train_new_show, val_new_show = convert_to_hidden(train_dataset,config,train_new_show,val_new_show)\n",
    "train_new_show, val_new_show = convert_to_hidden(val_dataset,config,train_new_show,val_new_show)\n",
    "\n",
    "test_new_show = {\"1_lab_seen\":{\"text\":[],\"labels\":[]},\"2_lab_unseen\":{\"text\":[],\"labels\":[]}}\n",
    "def calculate_train_hidden_met(test_dataset,config,test_new_show):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    for i in range(0,len(test_dataset[\"text\"])):\n",
    "        if compare_list(test_dataset[\"labels\"][i],config[\"train_1_lab\"]):\n",
    "            if len(test_dataset[\"labels\"][i]) > 0:\n",
    "                old_string = test_dataset[\"text\"][i]\n",
    "                random_number = random.randint(0, len(entail_con)) - 1\n",
    "                doc = nlp(old_string)\n",
    "                old_string = (\" \").join([\" \".join([token.text for token in sent if not token.is_punct]) for sent in doc.sents])[:-1]\n",
    "                for j in range(0,8):\n",
    "                    new_string = old_string + \" \" + entail_con[random_number] + \" \" + definitions[label_to_def[j]]\n",
    "                    test_new_show[\"1_lab_seen\"][\"text\"].append(new_string)\n",
    "                    if j in test_dataset[\"labels\"][i]:\n",
    "                        test_new_show[\"1_lab_seen\"][\"labels\"].append([0,1])\n",
    "                    else:\n",
    "                        test_new_show[\"1_lab_seen\"][\"labels\"].append([1,0])\n",
    "\n",
    "        else :\n",
    "            if len(test_dataset[\"labels\"][i]) > 0:\n",
    "                old_string = test_dataset[\"text\"][i]\n",
    "                random_number = random.randint(0, len(entail_con)) - 1\n",
    "                doc = nlp(old_string)\n",
    "                old_string = (\" \").join([\" \".join([token.text for token in sent if not token.is_punct]) for sent in doc.sents])[:-1]\n",
    "                for j in range(0,8):\n",
    "                    new_string = old_string + \" \" + entail_con[random_number] + \" \" + definitions[label_to_def[j]]\n",
    "                    test_new_show[\"2_lab_unseen\"][\"text\"].append(new_string)\n",
    "\n",
    "                    if j in test_dataset[\"labels\"][i]:\n",
    "                        test_new_show[\"2_lab_unseen\"][\"labels\"].append([0,1])\n",
    "                    else:\n",
    "                        test_new_show[\"2_lab_unseen\"][\"labels\"].append([1,0])\n",
    "    \n",
    "    return test_new_show\n",
    "\n",
    "test_new_show = calculate_train_hidden_met(test_dataset,config,test_new_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 1 lab\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model1 = BERTClassifier(2)\n",
    "base_model1.to(device)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(base_model1.parameters(), lr=learning_rate)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset,num_classes,tokenizer):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.texts = self.dataset[\"text\"]\n",
    "        self.labels = self.dataset[\"labels\"]\n",
    "        self.num_classes = num_classes\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "    \n",
    "        multi_label = torch.tensor(label,dtype=torch.float)\n",
    "        #multi_label = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        #multi_label[label] = 1\n",
    "        \n",
    "        return {'input_ids':input_ids, 'attention_mask':attention_mask, 'multi_label':multi_label}\n",
    "    \n",
    "def list_it(curr):\n",
    "    return [curr]\n",
    "\n",
    "def delist_it(curr):\n",
    "    new_list = []\n",
    "    for i in curr:\n",
    "        new_list.append(i[0][0])\n",
    "    return new_list\n",
    "\n",
    "def str2list(curr):\n",
    "    if curr==\"[]\":\n",
    "        return []\n",
    "    else:\n",
    "        return [int(x) for x in curr[1:-1].split(',')]\n",
    "\n",
    "batch_size = 16\n",
    "num_classes = 2\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "train_custom1 = CustomDataset(train_new_show[\"1_lab\"], num_classes,tokenizer)\n",
    "train_dataloader1 = DataLoader(train_custom1, batch_size=batch_size, shuffle=True)\n",
    "valid_custom1 = CustomDataset(val_new_show[\"1_lab\"], num_classes,tokenizer)\n",
    "val_dataloader1 = DataLoader(valid_custom1, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def train(base_model,train_dataloader,val_dataloader,optimizer,loss_function):\n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    valid_interval = 20  # Perform validation and save model every 10 iterations\n",
    "    iteration = 0\n",
    "    max_f1 = 0\n",
    "    stop_criterion = 2000000\n",
    "\n",
    "    running_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        base_model.train()  # Set the model to training mode\n",
    "        for curr_batch in train_dataloader:\n",
    "            \n",
    "            if iteration > stop_criterion:\n",
    "                break\n",
    "            \n",
    "            input_ids = curr_batch['input_ids'].to(device)\n",
    "            attention_mask = curr_batch['attention_mask'].to(device)\n",
    "            targets = curr_batch['multi_label'].to(device)\n",
    "\n",
    "\n",
    "\n",
    "            outputs = base_model(input_ids,attention_mask)\n",
    "            loss = loss_function(outputs.to(device), targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #print (iteration)\n",
    "            running_loss.append(loss.item())\n",
    "            if len(running_loss) > 20:\n",
    "                running_loss.pop(0)\n",
    "            print (f\"Epoch : {epoch} ,Iteration : {iteration}, training loss: {loss:.4f} , running loss:{sum(running_loss)/len(running_loss)}\",find_metrics(targets,outputs))\n",
    "            \n",
    "            # freeing up excess memory\n",
    "            del loss, outputs\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \n",
    "            # Validation and model saving\n",
    "            if iteration % valid_interval == 0:\n",
    "                base_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    total_loss = []\n",
    "                    f1_micro = []\n",
    "                    f1_macro = []\n",
    "                    f1_avg = []\n",
    "                    for val_batch in val_dataloader:\n",
    "                        val_input_ids = val_batch['input_ids'].to(device)\n",
    "                        val_attention_mask = val_batch['attention_mask'].to(device)\n",
    "                        val_targets = val_batch['multi_label'].to(device)\n",
    "\n",
    "\n",
    "                        outputs = base_model(val_input_ids,val_attention_mask)\n",
    "                        loss = loss_function(outputs.to(device), val_targets)\n",
    "                        \n",
    "                        total_loss.append(loss.item())\n",
    "                        val_out = find_metrics(val_targets,outputs)\n",
    "                        f1_micro.append(val_out[0])\n",
    "                        f1_macro.append(val_out[1])\n",
    "                        f1_avg.append(val_out[2])\n",
    "                        \n",
    "                        # emptying memory\n",
    "                        del val_out, loss, outputs\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                    avg_acc = sum(f1_avg)/len(f1_avg)\n",
    "                    avg_f1mic = sum(f1_micro)/len(f1_micro)\n",
    "                    avg_f1mac = sum(f1_macro)/len(f1_macro)\n",
    "                    avg_loss = sum(total_loss)/len(total_loss)\n",
    "                    \n",
    "                    print (f\"Validation loss : {sum(total_loss)/len(total_loss)} \", ' ,acc : ',avg_acc,\" ,f1-micro : \",avg_f1mic,\" ,f1-macro : \",avg_f1mac)\n",
    "                    if avg_f1mic > max_f1 :\n",
    "                        max_f1 = avg_f1mic\n",
    "                        torch.save(base_model.state_dict(),f\"models_1_lab/model_{iteration}.pth\")\n",
    "                    \n",
    "                    del total_loss, f1_micro, f1_macro, f1_avg\n",
    "\n",
    "                base_model.train()  # Set the model back to training mode\n",
    "            \n",
    "            iteration += 1\n",
    "    return base_model, train_dataloader, optimizer, loss_function\n",
    "\n",
    "base_model1, train_dataloader1, optimizer, loss_function = train(base_model1,train_dataloader1,val_dataloader1,optimizer,loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357e575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4a829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18930cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7320fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
