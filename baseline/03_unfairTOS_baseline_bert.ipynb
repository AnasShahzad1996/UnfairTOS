{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0747e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------# Import libraries and datasets #------#\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import datasets as dts\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import gc\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from transformers import BertModel,AutoModel\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1bdd8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset lex_glue (/home/anas/.cache/huggingface/datasets/lex_glue/unfair_tos/1.0.0/8a66420941bf6e77a7ddd4da4d3bfb7ba88ef48c1d55302a568ac650a095ca3a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa69d352fc643a7bfb00de1bd507b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dts.load_dataset('lex_glue','unfair_tos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938c7242",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.DataFrame.from_dict(dataset[\"train\"])\n",
    "val_dataset = pd.DataFrame.from_dict(dataset[\"validation\"])\n",
    "test_dataset = pd.DataFrame.from_dict(dataset[\"test\"])\n",
    "\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6004daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = {\"Limitation of liability\": [\"This clause stipulates that the duty to pay damages is limited or excluded, for certain kind of losses, under certain conditions. \"]\n",
    "               , \"Unilateral termination\": [\"This clause gives provider the right to suspend and/or terminate the service and/or the contract, and sometimes details the circumstances under which the provider claims to have a right to do so.\"]\n",
    "               , \"Unilateral change\": [\"This clause specifies the conditions under which the service provider could amend and modify the terms of service and/or the service itself.\"]\n",
    "               , \"Content removal\": [\"This clause gives the provider a right to modify/delete userâ€™s content, including in-app purchases, and sometimes specifies the conditions under which the service provider may do so.\"]\n",
    "               , \"Contract by using\": [\"This clause stipulates that the consumer is bound by the terms of use of a specific service, simply by using the service, without even being required to mark that he or she has read and accepted them.\"]\n",
    "               , \"Choice of law\": [\"This clause specifies what law will govern the contract, meaning also what law will be applied in potential adjudication of a dispute arising under the contract.\"]\n",
    "               , \"Jurisdiction\": [\"This selection clause requires or allows the parties to resolve their disputes through an arbitration process, before the case could go to court.\"]\n",
    "               , \"Arbitration\": [\"This forum selection clause requires or allows the parties to resolve their disputes through an arbitration process, before the case could go to court however, such a clause may or may not specify that arbitration should occur within a specific jurisdiction. \"]}\n",
    "\n",
    "entailment_con = [\"entails\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588159dd",
   "metadata": {},
   "source": [
    "### Default with 8 + 1 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fd4e005f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa37fad6a72428d946eb851f01e2bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac1ccf38b324ed697135982cbc4c791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTClassifier(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc): Linear(in_features=768, out_features=8, bias=True)\n",
      "  (sig): GELU(approximate='none')\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "        #self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(768, 8)\n",
    "        self.sig = torch.nn.GELU()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _ , pooled_output = self.bert(input_ids=input_ids, attention_mask =attention_mask,return_dict=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        prediction = self.sig(logits)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "learning_rate = 3e-5\n",
    "num_classes = 8\n",
    "base_model = BERTClassifier(num_classes)\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(base_model.parameters(), lr=learning_rate)\n",
    "print (base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "698e897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset,num_classes,tokenizer):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.texts = self.dataset[\"text\"]\n",
    "        self.labels = self.dataset[\"labels\"]\n",
    "        self.num_classes = num_classes\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        #print (\"original label: \",label)\n",
    "        # Convert label to one-hot encoding\n",
    "        multi_label = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        multi_label[label] = 1\n",
    "        #print (\"one hot multi : \",multi_label)\n",
    "        return {'input_ids':input_ids, 'attention_mask':attention_mask, 'multi_label':multi_label}\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "batch_size = 8\n",
    "train_custom = CustomDataset(train_dataset, num_classes,tokenizer)\n",
    "train_dataloader = DataLoader(train_custom, batch_size=batch_size, shuffle=True)\n",
    "valid_custom = CustomDataset(val_dataset, num_classes,tokenizer)\n",
    "val_dataloader = DataLoader(valid_custom, batch_size=batch_size, shuffle=True)\n",
    "def find_metrics(targets,prediction):\n",
    "    final_pred = ((torch.sigmoid(prediction) >= 0.5) * 1.0)\n",
    "    np_tar = targets.cpu().detach().numpy()\n",
    "    np_pred = final_pred.cpu().detach().numpy()\n",
    "    \n",
    "    avg_f1_mic = f1_score(np_tar.flatten(), np_pred.flatten(), average='micro',zero_division=0)\n",
    "    avg_f1_mac = f1_score(np_tar, np_pred, average='macro',zero_division=1)\n",
    "    avg_acc = accuracy_score(np_tar, np_pred)\n",
    "    del np_tar\n",
    "    del np_pred\n",
    "    del final_pred\n",
    "    return avg_f1_mic, avg_f1_mac, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "63e68bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 1, training loss: 1.6184 , running loss:1.6184499263763428 (0.875, 0.875, 0.0)\n",
      "Epoch : 1 ,Iteration : 2, training loss: 2.1937 , running loss:1.9060617685317993 (0.875, 0.875, 0.0)\n",
      "Epoch : 2 ,Iteration : 3, training loss: 1.5850 , running loss:1.7990306615829468 (0.75, 0.75, 0.0)\n",
      "Epoch : 3 ,Iteration : 4, training loss: 1.8905 , running loss:1.8219015300273895 (0.875, 0.875, 0.0)\n",
      "Epoch : 4 ,Iteration : 5, training loss: 1.6284 , running loss:1.7831977367401124 (0.75, 0.75, 0.0)\n",
      "Epoch : 5 ,Iteration : 6, training loss: 1.6870 , running loss:1.7671671311060588 (0.75, 0.75, 0.0)\n",
      "Epoch : 6 ,Iteration : 7, training loss: 1.5712 , running loss:1.7391750812530518 (0.875, 0.875, 0.0)\n",
      "Epoch : 7 ,Iteration : 8, training loss: 1.4853 , running loss:1.707445204257965 (1.0, 1.0, 1.0)\n",
      "Epoch : 8 ,Iteration : 9, training loss: 1.4633 , running loss:1.680318898624844 (0.875, 0.875, 0.0)\n",
      "Epoch : 9 ,Iteration : 10, training loss: 1.4691 , running loss:1.6592008233070374 (1.0, 1.0, 1.0)\n",
      "Epoch : 10 ,Iteration : 11, training loss: 1.4712 , running loss:1.6421129595149646 (1.0, 1.0, 1.0)\n",
      "Epoch : 11 ,Iteration : 12, training loss: 1.4459 , running loss:1.6257626712322235 (1.0, 1.0, 1.0)\n",
      "Epoch : 12 ,Iteration : 13, training loss: 1.4443 , running loss:1.611807080415579 (1.0, 1.0, 1.0)\n",
      "Epoch : 13 ,Iteration : 14, training loss: 1.4260 , running loss:1.5985349757330758 (1.0, 1.0, 1.0)\n",
      "Epoch : 14 ,Iteration : 15, training loss: 1.4164 , running loss:1.5863911946614584 (1.0, 1.0, 1.0)\n",
      "Epoch : 15 ,Iteration : 16, training loss: 1.4705 , running loss:1.5791469067335129 (1.0, 1.0, 1.0)\n",
      "Epoch : 16 ,Iteration : 17, training loss: 1.4179 , running loss:1.569660600493936 (1.0, 1.0, 1.0)\n",
      "Epoch : 17 ,Iteration : 18, training loss: 1.4284 , running loss:1.561812652481927 (1.0, 1.0, 1.0)\n",
      "Epoch : 18 ,Iteration : 19, training loss: 1.4228 , running loss:1.5544963761379844 (1.0, 1.0, 1.0)\n",
      "eval epoch\n",
      "Epoch : 19 ,Iteration : 20, training loss: 1.4048 , running loss:1.547013920545578 (1.0, 1.0, 1.0)\n",
      "Epoch : 20 ,Iteration : 21, training loss: 1.4055 , running loss:1.5363640010356903 (1.0, 1.0, 1.0)\n",
      "Epoch : 21 ,Iteration : 22, training loss: 1.4174 , running loss:1.4975519478321075 (1.0, 1.0, 1.0)\n",
      "Epoch : 22 ,Iteration : 23, training loss: 1.4017 , running loss:1.4883862257003784 (1.0, 1.0, 1.0)\n",
      "Epoch : 23 ,Iteration : 24, training loss: 1.4110 , running loss:1.4644106149673461 (1.0, 1.0, 1.0)\n",
      "Epoch : 24 ,Iteration : 25, training loss: 1.4058 , running loss:1.4532806158065796 (1.0, 1.0, 1.0)\n",
      "Epoch : 25 ,Iteration : 26, training loss: 1.4371 , running loss:1.4407837867736817 (1.0, 1.0, 1.0)\n",
      "Epoch : 26 ,Iteration : 27, training loss: 1.4020 , running loss:1.4323249638080597 (1.0, 1.0, 1.0)\n",
      "Epoch : 27 ,Iteration : 28, training loss: 1.4006 , running loss:1.4280885577201843 (1.0, 1.0, 1.0)\n",
      "Epoch : 28 ,Iteration : 29, training loss: 1.3987 , running loss:1.4248587548732758 (1.0, 1.0, 1.0)\n",
      "Epoch : 29 ,Iteration : 30, training loss: 1.4013 , running loss:1.4214651584625244 (1.0, 1.0, 1.0)\n",
      "Epoch : 30 ,Iteration : 31, training loss: 1.3996 , running loss:1.4178827345371245 (1.0, 1.0, 1.0)\n",
      "Epoch : 31 ,Iteration : 32, training loss: 1.3999 , running loss:1.4155797839164734 (1.0, 1.0, 1.0)\n",
      "Epoch : 32 ,Iteration : 33, training loss: 1.4100 , running loss:1.413864052295685 (1.0, 1.0, 1.0)\n",
      "Epoch : 33 ,Iteration : 34, training loss: 1.4050 , running loss:1.4128121256828308 (1.0, 1.0, 1.0)\n",
      "Epoch : 34 ,Iteration : 35, training loss: 1.4329 , running loss:1.413638710975647 (1.0, 1.0, 1.0)\n",
      "Epoch : 35 ,Iteration : 36, training loss: 1.4183 , running loss:1.4110289633274078 (1.0, 1.0, 1.0)\n",
      "Epoch : 36 ,Iteration : 37, training loss: 1.4393 , running loss:1.4121002197265624 (1.0, 1.0, 1.0)\n",
      "Epoch : 37 ,Iteration : 38, training loss: 1.4070 , running loss:1.4110320270061494 (1.0, 1.0, 1.0)\n",
      "Epoch : 38 ,Iteration : 39, training loss: 1.3941 , running loss:1.409598058462143 (1.0, 1.0, 1.0)\n",
      "eval epoch\n",
      "Epoch : 39 ,Iteration : 40, training loss: 1.3951 , running loss:1.4091102004051208 (1.0, 1.0, 1.0)\n",
      "Epoch : 40 ,Iteration : 41, training loss: 1.4032 , running loss:1.4089997947216033 (1.0, 1.0, 1.0)\n",
      "Epoch : 41 ,Iteration : 42, training loss: 1.3977 , running loss:1.408014714717865 (1.0, 1.0, 1.0)\n",
      "Epoch : 42 ,Iteration : 43, training loss: 1.3965 , running loss:1.407755011320114 (1.0, 1.0, 1.0)\n",
      "Epoch : 43 ,Iteration : 44, training loss: 1.4203 , running loss:1.4082219123840332 (1.0, 1.0, 1.0)\n",
      "Epoch : 44 ,Iteration : 45, training loss: 1.4040 , running loss:1.4081336915493012 (1.0, 1.0, 1.0)\n",
      "Epoch : 45 ,Iteration : 46, training loss: 1.3937 , running loss:1.4059649109840393 (1.0, 1.0, 1.0)\n",
      "Epoch : 46 ,Iteration : 47, training loss: 1.4075 , running loss:1.4062356293201446 (1.0, 1.0, 1.0)\n",
      "Epoch : 47 ,Iteration : 48, training loss: 1.4033 , running loss:1.4063692212104797 (1.0, 1.0, 1.0)\n",
      "Epoch : 48 ,Iteration : 49, training loss: 1.3936 , running loss:1.406113338470459 (1.0, 1.0, 1.0)\n",
      "Epoch : 49 ,Iteration : 50, training loss: 1.3970 , running loss:1.4059021413326263 (1.0, 1.0, 1.0)\n",
      "Epoch : 50 ,Iteration : 51, training loss: 1.4072 , running loss:1.40628120303154 (1.0, 1.0, 1.0)\n",
      "Epoch : 51 ,Iteration : 52, training loss: 1.3963 , running loss:1.4061060130596161 (1.0, 1.0, 1.0)\n",
      "Epoch : 52 ,Iteration : 53, training loss: 1.4016 , running loss:1.4056841731071472 (1.0, 1.0, 1.0)\n",
      "Epoch : 53 ,Iteration : 54, training loss: 1.4592 , running loss:1.4083945453166962 (1.0, 1.0, 1.0)\n",
      "Epoch : 54 ,Iteration : 55, training loss: 1.4017 , running loss:1.406832605600357 (1.0, 1.0, 1.0)\n",
      "Epoch : 55 ,Iteration : 56, training loss: 1.4215 , running loss:1.4069951176643372 (1.0, 1.0, 1.0)\n",
      "Epoch : 56 ,Iteration : 57, training loss: 1.3954 , running loss:1.404797899723053 (1.0, 1.0, 1.0)\n",
      "Epoch : 57 ,Iteration : 58, training loss: 1.4438 , running loss:1.406635767221451 (1.0, 1.0, 1.0)\n",
      "Epoch : 58 ,Iteration : 59, training loss: 1.3962 , running loss:1.4067379891872407 (1.0, 1.0, 1.0)\n",
      "eval epoch\n",
      "Epoch : 59 ,Iteration : 60, training loss: 1.4011 , running loss:1.4070370495319366 (1.0, 1.0, 1.0)\n",
      "Epoch : 60 ,Iteration : 61, training loss: 1.4119 , running loss:1.4074696600437164 (1.0, 1.0, 1.0)\n",
      "Epoch : 61 ,Iteration : 62, training loss: 1.4017 , running loss:1.407666552066803 (1.0, 1.0, 1.0)\n",
      "Epoch : 62 ,Iteration : 63, training loss: 1.3936 , running loss:1.4075239181518555 (1.0, 1.0, 1.0)\n",
      "Epoch : 63 ,Iteration : 64, training loss: 1.3945 , running loss:1.406231302022934 (1.0, 1.0, 1.0)\n",
      "Epoch : 64 ,Iteration : 65, training loss: 1.3933 , running loss:1.4056959986686706 (1.0, 1.0, 1.0)\n",
      "Epoch : 65 ,Iteration : 66, training loss: 1.3935 , running loss:1.4056871056556701 (1.0, 1.0, 1.0)\n",
      "Epoch : 66 ,Iteration : 67, training loss: 1.4520 , running loss:1.4079135298728942 (1.0, 1.0, 1.0)\n",
      "Epoch : 67 ,Iteration : 68, training loss: 1.4031 , running loss:1.4079022824764251 (1.0, 1.0, 1.0)\n",
      "Epoch : 68 ,Iteration : 69, training loss: 1.3929 , running loss:1.4078691124916076 (1.0, 1.0, 1.0)\n",
      "Epoch : 69 ,Iteration : 70, training loss: 1.3935 , running loss:1.4076914489269257 (1.0, 1.0, 1.0)\n",
      "Epoch : 70 ,Iteration : 71, training loss: 1.4074 , running loss:1.4077013611793519 (1.0, 1.0, 1.0)\n",
      "Epoch : 71 ,Iteration : 72, training loss: 1.4272 , running loss:1.4092427253723145 (1.0, 1.0, 1.0)\n",
      "Epoch : 72 ,Iteration : 73, training loss: 1.3984 , running loss:1.409081482887268 (1.0, 1.0, 1.0)\n",
      "Epoch : 73 ,Iteration : 74, training loss: 1.4490 , running loss:1.4085734844207765 (1.0, 1.0, 1.0)\n",
      "Epoch : 74 ,Iteration : 75, training loss: 1.3928 , running loss:1.4081321120262147 (1.0, 1.0, 1.0)\n",
      "Epoch : 75 ,Iteration : 76, training loss: 1.4292 , running loss:1.4085143327713012 (1.0, 1.0, 1.0)\n",
      "Epoch : 76 ,Iteration : 77, training loss: 1.3964 , running loss:1.4085654377937318 (1.0, 1.0, 1.0)\n",
      "Epoch : 77 ,Iteration : 78, training loss: 1.4087 , running loss:1.406810462474823 (1.0, 1.0, 1.0)\n",
      "Epoch : 78 ,Iteration : 79, training loss: 1.4252 , running loss:1.408262699842453 (1.0, 1.0, 1.0)\n",
      "eval epoch\n",
      "Epoch : 79 ,Iteration : 80, training loss: 1.3963 , running loss:1.4080246448516847 (1.0, 1.0, 1.0)\n",
      "Epoch : 80 ,Iteration : 81, training loss: 1.4003 , running loss:1.4074471056461335 (1.0, 1.0, 1.0)\n",
      "Epoch : 81 ,Iteration : 82, training loss: 1.3927 , running loss:1.4069972276687621 (1.0, 1.0, 1.0)\n",
      "Epoch : 82 ,Iteration : 83, training loss: 1.4034 , running loss:1.407489162683487 (1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 83 ,Iteration : 84, training loss: 1.3963 , running loss:1.4075821936130524 (1.0, 1.0, 1.0)\n",
      "Epoch : 84 ,Iteration : 85, training loss: 1.4088 , running loss:1.4083540916442872 (1.0, 1.0, 1.0)\n",
      "Epoch : 85 ,Iteration : 86, training loss: 1.4261 , running loss:1.409983789920807 (1.0, 1.0, 1.0)\n",
      "Epoch : 86 ,Iteration : 87, training loss: 1.3923 , running loss:1.407001680135727 (1.0, 1.0, 1.0)\n",
      "Epoch : 87 ,Iteration : 88, training loss: 1.3982 , running loss:1.4067601144313813 (1.0, 1.0, 1.0)\n",
      "Epoch : 88 ,Iteration : 89, training loss: 1.3960 , running loss:1.4069125294685363 (1.0, 1.0, 1.0)\n",
      "Epoch : 89 ,Iteration : 90, training loss: 1.4050 , running loss:1.407490473985672 (1.0, 1.0, 1.0)\n",
      "Epoch : 90 ,Iteration : 91, training loss: 1.4073 , running loss:1.4074879586696625 (1.0, 1.0, 1.0)\n",
      "Epoch : 91 ,Iteration : 92, training loss: 1.4480 , running loss:1.4085281789302826 (1.0, 1.0, 1.0)\n",
      "Epoch : 92 ,Iteration : 93, training loss: 1.3930 , running loss:1.4082599461078644 (1.0, 1.0, 1.0)\n",
      "Epoch : 93 ,Iteration : 94, training loss: 1.4452 , running loss:1.4080714643001557 (1.0, 1.0, 1.0)\n",
      "Epoch : 94 ,Iteration : 95, training loss: 1.4044 , running loss:1.4086511135101318 (1.0, 1.0, 1.0)\n",
      "Epoch : 95 ,Iteration : 96, training loss: 1.3950 , running loss:1.4069434523582458 (1.0, 1.0, 1.0)\n",
      "Epoch : 96 ,Iteration : 97, training loss: 1.3979 , running loss:1.4070197761058807 (1.0, 1.0, 1.0)\n",
      "Epoch : 97 ,Iteration : 98, training loss: 1.4101 , running loss:1.4070906162261962 (1.0, 1.0, 1.0)\n",
      "Epoch : 98 ,Iteration : 99, training loss: 1.4270 , running loss:1.4071803152561189 (1.0, 1.0, 1.0)\n",
      "eval epoch\n",
      "Epoch : 99 ,Iteration : 100, training loss: 1.3933 , running loss:1.4070294678211213 (1.0, 1.0, 1.0)\n",
      "Epoch : 100 ,Iteration : 101, training loss: 1.4087 , running loss:1.4074454605579376 (1.0, 1.0, 1.0)\n",
      "Epoch : 101 ,Iteration : 102, training loss: 1.3958 , running loss:1.4076042115688323 (1.0, 1.0, 1.0)\n",
      "Epoch : 102 ,Iteration : 103, training loss: 1.4199 , running loss:1.4084284365177155 (1.0, 1.0, 1.0)\n",
      "Epoch : 103 ,Iteration : 104, training loss: 1.3985 , running loss:1.408537071943283 (1.0, 1.0, 1.0)\n",
      "Epoch : 104 ,Iteration : 105, training loss: 1.4032 , running loss:1.408257830142975 (1.0, 1.0, 1.0)\n",
      "Epoch : 105 ,Iteration : 106, training loss: 1.4290 , running loss:1.4084004282951355 (1.0, 1.0, 1.0)\n",
      "Epoch : 106 ,Iteration : 107, training loss: 1.3929 , running loss:1.4084264755249023 (1.0, 1.0, 1.0)\n",
      "Epoch : 107 ,Iteration : 108, training loss: 1.4347 , running loss:1.4102490603923798 (1.0, 1.0, 1.0)\n",
      "Epoch : 108 ,Iteration : 109, training loss: 1.4637 , running loss:1.4136375486850739 (1.0, 1.0, 1.0)\n",
      "Epoch : 109 ,Iteration : 110, training loss: 1.4131 , running loss:1.4140379548072814 (1.0, 1.0, 1.0)\n",
      "Epoch : 110 ,Iteration : 111, training loss: 1.4102 , running loss:1.4141799628734588 (1.0, 1.0, 1.0)\n",
      "Epoch : 111 ,Iteration : 112, training loss: 1.4204 , running loss:1.4128012239933014 (1.0, 1.0, 1.0)\n",
      "Epoch : 112 ,Iteration : 113, training loss: 1.3989 , running loss:1.4130965232849122 (1.0, 1.0, 1.0)\n",
      "Epoch : 113 ,Iteration : 114, training loss: 1.3929 , running loss:1.4104799807071686 (1.0, 1.0, 1.0)\n",
      "Epoch : 114 ,Iteration : 115, training loss: 1.3973 , running loss:1.4101221919059754 (1.0, 1.0, 1.0)\n",
      "Epoch : 115 ,Iteration : 116, training loss: 1.3947 , running loss:1.4101059794425965 (1.0, 1.0, 1.0)\n",
      "Epoch : 116 ,Iteration : 117, training loss: 1.3921 , running loss:1.4098160922527314 (1.0, 1.0, 1.0)\n",
      "Epoch : 117 ,Iteration : 118, training loss: 1.4416 , running loss:1.411388248205185 (1.0, 1.0, 1.0)\n",
      "Epoch : 118 ,Iteration : 119, training loss: 1.3969 , running loss:1.4098828792572022 (1.0, 1.0, 1.0)\n",
      "eval epoch\n",
      "Epoch : 119 ,Iteration : 120, training loss: 1.3913 , running loss:1.409781014919281 (1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6746/2183262614.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverfit_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6746/2183262614.py\u001b[0m in \u001b[0;36moverfit_one\u001b[0;34m(base_model, fit_example)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# freeing up excess memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# overfitting on one example:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model.to(device)\n",
    "one_example = None\n",
    "while one_example == None:\n",
    "    curr_batch = next(iter(train_dataloader))\n",
    "    if torch.sum(curr_batch[\"multi_label\"]) > 1:\n",
    "        one_example = curr_batch\n",
    "\n",
    "def overfit_one(base_model,fit_example):\n",
    "    num_epochs = 1000\n",
    "    running_loss = []\n",
    "    iteration = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        base_model.train()  # Set the model to training mode            \n",
    "        \n",
    "        iteration +=1\n",
    "        if iteration % 20 == 0:\n",
    "            print (\"eval epoch\")\n",
    "            base_model.eval()\n",
    "        input_ids = fit_example['input_ids'].to(device)\n",
    "        attention_mask = fit_example['attention_mask'].to(device)\n",
    "        targets = fit_example['multi_label'].to(device)\n",
    "\n",
    "\n",
    "        outputs = base_model(input_ids,attention_mask)\n",
    "        loss = loss_function(outputs.to(device), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #print (iteration)\n",
    "        running_loss.append(loss.item())\n",
    "        if len(running_loss) > 20:\n",
    "            running_loss.pop(0)\n",
    "        #print (outputs,\"||||\",targets)\n",
    "        #print (\"debug metrics : \",find_metrics(targets,targets))\n",
    "        print (f\"Epoch : {epoch} ,Iteration : {iteration}, training loss: {loss:.4f} , running loss:{sum(running_loss)/len(running_loss)}\",find_metrics(targets,outputs))\n",
    "\n",
    "        if iteration % 20 == 0:\n",
    "            base_model.train()\n",
    "\n",
    "        \n",
    "        # freeing up excess memory\n",
    "        del loss, outputs\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return base_model\n",
    "\n",
    "base_model = overfit_one(base_model,one_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d35c015f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 0, training loss: 0.0825 , running loss:0.08245602995157242 (0.984375, 0.875, 0.875)\n",
      "Validation loss : 0.07325815737508891   ,acc :  0.8991228070175439  ,f1-micro :  0.9863486842105263  ,f1-macro :  0.8951754385964912\n",
      "Epoch : 0 ,Iteration : 1, training loss: 0.0275 , running loss:0.05499326344579458 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 2, training loss: 0.0737 , running loss:0.061231689527630806 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 3, training loss: 0.0865 , running loss:0.0675589400343597 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 4, training loss: 0.0808 , running loss:0.07020175866782666 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 5, training loss: 0.0258 , running loss:0.06279518237958352 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 6, training loss: 0.0266 , running loss:0.05762733519077301 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 7, training loss: 0.0821 , running loss:0.060682171024382114 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 8, training loss: 0.0781 , running loss:0.0626127529475424 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 9, training loss: 0.1252 , running loss:0.06887051239609718 (0.96875, 0.75, 0.75)\n",
      "Epoch : 0 ,Iteration : 10, training loss: 0.1987 , running loss:0.08067656714807857 (0.953125, 0.625, 0.75)\n",
      "Validation loss : 0.07280916861144074   ,acc :  0.8991228070175439  ,f1-micro :  0.9863486842105263  ,f1-macro :  0.8973684210526316\n",
      "Epoch : 0 ,Iteration : 11, training loss: 0.1929 , running loss:0.09002779237926006 (0.953125, 0.75, 0.625)\n",
      "Epoch : 0 ,Iteration : 12, training loss: 0.0263 , running loss:0.08512882143259048 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 13, training loss: 0.0698 , running loss:0.08403676695057324 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 14, training loss: 0.0730 , running loss:0.08329940189917882 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 15, training loss: 0.0277 , running loss:0.07982452656142414 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 16, training loss: 0.0276 , running loss:0.07675059249295908 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 17, training loss: 0.0285 , running loss:0.07406871827940147 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 18, training loss: 0.0264 , running loss:0.07155726379469822 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 19, training loss: 0.0257 , running loss:0.06926501952111722 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 20, training loss: 0.0245 , running loss:0.06636686585843563 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.07221315427961057   ,acc :  0.8991228070175439  ,f1-micro :  0.9863486842105263  ,f1-macro :  0.8956140350877193\n",
      "Epoch : 0 ,Iteration : 21, training loss: 0.0250 , running loss:0.0662423993460834 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 22, training loss: 0.0245 , running loss:0.06378269689157605 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 23, training loss: 0.0759 , running loss:0.06325268605723977 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 24, training loss: 0.0239 , running loss:0.06040723379701376 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 25, training loss: 0.0240 , running loss:0.06031873859465122 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 26, training loss: 0.1451 , running loss:0.06624271543696522 (0.96875, 0.75, 0.75)\n",
      "Epoch : 0 ,Iteration : 27, training loss: 0.1334 , running loss:0.06880717733874916 (0.96875, 0.75, 0.75)\n",
      "Epoch : 0 ,Iteration : 28, training loss: 0.1312 , running loss:0.07146386662498116 (0.96875, 0.75, 0.75)\n",
      "Epoch : 0 ,Iteration : 29, training loss: 0.0257 , running loss:0.06648751916363835 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 30, training loss: 0.0239 , running loss:0.05774369267746806 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.07292946469234793   ,acc :  0.8991228070175439  ,f1-micro :  0.9863486842105263  ,f1-macro :  0.8969298245614035\n",
      "Epoch : 0 ,Iteration : 31, training loss: 0.0807 , running loss:0.05213175779208541 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 32, training loss: 0.1298 , running loss:0.057306223269551995 (0.96875, 0.875, 0.75)\n",
      "Epoch : 0 ,Iteration : 33, training loss: 0.1921 , running loss:0.06341820796951651 (0.953125, 0.75, 0.625)\n",
      "Epoch : 0 ,Iteration : 34, training loss: 0.0759 , running loss:0.06356341829523444 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 35, training loss: 0.0231 , running loss:0.06333165159448981 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 36, training loss: 0.0815 , running loss:0.06602792730554938 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 37, training loss: 0.0226 , running loss:0.06573383221402765 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 38, training loss: 0.0712 , running loss:0.06797639084979892 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 39, training loss: 0.0786 , running loss:0.07062004869803787 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 40, training loss: 0.0790 , running loss:0.0733470625244081 (0.984375, 0.875, 0.875)\n",
      "Validation loss : 0.07158028529793548   ,acc :  0.8991228070175439  ,f1-micro :  0.9863486842105263  ,f1-macro :  0.8995614035087719\n",
      "Epoch : 0 ,Iteration : 41, training loss: 0.0220 , running loss:0.0731965595856309 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 42, training loss: 0.0793 , running loss:0.07593360804021358 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 43, training loss: 0.0781 , running loss:0.0760404571890831 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 44, training loss: 0.0707 , running loss:0.07837976394221187 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 45, training loss: 0.0228 , running loss:0.0783180270344019 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 46, training loss: 0.0848 , running loss:0.07530137188732625 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 47, training loss: 0.1205 , running loss:0.07466062419116497 (0.96875, 0.875, 0.75)\n",
      "Epoch : 0 ,Iteration : 48, training loss: 0.0240 , running loss:0.06929944194853306 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 49, training loss: 0.0798 , running loss:0.07200738824903966 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 50, training loss: 0.0871 , running loss:0.07517065834254026 (0.984375, 0.875, 0.875)\n",
      "Validation loss : 0.07198354803809995   ,acc :  0.8983918128654972  ,f1-micro :  0.9862573099415204  ,f1-macro :  0.8986842105263158\n",
      "Epoch : 0 ,Iteration : 51, training loss: 0.0225 , running loss:0.07226131800562144 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 52, training loss: 0.0881 , running loss:0.0701758248731494 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 53, training loss: 0.0763 , running loss:0.06438930612057447 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 54, training loss: 0.0781 , running loss:0.0644996901974082 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 55, training loss: 0.0662 , running loss:0.06665677726268768 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 56, training loss: 0.0218 , running loss:0.0636698391288519 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 57, training loss: 0.0221 , running loss:0.06364755788818002 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 58, training loss: 0.0218 , running loss:0.06117723481729627 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 59, training loss: 0.0701 , running loss:0.060751227755099536 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 60, training loss: 0.0767 , running loss:0.06063491264358163 (0.984375, 0.875, 0.875)\n",
      "Validation loss : 0.07064265370891805   ,acc :  0.8991228070175439  ,f1-micro :  0.9863486842105263  ,f1-macro :  0.8978070175438596\n",
      "Epoch : 0 ,Iteration : 61, training loss: 0.0206 , running loss:0.06056093890219927 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 62, training loss: 0.0864 , running loss:0.06091695111244917 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 63, training loss: 0.0205 , running loss:0.058038254268467426 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 64, training loss: 0.0210 , running loss:0.05555644119158387 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 65, training loss: 0.1287 , running loss:0.060854781046509744 (0.96875, 0.75, 0.75)\n",
      "Epoch : 0 ,Iteration : 66, training loss: 0.0848 , running loss:0.06085459403693676 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 67, training loss: 0.0769 , running loss:0.05867327675223351 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 68, training loss: 0.0864 , running loss:0.061796015873551366 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 69, training loss: 0.0790 , running loss:0.061757204681634904 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 70, training loss: 0.2349 , running loss:0.06914490163326263 (0.9375, 0.625, 0.625)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss : 0.07033562038682009   ,acc :  0.8991228070175439  ,f1-micro :  0.9863486842105263  ,f1-macro :  0.8969298245614035\n",
      "Epoch : 0 ,Iteration : 71, training loss: 0.0736 , running loss:0.07170093692839145 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 72, training loss: 0.0868 , running loss:0.07163555324077606 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 73, training loss: 0.0202 , running loss:0.06882689092308283 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 74, training loss: 0.0209 , running loss:0.06596499318256974 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 75, training loss: 0.0205 , running loss:0.06368089141324162 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 76, training loss: 0.0202 , running loss:0.06360357459634543 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 77, training loss: 0.0206 , running loss:0.06352738104760647 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 78, training loss: 0.0201 , running loss:0.06344344280660152 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 79, training loss: 0.2138 , running loss:0.07062900997698307 (0.953125, 0.75, 0.75)\n",
      "Epoch : 0 ,Iteration : 80, training loss: 0.0864 , running loss:0.07111598178744316 (0.984375, 0.875, 0.875)\n",
      "Validation loss : 0.07049139045309602   ,acc :  0.8983918128654972  ,f1-micro :  0.9862573099415204  ,f1-macro :  0.8986842105263158\n",
      "Epoch : 0 ,Iteration : 81, training loss: 0.0710 , running loss:0.07363860923796892 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 82, training loss: 0.2097 , running loss:0.07980256732553244 (0.953125, 0.625, 0.75)\n",
      "Epoch : 0 ,Iteration : 83, training loss: 0.0195 , running loss:0.07975417450070381 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 84, training loss: 0.0755 , running loss:0.08247815640643238 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 85, training loss: 0.0196 , running loss:0.0770198936574161 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 86, training loss: 0.1399 , running loss:0.0797785053960979 (0.96875, 0.875, 0.75)\n",
      "Epoch : 0 ,Iteration : 87, training loss: 0.2496 , running loss:0.08841325109824538 (0.9375, 0.625, 0.625)\n",
      "Epoch : 0 ,Iteration : 88, training loss: 0.0756 , running loss:0.08787064207717776 (0.984375, 0.875, 0.875)\n",
      "Epoch : 0 ,Iteration : 89, training loss: 0.0193 , running loss:0.0848836143501103 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 90, training loss: 0.1297 , running loss:0.0796260011382401 (0.96875, 0.75, 0.75)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6746/2078659977.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6746/2078659977.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(base_model, train_dataloader, optimizer, loss_function)\u001b[0m\n\u001b[1;32m     63\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                         \u001b[0mval_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                         \u001b[0mf1_micro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "def train(base_model,train_dataloader,optimizer,loss_function):\n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    valid_interval = 10  # Perform validation and save model every 10 iterations\n",
    "    iteration = 0\n",
    "    \n",
    "    stop_criterion = 2000000\n",
    "\n",
    "    running_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        base_model.train()  # Set the model to training mode\n",
    "        for curr_batch in train_dataloader:\n",
    "            \n",
    "            if iteration > stop_criterion:\n",
    "                break\n",
    "            \n",
    "            input_ids = curr_batch['input_ids'].to(device)\n",
    "            attention_mask = curr_batch['attention_mask'].to(device)\n",
    "            targets = curr_batch['multi_label'].to(device)\n",
    "\n",
    "\n",
    "            outputs = base_model(input_ids,attention_mask)\n",
    "            loss = loss_function(outputs.to(device), targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #print (iteration)\n",
    "            running_loss.append(loss.item())\n",
    "            if len(running_loss) > 20:\n",
    "                running_loss.pop(0)\n",
    "            #print (f\"Epoch : {epoch} ,Iteration : {iteration}, training loss: {loss:.4f} , running loss:{sum(running_loss)/len(running_loss)}\")            \n",
    "            print (f\"Epoch : {epoch} ,Iteration : {iteration}, training loss: {loss:.4f} , running loss:{sum(running_loss)/len(running_loss)}\",find_metrics(targets,outputs))\n",
    "            \n",
    "            # freeing up excess memory\n",
    "            del loss, outputs\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Validation and model saving\n",
    "            if iteration % valid_interval == 0:\n",
    "                base_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    total_loss = []\n",
    "                    f1_micro = []\n",
    "                    f1_macro = []\n",
    "                    f1_avg = []\n",
    "                    for val_batch in val_dataloader:\n",
    "                        val_input_ids = val_batch['input_ids'].to(device)\n",
    "                        val_attention_mask = val_batch['attention_mask'].to(device)\n",
    "                        val_targets = val_batch['multi_label'].to(device)\n",
    "\n",
    "\n",
    "                        outputs = base_model(val_input_ids,val_attention_mask)\n",
    "                        loss = loss_function(outputs.to(device), val_targets)                        \n",
    "                        \n",
    "                        total_loss.append(loss.item())\n",
    "                        val_out = find_metrics(val_targets,outputs)\n",
    "                        f1_micro.append(val_out[0])\n",
    "                        f1_macro.append(val_out[1])\n",
    "                        f1_avg.append(val_out[2])\n",
    "                        \n",
    "                        # emptying memory\n",
    "                        del val_out, loss, outputs\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                    avg_acc = sum(f1_avg)/len(f1_avg)\n",
    "                    avg_f1mic = sum(f1_micro)/len(f1_micro)\n",
    "                    avg_f1mac = sum(f1_macro)/len(f1_macro)\n",
    "                    avg_loss = sum(total_loss)/len(total_loss)\n",
    "                    print (f\"Validation loss : {sum(total_loss)/len(total_loss)} \", ' ,acc : ',avg_acc,\" ,f1-micro : \",avg_f1mic,\" ,f1-macro : \",avg_f1mac)\n",
    "                    torch.save(base_model.state_dict(),f\"model_trained/model_{iteration}.pth\")\n",
    "                    #wandb.log({\"Validation Loss\": sum(total_loss)/len(total_loss)})\n",
    "                    del total_loss, f1_micro, f1_macro, f1_avg\n",
    "\n",
    "                base_model.train()  # Set the model back to training mode\n",
    "            \n",
    "            iteration += 1\n",
    "    return base_model, train_dataloader, optimizer, loss_function\n",
    "\n",
    "base_model, train_dataloader, optimizer, loss_function = train(base_model,train_dataloader,optimizer,loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a758e8a",
   "metadata": {},
   "source": [
    "### Find out memory leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8ead40ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7efb7a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337545\n",
      "<class 'torch.Tensor'> torch.Size([0])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([30522, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([512, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([2, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([8])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([8, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.Tensor'> torch.Size([1, 512])\n",
      "<class 'torch.Tensor'> torch.Size([1, 512])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768])\n",
      "<class 'torch.Tensor'> torch.Size([1, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 12, 64, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 12, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 12, 64, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 12, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 12, 64, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 12, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 12, 64, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 12, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 12, 64, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 12, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 12, 64, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 12, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 12, 64, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 12, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 12, 64, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 12, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 3072])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 8])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 8])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64])\n",
      "<class 'torch.Tensor'> torch.Size([1, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 1, 1, 64])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 64, 768])\n",
      "<class 'torch.Tensor'> torch.Size([64, 12, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# prints currently alive Tensors and Variables\n",
    "import torch\n",
    "import gc\n",
    "print (len(gc.get_objects()))\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99e95409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local variables:\n",
      "__name__: 57 bytes\n",
      "__doc__: 113 bytes\n",
      "__package__: 16 bytes\n",
      "__loader__: 16 bytes\n",
      "__spec__: 16 bytes\n",
      "__builtin__: 72 bytes\n",
      "__builtins__: 72 bytes\n",
      "_ih: 312 bytes\n",
      "_oh: 232 bytes\n",
      "_dh: 64 bytes\n",
      "In: 312 bytes\n",
      "Out: 232 bytes\n",
      "get_ipython: 64 bytes\n",
      "exit: 48 bytes\n",
      "quit: 48 bytes\n",
      "_: 49 bytes\n",
      "__: 49 bytes\n",
      "___: 49 bytes\n",
      "_i: 385 bytes\n",
      "_ii: 341 bytes\n",
      "_iii: 99 bytes\n",
      "_i1: 1634 bytes\n",
      "np: 72 bytes\n",
      "torch: 72 bytes\n",
      "nn: 72 bytes\n",
      "optim: 72 bytes\n",
      "Dataset: 1064 bytes\n",
      "DataLoader: 1472 bytes\n",
      "BertTokenizer: 2008 bytes\n",
      "dts: 72 bytes\n",
      "pd: 72 bytes\n",
      "sns: 72 bytes\n",
      "plt: 72 bytes\n",
      "nltk: 72 bytes\n",
      "re: 72 bytes\n",
      "gc: 72 bytes\n",
      "stopwords: 48 bytes\n",
      "CountVectorizer: 1064 bytes\n",
      "TfidfVectorizer: 1064 bytes\n",
      "WordCloud: 1064 bytes\n",
      "STOPWORDS: 8408 bytes\n",
      "SnowballStemmer: 1064 bytes\n",
      "train_test_split: 136 bytes\n",
      "TfidfTransformer: 1064 bytes\n",
      "MultinomialNB: 1064 bytes\n",
      "OneVsRestClassifier: 1064 bytes\n",
      "LinearSVC: 1064 bytes\n",
      "LogisticRegression: 1064 bytes\n",
      "Pipeline: 1064 bytes\n",
      "MultiLabelBinarizer: 1064 bytes\n",
      "BinaryRelevance: 1064 bytes\n",
      "ClassifierChain: 1064 bytes\n",
      "TomekLinks: 1064 bytes\n",
      "RandomUnderSampler: 1064 bytes\n",
      "BertModel: 1472 bytes\n",
      "f1_score: 136 bytes\n",
      "hamming_loss: 136 bytes\n",
      "accuracy_score: 136 bytes\n",
      "_i2: 100 bytes\n",
      "dataset: 248 bytes\n",
      "_i3: 267 bytes\n",
      "train_dataset: 1606340 bytes\n",
      "val_dataset: 681180 bytes\n",
      "test_dataset: 474769 bytes\n",
      "stop_words: 1488 bytes\n",
      "_i4: 5411 bytes\n",
      "definitions: 360 bytes\n",
      "entailment_con: 64 bytes\n",
      "_i5: 805 bytes\n",
      "BERTClassifier: 1472 bytes\n",
      "learning_rate: 24 bytes\n",
      "num_classes: 28 bytes\n",
      "base_model: 48 bytes\n",
      "loss_function: 48 bytes\n",
      "optimizer: 48 bytes\n",
      "_i6: 1494 bytes\n",
      "CustomDataset: 1064 bytes\n",
      "tokenizer: 48 bytes\n",
      "batch_size: 28 bytes\n",
      "train_custom: 48 bytes\n",
      "train_dataloader: 48 bytes\n",
      "valid_custom: 48 bytes\n",
      "valid_dataloader: 48 bytes\n",
      "_i7: 1872 bytes\n",
      "device: 24 bytes\n",
      "train: 136 bytes\n",
      "_i8: 99 bytes\n",
      "_i9: 783 bytes\n",
      "_i10: 1494 bytes\n",
      "_i11: 1872 bytes\n",
      "_i12: 1516 bytes\n",
      "_i13: 1872 bytes\n",
      "_i14: 765 bytes\n",
      "_i15: 1516 bytes\n",
      "_i16: 1872 bytes\n",
      "_i17: 1494 bytes\n",
      "_i18: 1872 bytes\n",
      "_i19: 341 bytes\n",
      "obj: 232 bytes\n",
      "_i20: 1876 bytes\n",
      "_i21: 99 bytes\n",
      "_i22: 341 bytes\n",
      "_i23: 385 bytes\n",
      "sys: 72 bytes\n",
      "name: 52 bytes\n",
      "value: 52 bytes\n",
      "_i24: 385 bytes\n",
      "Global variables:\n",
      "__name__: 57 bytes\n",
      "__doc__: 113 bytes\n",
      "__package__: 16 bytes\n",
      "__loader__: 16 bytes\n",
      "__spec__: 16 bytes\n",
      "__builtin__: 72 bytes\n",
      "__builtins__: 72 bytes\n",
      "_ih: 312 bytes\n",
      "_oh: 232 bytes\n",
      "_dh: 64 bytes\n",
      "In: 312 bytes\n",
      "Out: 232 bytes\n",
      "get_ipython: 64 bytes\n",
      "exit: 48 bytes\n",
      "quit: 48 bytes\n",
      "_: 49 bytes\n",
      "__: 49 bytes\n",
      "___: 49 bytes\n",
      "_i: 385 bytes\n",
      "_ii: 341 bytes\n",
      "_iii: 99 bytes\n",
      "_i1: 1634 bytes\n",
      "np: 72 bytes\n",
      "torch: 72 bytes\n",
      "nn: 72 bytes\n",
      "optim: 72 bytes\n",
      "Dataset: 1064 bytes\n",
      "DataLoader: 1472 bytes\n",
      "BertTokenizer: 2008 bytes\n",
      "dts: 72 bytes\n",
      "pd: 72 bytes\n",
      "sns: 72 bytes\n",
      "plt: 72 bytes\n",
      "nltk: 72 bytes\n",
      "re: 72 bytes\n",
      "gc: 72 bytes\n",
      "stopwords: 48 bytes\n",
      "CountVectorizer: 1064 bytes\n",
      "TfidfVectorizer: 1064 bytes\n",
      "WordCloud: 1064 bytes\n",
      "STOPWORDS: 8408 bytes\n",
      "SnowballStemmer: 1064 bytes\n",
      "train_test_split: 136 bytes\n",
      "TfidfTransformer: 1064 bytes\n",
      "MultinomialNB: 1064 bytes\n",
      "OneVsRestClassifier: 1064 bytes\n",
      "LinearSVC: 1064 bytes\n",
      "LogisticRegression: 1064 bytes\n",
      "Pipeline: 1064 bytes\n",
      "MultiLabelBinarizer: 1064 bytes\n",
      "BinaryRelevance: 1064 bytes\n",
      "ClassifierChain: 1064 bytes\n",
      "TomekLinks: 1064 bytes\n",
      "RandomUnderSampler: 1064 bytes\n",
      "BertModel: 1472 bytes\n",
      "f1_score: 136 bytes\n",
      "hamming_loss: 136 bytes\n",
      "accuracy_score: 136 bytes\n",
      "_i2: 100 bytes\n",
      "dataset: 248 bytes\n",
      "_i3: 267 bytes\n",
      "train_dataset: 1606340 bytes\n",
      "val_dataset: 681180 bytes\n",
      "test_dataset: 474769 bytes\n",
      "stop_words: 1488 bytes\n",
      "_i4: 5411 bytes\n",
      "definitions: 360 bytes\n",
      "entailment_con: 64 bytes\n",
      "_i5: 805 bytes\n",
      "BERTClassifier: 1472 bytes\n",
      "learning_rate: 24 bytes\n",
      "num_classes: 28 bytes\n",
      "base_model: 48 bytes\n",
      "loss_function: 48 bytes\n",
      "optimizer: 48 bytes\n",
      "_i6: 1494 bytes\n",
      "CustomDataset: 1064 bytes\n",
      "tokenizer: 48 bytes\n",
      "batch_size: 28 bytes\n",
      "train_custom: 48 bytes\n",
      "train_dataloader: 48 bytes\n",
      "valid_custom: 48 bytes\n",
      "valid_dataloader: 48 bytes\n",
      "_i7: 1872 bytes\n",
      "device: 24 bytes\n",
      "train: 136 bytes\n",
      "_i8: 99 bytes\n",
      "_i9: 783 bytes\n",
      "_i10: 1494 bytes\n",
      "_i11: 1872 bytes\n",
      "_i12: 1516 bytes\n",
      "_i13: 1872 bytes\n",
      "_i14: 765 bytes\n",
      "_i15: 1516 bytes\n",
      "_i16: 1872 bytes\n",
      "_i17: 1494 bytes\n",
      "_i18: 1872 bytes\n",
      "_i19: 341 bytes\n",
      "obj: 232 bytes\n",
      "_i20: 1876 bytes\n",
      "_i21: 99 bytes\n",
      "_i22: 341 bytes\n",
      "_i23: 385 bytes\n",
      "sys: 72 bytes\n",
      "name: 52 bytes\n",
      "value: 52 bytes\n",
      "_i24: 385 bytes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Print local variable names with memory space\n",
    "print(\"Local variables:\")\n",
    "for name, value in locals().items():\n",
    "    print(f\"{name}: {sys.getsizeof(value)} bytes\")\n",
    "\n",
    "# Print global variable names with memory space\n",
    "print(\"Global variables:\")\n",
    "for name, value in globals().items():\n",
    "    print(f\"{name}: {sys.getsizeof(value)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd7e867",
   "metadata": {},
   "source": [
    "### Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "330fbdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_it(curr):\n",
    "    return [curr]\n",
    "\n",
    "train_dataset['text_2d'] = train_dataset[\"text\"].apply(list_it)\n",
    "train_dataset['str_labels'] = train_dataset[\"labels\"].apply(str)\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(np.array(train_dataset[\"text_2d\"]).reshape(-1, 1),train_dataset['str_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6d12e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delist_it(curr):\n",
    "    new_list = []\n",
    "    for i in curr:\n",
    "        new_list.append(i[0][0])\n",
    "    return new_list\n",
    "def str2list(curr):\n",
    "    if curr==\"[]\":\n",
    "        return []\n",
    "    else:\n",
    "        return [int(x) for x in curr[1:-1].split(',')]\n",
    "x_train_weight = {}\n",
    "x_train_weight[\"text\"] = delist_it(X_train_resampled)\n",
    "x_train_weight[\"labels\"] = y_train_resampled.apply(str2list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cb973ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_custom_w = CustomDataset(x_train_weight, num_classes,tokenizer)\n",
    "train_dataloader_w = DataLoader(train_custom_w, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "032547cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 0, training loss: 0.7228 , running loss:0.7227567434310913 (0.53125, 0.23402777777777778, 0.0)\n",
      "Validation loss : 0.6665522667399624   ,acc :  0.016228070175438595  ,f1-micro :  0.764437134502924  ,f1-macro :  0.41703390420495645\n",
      "Epoch : 0 ,Iteration : 1, training loss: 0.6695 , running loss:0.696148693561554 (0.703125, 0.22499999999999998, 0.0)\n",
      "Epoch : 0 ,Iteration : 2, training loss: 0.5594 , running loss:0.6505704720815023 (0.796875, 0.3409090909090909, 0.125)\n",
      "Epoch : 0 ,Iteration : 3, training loss: 0.5836 , running loss:0.6338293254375458 (0.8125, 0.5056818181818181, 0.125)\n",
      "Epoch : 0 ,Iteration : 4, training loss: 0.5987 , running loss:0.6268131971359253 (0.78125, 0.425, 0.125)\n",
      "Epoch : 0 ,Iteration : 5, training loss: 0.5837 , running loss:0.6196288466453552 (0.8125, 0.6333333333333333, 0.25)\n",
      "Epoch : 0 ,Iteration : 6, training loss: 0.5572 , running loss:0.6107116256441388 (0.8125, 0.425, 0.25)\n",
      "Epoch : 0 ,Iteration : 7, training loss: 0.5589 , running loss:0.6042363345623016 (0.765625, 0.3181818181818182, 0.25)\n",
      "Epoch : 0 ,Iteration : 8, training loss: 0.5040 , running loss:0.5930991503927443 (0.828125, 0.7583333333333333, 0.25)\n",
      "Epoch : 0 ,Iteration : 9, training loss: 0.5618 , running loss:0.589965945482254 (0.734375, 0.25, 0.0)\n",
      "Epoch : 0 ,Iteration : 10, training loss: 0.5289 , running loss:0.5844146392562173 (0.78125, 0.4305555555555556, 0.125)\n",
      "Validation loss : 0.44426877446341934   ,acc :  0.03508771929824561  ,f1-micro :  0.86875  ,f1-macro :  0.7959531720715923\n",
      "Epoch : 0 ,Iteration : 11, training loss: 0.4819 , running loss:0.5758753071228663 (0.84375, 0.5681818181818181, 0.375)\n",
      "Epoch : 0 ,Iteration : 12, training loss: 0.4721 , running loss:0.56789136162171 (0.875, 0.5833333333333333, 0.375)\n",
      "Epoch : 0 ,Iteration : 13, training loss: 0.5131 , running loss:0.5639801898172924 (0.8125, 0.4375, 0.125)\n",
      "Epoch : 0 ,Iteration : 14, training loss: 0.4743 , running loss:0.5580031077067057 (0.875, 0.4583333333333333, 0.125)\n",
      "Epoch : 0 ,Iteration : 15, training loss: 0.4835 , running loss:0.553347036242485 (0.84375, 0.5625, 0.0)\n",
      "Epoch : 0 ,Iteration : 16, training loss: 0.4375 , running loss:0.5465308760895449 (0.875, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 17, training loss: 0.4232 , running loss:0.5396791746219 (0.875, 0.5, 0.0)\n",
      "Epoch : 0 ,Iteration : 18, training loss: 0.4203 , running loss:0.5333939806411141 (0.875, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 19, training loss: 0.4465 , running loss:0.529047779738903 (0.859375, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 20, training loss: 0.4430 , running loss:0.5150609269738198 (0.875, 0.375, 0.0)\n",
      "Validation loss : 0.3328185340814423   ,acc :  0.8929824561403509  ,f1-micro :  0.9855811403508772  ,f1-macro :  0.8921052631578947\n",
      "Epoch : 0 ,Iteration : 21, training loss: 0.4085 , running loss:0.5020091503858566 (0.859375, 0.5, 0.0)\n",
      "Epoch : 0 ,Iteration : 22, training loss: 0.4423 , running loss:0.496154373884201 (0.828125, 0.5, 0.0)\n",
      "Epoch : 0 ,Iteration : 23, training loss: 0.4094 , running loss:0.48744494915008546 (0.859375, 0.5, 0.0)\n",
      "Epoch : 0 ,Iteration : 24, training loss: 0.4258 , running loss:0.47879658639431 (0.84375, 0.25, 0.0)\n",
      "Epoch : 0 ,Iteration : 25, training loss: 0.3861 , running loss:0.4689186066389084 (0.875, 0.5, 0.0)\n",
      "Epoch : 0 ,Iteration : 26, training loss: 0.4797 , running loss:0.4650442063808441 (0.859375, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 27, training loss: 0.3769 , running loss:0.45594397485256194 (0.890625, 0.4375, 0.125)\n",
      "Epoch : 0 ,Iteration : 28, training loss: 0.3852 , running loss:0.45000589191913604 (0.859375, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 29, training loss: 0.3910 , running loss:0.44146682769060136 (0.875, 0.125, 0.0)\n",
      "Epoch : 0 ,Iteration : 30, training loss: 0.3807 , running loss:0.4340549409389496 (0.859375, 0.5, 0.0)\n",
      "Validation loss : 0.26577516177244354   ,acc :  0.8978070175438596  ,f1-micro :  0.9861842105263158  ,f1-macro :  0.8969298245614035\n",
      "Epoch : 0 ,Iteration : 31, training loss: 0.3811 , running loss:0.4290143668651581 (0.875, 0.625, 0.125)\n",
      "Epoch : 0 ,Iteration : 32, training loss: 0.4049 , running loss:0.42565408200025556 (0.875, 0.25, 0.0)\n",
      "Epoch : 0 ,Iteration : 33, training loss: 0.3225 , running loss:0.4161200672388077 (0.890625, 0.625, 0.125)\n",
      "Epoch : 0 ,Iteration : 34, training loss: 0.3835 , running loss:0.4115790009498596 (0.84375, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 35, training loss: 0.4180 , running loss:0.40830221474170686 (0.859375, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 36, training loss: 0.3334 , running loss:0.4031008809804916 (0.875, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 37, training loss: 0.4075 , running loss:0.4023150116205215 (0.828125, 0.25, 0.0)\n",
      "Epoch : 0 ,Iteration : 38, training loss: 0.3670 , running loss:0.3996538072824478 (0.84375, 0.625, 0.0)\n",
      "Epoch : 0 ,Iteration : 39, training loss: 0.4001 , running loss:0.39733706712722777 (0.84375, 0.25, 0.0)\n",
      "Epoch : 0 ,Iteration : 40, training loss: 0.3075 , running loss:0.39055938422679903 (0.875, 0.625, 0.0)\n",
      "Validation loss : 0.2274557492712088   ,acc :  0.8991228070175439  ,f1-micro :  0.9863486842105263  ,f1-macro :  0.8973684210526316\n",
      "Epoch : 0 ,Iteration : 41, training loss: 0.4504 , running loss:0.3926540330052376 (0.859375, 0.25, 0.0)\n",
      "Epoch : 0 ,Iteration : 42, training loss: 0.3625 , running loss:0.3886616572737694 (0.84375, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 43, training loss: 0.3602 , running loss:0.3862014338374138 (0.875, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 44, training loss: 0.3423 , running loss:0.3820288136601448 (0.875, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 45, training loss: 0.3587 , running loss:0.38065786808729174 (0.875, 0.25, 0.0)\n",
      "Epoch : 0 ,Iteration : 46, training loss: 0.3949 , running loss:0.37641611546278 (0.859375, 0.125, 0.0)\n",
      "Epoch : 0 ,Iteration : 47, training loss: 0.3710 , running loss:0.3761199712753296 (0.828125, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 48, training loss: 0.3573 , running loss:0.3747253954410553 (0.859375, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 49, training loss: 0.3156 , running loss:0.3709559187293053 (0.859375, 0.5, 0.0)\n",
      "Epoch : 0 ,Iteration : 50, training loss: 0.2887 , running loss:0.36635940670967104 (0.90625, 0.6875, 0.25)\n",
      "Validation loss : 0.2153221103705858   ,acc :  0.9  ,f1-micro :  0.9864583333333333  ,f1-macro :  0.8967836257309942\n",
      "Epoch : 0 ,Iteration : 51, training loss: 0.3470 , running loss:0.364653417468071 (0.859375, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 52, training loss: 0.4139 , running loss:0.36510435342788694 (0.84375, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 53, training loss: 0.4097 , running loss:0.3694689080119133 (0.828125, 0.125, 0.0)\n",
      "Epoch : 0 ,Iteration : 54, training loss: 0.4195 , running loss:0.3712689861655235 (0.859375, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 55, training loss: 0.2803 , running loss:0.36438513100147246 (0.90625, 0.6, 0.25)\n",
      "Epoch : 0 ,Iteration : 56, training loss: 0.3545 , running loss:0.3654380068182945 (0.84375, 0.25, 0.0)\n",
      "Epoch : 0 ,Iteration : 57, training loss: 0.3228 , running loss:0.3612041980028152 (0.875, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 58, training loss: 0.3163 , running loss:0.35866607129573824 (0.84375, 0.5, 0.0)\n",
      "Epoch : 0 ,Iteration : 59, training loss: 0.2913 , running loss:0.3532264858484268 (0.90625, 0.6, 0.25)\n",
      "Epoch : 0 ,Iteration : 60, training loss: 0.3365 , running loss:0.35467880964279175 (0.859375, 0.25, 0.0)\n",
      "Validation loss : 0.19427156547705332   ,acc :  0.8847953216374269  ,f1-micro :  0.9845577485380116  ,f1-macro :  0.8874269005847953\n",
      "Epoch : 0 ,Iteration : 61, training loss: 0.3350 , running loss:0.3489090710878372 (0.875, 0.5, 0.0)\n",
      "Epoch : 0 ,Iteration : 62, training loss: 0.3509 , running loss:0.3483323141932487 (0.859375, 0.125, 0.0)\n",
      "Epoch : 0 ,Iteration : 63, training loss: 0.3184 , running loss:0.3462400421500206 (0.875, 0.3125, 0.125)\n",
      "Epoch : 0 ,Iteration : 64, training loss: 0.3217 , running loss:0.34520977586507795 (0.859375, 0.4375, 0.125)\n",
      "Epoch : 0 ,Iteration : 65, training loss: 0.2770 , running loss:0.3411242440342903 (0.890625, 0.675, 0.125)\n",
      "Epoch : 0 ,Iteration : 66, training loss: 0.2869 , running loss:0.33572306483983994 (0.90625, 0.5714285714285714, 0.25)\n",
      "Epoch : 0 ,Iteration : 67, training loss: 0.3420 , running loss:0.3342745780944824 (0.859375, 0.5, 0.0)\n",
      "Epoch : 0 ,Iteration : 68, training loss: 0.2989 , running loss:0.3313536524772644 (0.875, 0.4375, 0.125)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 69, training loss: 0.3168 , running loss:0.33141190111637114 (0.90625, 0.375, 0.25)\n",
      "Epoch : 0 ,Iteration : 70, training loss: 0.2569 , running loss:0.32981830537319184 (0.875, 0.6666666666666667, 0.125)\n",
      "Validation loss : 0.19155651388461128   ,acc :  0.8747076023391812  ,f1-micro :  0.9832419590643274  ,f1-macro :  0.8840643274853802\n",
      "Epoch : 0 ,Iteration : 71, training loss: 0.2903 , running loss:0.3269809141755104 (0.875, 0.4375, 0.125)\n",
      "Epoch : 0 ,Iteration : 72, training loss: 0.3663 , running loss:0.32460304349660873 (0.875, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 73, training loss: 0.3073 , running loss:0.3194793462753296 (0.875, 0.5625, 0.125)\n",
      "Epoch : 0 ,Iteration : 74, training loss: 0.3051 , running loss:0.31375869959592817 (0.875, 0.35, 0.25)\n",
      "Epoch : 0 ,Iteration : 75, training loss: 0.3889 , running loss:0.31918873339891435 (0.859375, 0.25, 0.0)\n",
      "Epoch : 0 ,Iteration : 76, training loss: 0.2957 , running loss:0.3162463799118996 (0.875, 0.375, 0.0)\n",
      "Epoch : 0 ,Iteration : 77, training loss: 0.3387 , running loss:0.31704258024692533 (0.875, 0.35, 0.25)\n",
      "Epoch : 0 ,Iteration : 78, training loss: 0.3852 , running loss:0.3204907447099686 (0.8571428571428571, 0.375, 0.0)\n",
      "Epoch : 1 ,Iteration : 79, training loss: 0.3246 , running loss:0.3221513956785202 (0.859375, 0.5, 0.125)\n",
      "Epoch : 1 ,Iteration : 80, training loss: 0.2758 , running loss:0.31911509931087495 (0.90625, 0.6625, 0.375)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6746/1623222666.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader_w\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6746/1623222666.py\u001b[0m in \u001b[0;36mtrain_w\u001b[0;34m(base_model, train_dataloader, optimizer, loss_function)\u001b[0m\n\u001b[1;32m     71\u001b[0m                         \u001b[0;31m# emptying memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                         \u001b[0;32mdel\u001b[0m \u001b[0mval_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "def train_w(base_model,train_dataloader,optimizer,loss_function):\n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    valid_interval = 10  # Perform validation and save model every 10 iterations\n",
    "    iteration = 0\n",
    "    \n",
    "    stop_criterion = 2000000\n",
    "\n",
    "    running_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        base_model.train()  # Set the model to training mode\n",
    "        for curr_batch in train_dataloader:\n",
    "            \n",
    "            if iteration > stop_criterion:\n",
    "                break\n",
    "            \n",
    "            input_ids = curr_batch['input_ids'].to(device)\n",
    "            attention_mask = curr_batch['attention_mask'].to(device)\n",
    "            targets = curr_batch['multi_label'].to(device)\n",
    "\n",
    "\n",
    "            outputs = base_model(input_ids,attention_mask)\n",
    "            loss = loss_function(outputs.to(device), targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #print (iteration)\n",
    "            running_loss.append(loss.item())\n",
    "            if len(running_loss) > 20:\n",
    "                running_loss.pop(0)\n",
    "            #print (f\"Epoch : {epoch} ,Iteration : {iteration}, training loss: {loss:.4f} , running loss:{sum(running_loss)/len(running_loss)}\")            \n",
    "            print (f\"Epoch : {epoch} ,Iteration : {iteration}, training loss: {loss:.4f} , running loss:{sum(running_loss)/len(running_loss)}\",find_metrics(targets,outputs))\n",
    "            \n",
    "            # freeing up excess memory\n",
    "            del loss, outputs\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Validation and model saving\n",
    "            if iteration % valid_interval == 0:\n",
    "                base_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    total_loss = []\n",
    "                    f1_micro = []\n",
    "                    f1_macro = []\n",
    "                    f1_avg = []\n",
    "                    for val_batch in val_dataloader:\n",
    "                        val_input_ids = val_batch['input_ids'].to(device)\n",
    "                        val_attention_mask = val_batch['attention_mask'].to(device)\n",
    "                        val_targets = val_batch['multi_label'].to(device)\n",
    "\n",
    "\n",
    "                        outputs = base_model(val_input_ids,val_attention_mask)\n",
    "                        loss = loss_function(outputs.to(device), val_targets)                        \n",
    "                        \n",
    "                        total_loss.append(loss.item())\n",
    "                        val_out = find_metrics(val_targets,outputs)\n",
    "                        f1_micro.append(val_out[0])\n",
    "                        f1_macro.append(val_out[1])\n",
    "                        f1_avg.append(val_out[2])\n",
    "                        \n",
    "                        # emptying memory\n",
    "                        del val_out, loss, outputs\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                    avg_acc = sum(f1_avg)/len(f1_avg)\n",
    "                    avg_f1mic = sum(f1_micro)/len(f1_micro)\n",
    "                    avg_f1mac = sum(f1_macro)/len(f1_macro)\n",
    "                    avg_loss = sum(total_loss)/len(total_loss)\n",
    "                    print (f\"Validation loss : {sum(total_loss)/len(total_loss)} \", ' ,acc : ',avg_acc,\" ,f1-micro : \",avg_f1mic,\" ,f1-macro : \",avg_f1mac)\n",
    "                    torch.save(base_model.state_dict(),f\"model_trained/model_w_{iteration}.pth\")\n",
    "                    #wandb.log({\"Validation Loss\": sum(total_loss)/len(total_loss)})\n",
    "                    del total_loss, f1_micro, f1_macro, f1_avg\n",
    "\n",
    "                base_model.train()  # Set the model back to training mode\n",
    "            \n",
    "            iteration += 1\n",
    "    return base_model, train_dataloader, optimizer, loss_function\n",
    "\n",
    "base_model, train_dataloader_w, optimizer, loss_function = train_w(base_model,train_dataloader_w ,optimizer,loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb13bce",
   "metadata": {},
   "source": [
    "### Computing metrics from the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7323d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics for normal dataset\n",
    "# computer metrics for the entire dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
