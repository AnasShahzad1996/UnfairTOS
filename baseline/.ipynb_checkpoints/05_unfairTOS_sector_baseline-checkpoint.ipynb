{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "358ec1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------# Import libraries and datasets #------#\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import datasets as dts\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import gc\n",
    "import random\n",
    "import spacy\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "from transformers import BertModel,AutoModel\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05714b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(dir_sent,dir_labs,dir_sec, files):\n",
    "    new_data = {\"text\":[],\"labels\":[],\"sector\":[]}\n",
    "    \n",
    "    # sector files\n",
    "    fin_sec    = (open(dir_sec+\"/\"+\"LIST_FINANCE.txt\", 'r').read()).split(\"\\n\")\n",
    "    game_sec   = (open(dir_sec+\"/\"+\"LIST_GAMES.txt\", 'r').read()).split(\"\\n\")\n",
    "    heal_sec   = (open(dir_sec+\"/\"+\"LIST_HEALTH.txt\", 'r').read()).split(\"\\n\")\n",
    "    travel_sec = (open(dir_sec+\"/\"+\"LIST_TRAVEL.txt\", 'r').read()).split(\"\\n\")\n",
    "    other_sec  = (open(dir_sec+\"/\"+\"LIST_OTHERS.txt\", 'r').read()).split(\"\\n\")\n",
    "    \n",
    "    \n",
    "    for i in files:\n",
    "        doc_content = None\n",
    "        with open(dir_sent+\"/\"+i, 'r') as file:\n",
    "            doc_content = file.read()\n",
    "        doc_sentences = doc_content.split(\"\\n\")\n",
    "        new_data[\"text\"] = new_data[\"text\"] + doc_sentences\n",
    "        \n",
    "        # append the text based on presence in the list\n",
    "        if i in fin_sec:\n",
    "            new_list = [\"FIN\"] * len(doc_sentences)\n",
    "            new_data[\"sector\"] = new_data[\"sector\"] + new_list\n",
    "        elif i in game_sec:\n",
    "            new_list = [\"GAME\"] * len(doc_sentences)\n",
    "            new_data[\"sector\"] = new_data[\"sector\"] + new_list\n",
    "        elif i in heal_sec:\n",
    "            new_list = [\"HEALTH\"] * len(doc_sentences)\n",
    "            new_data[\"sector\"] = new_data[\"sector\"] + new_list\n",
    "        elif i in travel_sec:\n",
    "            new_list = [\"TRAVEL\"] * len(doc_sentences)\n",
    "            new_data[\"sector\"] = new_data[\"sector\"] + new_list\n",
    "        elif i in other_sec:\n",
    "            new_list = [\"OTHER\"] * len(doc_sentences)\n",
    "            new_data[\"sector\"] = new_data[\"sector\"] + new_list\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        doc_content = None\n",
    "        with open(dir_labs+\"/\"+i, 'r') as file:\n",
    "            doc_content = file.read()\n",
    "        doc_sentences = doc_content.split(\"\\n\")\n",
    "        new_data[\"labels\"] = new_data[\"labels\"] + doc_sentences\n",
    "\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n",
    "def process_data(dir_sent,dir_labs,dir_sec):\n",
    "    \n",
    "    ratio = [0.6,0.2,0.2]\n",
    "    \n",
    "    file_sent = os.listdir(dir_sent)        \n",
    "    file_labs = os.listdir(dir_labs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_dataset = make_data(dir_sent,dir_labs,dir_sec, file_sent[0:int(ratio[0] * len(dir_sent))])\n",
    "    val_dataset = make_data(dir_sent,dir_labs,dir_sec, file_sent[int(ratio[0] * len(dir_sent)):int((ratio[0] + ratio[1]) * len(dir_sent))])\n",
    "    test_dataset = make_data(dir_sent,dir_labs,dir_sec, file_sent[int((ratio[0] + ratio[1]) * len(dir_sent)):len(dir_sent)])\n",
    "            \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "        \n",
    "\n",
    "train_dataset, val_dataset, test_dataset = process_data(\"../datasets/corpus_142_ToS/corpus/sentences\",\"../datasets/corpus_142_ToS/corpus/tags\",\"../datasets/corpus_142_ToS/corpus/lists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12925e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector2ind = {\n",
    "    \"FIN\" : 0,\n",
    "    \"GAME\" : 1,\n",
    "    \"HEALTH\" : 2,\n",
    "    \"TRAVEL\" : 3,\n",
    "    \"OTHER\" : 4\n",
    "}\n",
    "label2ind = {\n",
    "    \"a1\":[0,0],\n",
    "    \"a2\":[0,1],\n",
    "    \"a3\":[0,2],\n",
    "    \"ch1\":[1,0],\n",
    "    \"ch2\":[1,1],\n",
    "    \"ch3\":[1,2],\n",
    "    \"cr1\":[2,0],\n",
    "    \"cr2\":[2,1],\n",
    "    \"cr3\":[2,2],\n",
    "    \"j1\":[3,0],\n",
    "    \"j2\":[3,1],\n",
    "    \"j3\":[3,2],\n",
    "    \"law1\":[4,0],\n",
    "    \"law2\":[4,1],\n",
    "    \"law3\":[4,2],\n",
    "    \"ltd1\":[5,0],\n",
    "    \"ltd2\":[5,1],\n",
    "    \"ltd3\":[5,2],\n",
    "    \"ter1\":[6,0],\n",
    "    \"ter2\":[6,1],\n",
    "    \"ter3\":[6,2],\n",
    "    \"use1\":[7,0],\n",
    "    \"use2\":[7,1],\n",
    "    \"use3\":[7,2],\n",
    "    \"pinc1\":[8,0],\n",
    "    \"pinc2\":[8,1],\n",
    "    \"pinc3\":[8,2],\n",
    "}\n",
    "ent_con = [\"entails that\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55910e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset,num_classes,num_sector,tokenizer):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.texts = self.dataset[\"text\"]\n",
    "        self.labels = self.dataset[\"labels\"]\n",
    "        self.sectors = self.dataset[\"sector\"]\n",
    "        self.num_classes = num_classes\n",
    "        self.num_sector = num_sector\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = (self.labels[index]).split(\" \")\n",
    "        sec_label = self.sectors[index]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        new_label = []\n",
    "        for i in range(0,len(label)):\n",
    "            if len(label[i]) > 1:\n",
    "                new_label.append(label2ind[label[i]][0])\n",
    "        multi_label = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        multi_label[new_label] = 1\n",
    "        \n",
    "        multi_sec = torch.zeros(self.num_sector,dtype=torch.float32)\n",
    "        multi_sec[sector2ind[sec_label]] = 1\n",
    "\n",
    "        return {'input_ids':input_ids, 'attention_mask':attention_mask, 'multi_label':multi_label,'multi_sector':multi_sec}\n",
    "\n",
    "batch_size = 8\n",
    "num_classes = 9\n",
    "num_sector = 5\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_custom = CustomDataset(train_dataset, num_classes,num_sector,tokenizer)\n",
    "train_dataloader = DataLoader(train_custom, batch_size=batch_size, shuffle=True)\n",
    "valid_custom = CustomDataset(val_dataset, num_classes,num_sector,tokenizer)\n",
    "val_dataloader = DataLoader(valid_custom, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903412a2",
   "metadata": {},
   "source": [
    "## BASELINE MODEL: LABEL WISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "284dad14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTClassifier(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc): Linear(in_features=768, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _ , pooled_output = self.bert(input_ids=input_ids, attention_mask =attention_mask,return_dict=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)        \n",
    "        return logits\n",
    "\n",
    "learning_rate = 3e-5\n",
    "base_model = BERTClassifier(num_classes)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(base_model.parameters(), lr=learning_rate)\n",
    "print (base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "303bf620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_metrics(targets,prediction):\n",
    "    final_pred = ((torch.sigmoid(prediction) >= 0.5) * 1.0) \n",
    "    np_tar = targets.cpu().detach().numpy()\n",
    "    np_pred = final_pred.cpu().detach().numpy()\n",
    "    \n",
    "    avg_f1_mic = f1_score(np_tar.flatten(), np_pred.flatten(), average='micro',zero_division=0)\n",
    "    avg_f1_mac = f1_score(np_tar, np_pred, average='macro',zero_division=1)\n",
    "    avg_acc = accuracy_score(np_tar, np_pred)\n",
    "    del np_tar\n",
    "    del np_pred\n",
    "    del final_pred\n",
    "    return avg_f1_mic, avg_f1_mac, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c05509e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 0, training loss: 0.6838 , running loss:0.6838474273681641 (0.5972222222222222, 0.044444444444444446, 0.0)\n",
      "Validation loss : 0.6189284272207892   ,acc :  0.002565982404692082  ,f1-micro :  0.78030303030303  ,f1-macro :  0.43960289014052517\n",
      "Epoch : 0 ,Iteration : 1, training loss: 0.6159 , running loss:0.6498613059520721 (0.763888888888889, 0.2222222222222222, 0.0)\n",
      "Epoch : 0 ,Iteration : 2, training loss: 0.5573 , running loss:0.619013766447703 (0.8055555555555556, 0.5555555555555556, 0.0)\n",
      "Epoch : 0 ,Iteration : 3, training loss: 0.5544 , running loss:0.6028692275285721 (0.8472222222222222, 0.4444444444444444, 0.375)\n",
      "Epoch : 0 ,Iteration : 4, training loss: 0.4571 , running loss:0.5737228870391846 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 5, training loss: 0.4978 , running loss:0.5610752503077189 (0.9166666666666666, 0.5555555555555556, 0.625)\n",
      "Epoch : 0 ,Iteration : 6, training loss: 0.4423 , running loss:0.5441131464072636 (0.9444444444444444, 0.5555555555555556, 0.75)\n",
      "Epoch : 0 ,Iteration : 7, training loss: 0.4081 , running loss:0.527114275842905 (0.9305555555555556, 0.6666666666666666, 0.625)\n",
      "Epoch : 0 ,Iteration : 8, training loss: 0.3760 , running loss:0.5103259748882718 (0.9583333333333334, 0.7777777777777778, 0.625)\n",
      "Epoch : 0 ,Iteration : 9, training loss: 0.3618 , running loss:0.49547153115272524 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 10, training loss: 0.3001 , running loss:0.4777147986672141 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 11, training loss: 0.2929 , running loss:0.46231654783089954 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 12, training loss: 0.3286 , running loss:0.45202799943777233 (0.9583333333333334, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 13, training loss: 0.3016 , running loss:0.4412841243403299 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 14, training loss: 0.2459 , running loss:0.42825693686803185 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 15, training loss: 0.2648 , running loss:0.41804315336048603 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 16, training loss: 0.2349 , running loss:0.4072684102198657 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 17, training loss: 0.2158 , running loss:0.3966293152835634 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 18, training loss: 0.2116 , running loss:0.3868901180593591 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 19, training loss: 0.2204 , running loss:0.37856491655111313 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 20, training loss: 0.1826 , running loss:0.35350037291646 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.18579933947482066   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139447  ,f1-macro :  0.9244053437601831\n",
      "Epoch : 0 ,Iteration : 21, training loss: 0.1730 , running loss:0.3313573822379112 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 22, training loss: 0.1731 , running loss:0.31214488446712496 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 23, training loss: 0.2180 , running loss:0.29532425180077554 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 24, training loss: 0.1597 , running loss:0.2804548151791096 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 25, training loss: 0.2130 , running loss:0.2662131354212761 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 26, training loss: 0.1449 , running loss:0.251343559473753 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 27, training loss: 0.1462 , running loss:0.23824647292494774 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 28, training loss: 0.1354 , running loss:0.2262175790965557 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 29, training loss: 0.1688 , running loss:0.21657069772481918 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 30, training loss: 0.1588 , running loss:0.20950320810079576 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 31, training loss: 0.1203 , running loss:0.20087172016501426 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 32, training loss: 0.1191 , running loss:0.1903963517397642 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 33, training loss: 0.1197 , running loss:0.1813023030757904 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 34, training loss: 0.1949 , running loss:0.1787519633769989 (0.9583333333333334, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 35, training loss: 0.1103 , running loss:0.1710246030241251 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 36, training loss: 0.1048 , running loss:0.16452295742928982 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 37, training loss: 0.1013 , running loss:0.15879921913146972 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 38, training loss: 0.1367 , running loss:0.15505416020751 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 39, training loss: 0.0950 , running loss:0.14878368452191354 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 40, training loss: 0.0934 , running loss:0.14432408325374127 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.10853111772330976   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139448  ,f1-macro :  0.9244053437601831\n",
      "Epoch : 0 ,Iteration : 41, training loss: 0.1593 , running loss:0.14363978393375873 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 42, training loss: 0.0880 , running loss:0.13938851244747638 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 43, training loss: 0.1190 , running loss:0.1344362836331129 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 44, training loss: 0.0867 , running loss:0.13078328035771847 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 45, training loss: 0.0849 , running loss:0.12437628842890262 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 46, training loss: 0.0783 , running loss:0.12104201465845107 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 47, training loss: 0.0823 , running loss:0.11784730143845082 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 48, training loss: 0.0807 , running loss:0.11511189974844456 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 49, training loss: 0.1815 , running loss:0.11574323289096355 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 50, training loss: 0.1140 , running loss:0.11350400671362877 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 51, training loss: 0.1466 , running loss:0.11481634378433228 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 52, training loss: 0.0717 , running loss:0.11244602277874946 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 53, training loss: 0.0678 , running loss:0.10985030755400657 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 54, training loss: 0.0697 , running loss:0.10359052121639252 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 55, training loss: 0.1023 , running loss:0.1031890545040369 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 56, training loss: 0.1021 , running loss:0.10305356718599797 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 57, training loss: 0.0668 , running loss:0.10132956504821777 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 58, training loss: 0.0638 , running loss:0.09768434390425682 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 59, training loss: 0.0985 , running loss:0.09786111935973167 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 60, training loss: 0.0988 , running loss:0.09813097231090069 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Validation loss : 0.0825850015446063   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139447  ,f1-macro :  0.924405343760183\n",
      "Epoch : 0 ,Iteration : 61, training loss: 0.0595 , running loss:0.09313887227326631 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 62, training loss: 0.0987 , running loss:0.093669299967587 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 63, training loss: 0.1359 , running loss:0.09451476428657771 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 64, training loss: 0.0557 , running loss:0.09296467248350382 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 65, training loss: 0.0926 , running loss:0.09335324708372354 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 66, training loss: 0.0576 , running loss:0.09231899753212928 (1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 67, training loss: 0.0942 , running loss:0.09291348978877068 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 68, training loss: 0.0540 , running loss:0.09157674498856068 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 69, training loss: 0.0527 , running loss:0.0851390790194273 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 70, training loss: 0.0505 , running loss:0.0819616287946701 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 71, training loss: 0.0507 , running loss:0.07716828770935535 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 72, training loss: 0.1247 , running loss:0.07982078716158866 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 73, training loss: 0.1313 , running loss:0.08299682401120663 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 74, training loss: 0.0892 , running loss:0.08397348076105118 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 75, training loss: 0.0481 , running loss:0.08126702159643173 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 76, training loss: 0.1371 , running loss:0.08301522135734558 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 77, training loss: 0.0953 , running loss:0.08443723022937774 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 78, training loss: 0.0869 , running loss:0.08559282571077347 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 79, training loss: 0.0449 , running loss:0.08291470445692539 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 80, training loss: 0.0451 , running loss:0.08023035507649183 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.07083885716902545   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139448  ,f1-macro :  0.925708699902249\n",
      "Epoch : 0 ,Iteration : 81, training loss: 0.0430 , running loss:0.07940825168043375 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 82, training loss: 0.0820 , running loss:0.07857482079416514 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 83, training loss: 0.0424 , running loss:0.07390060499310494 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 84, training loss: 0.0853 , running loss:0.07538055703043937 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 85, training loss: 0.0850 , running loss:0.07499756291508675 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 86, training loss: 0.1327 , running loss:0.07875524070113897 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 87, training loss: 0.0397 , running loss:0.07603218592703342 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 88, training loss: 0.0834 , running loss:0.07750313356518745 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 89, training loss: 0.0819 , running loss:0.07896406427025796 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 90, training loss: 0.1233 , running loss:0.08260503858327865 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 91, training loss: 0.1286 , running loss:0.08650057725608348 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 92, training loss: 0.0822 , running loss:0.08437393493950367 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 93, training loss: 0.1279 , running loss:0.0842006642371416 (0.9722222222222222, 0.8888888888888888, 0.75)\n",
      "Epoch : 0 ,Iteration : 94, training loss: 0.0795 , running loss:0.08371303901076317 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 95, training loss: 0.0392 , running loss:0.08326676543802022 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 96, training loss: 0.0827 , running loss:0.08054877053946256 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 97, training loss: 0.0357 , running loss:0.07756891418248416 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 98, training loss: 0.0812 , running loss:0.07728380057960749 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 99, training loss: 0.1733 , running loss:0.08370078038424253 (0.9583333333333334, 0.6666666666666666, 0.625)\n",
      "Epoch : 0 ,Iteration : 100, training loss: 0.1295 , running loss:0.0879202488809824 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Validation loss : 0.06523843379445439   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139451  ,f1-macro :  0.9270120560443147\n",
      "Epoch : 0 ,Iteration : 101, training loss: 0.0366 , running loss:0.08759793974459171 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 102, training loss: 0.0791 , running loss:0.08745464123785496 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 103, training loss: 0.0356 , running loss:0.08711460009217262 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 104, training loss: 0.1845 , running loss:0.0920739620923996 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 105, training loss: 0.0347 , running loss:0.08955951761454344 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 106, training loss: 0.0364 , running loss:0.0847409764304757 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 107, training loss: 0.0350 , running loss:0.08450303617864847 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 108, training loss: 0.0333 , running loss:0.08199925310909748 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 109, training loss: 0.0346 , running loss:0.07963080052286386 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 110, training loss: 0.0323 , running loss:0.07507937550544738 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 111, training loss: 0.0339 , running loss:0.070346605964005 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 112, training loss: 0.0795 , running loss:0.070213339664042 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 113, training loss: 0.0783 , running loss:0.06773251984268427 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 114, training loss: 0.0744 , running loss:0.067481286264956 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 115, training loss: 0.3102 , running loss:0.0810285497456789 (0.9166666666666666, 0.4444444444444444, 0.5)\n",
      "Epoch : 0 ,Iteration : 116, training loss: 0.1242 , running loss:0.08310048431158065 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 117, training loss: 0.0808 , running loss:0.08535791486501694 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 118, training loss: 0.0323 , running loss:0.08291183765977621 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 119, training loss: 0.0326 , running loss:0.07587534133344889 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 120, training loss: 0.0320 , running loss:0.07100029252469539 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.06195678658608642   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139452  ,f1-macro :  0.9257086999022485\n",
      "Epoch : 0 ,Iteration : 121, training loss: 0.0308 , running loss:0.07070902278646826 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 122, training loss: 0.0757 , running loss:0.07053810274228453 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 123, training loss: 0.0298 , running loss:0.07024808460846543 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 124, training loss: 0.0289 , running loss:0.062470487877726556 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 125, training loss: 0.1262 , running loss:0.0670468209311366 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 126, training loss: 0.1226 , running loss:0.07136070597916841 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 127, training loss: 0.0770 , running loss:0.07346289046108723 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 128, training loss: 0.0292 , running loss:0.07325561074540018 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 129, training loss: 0.0297 , running loss:0.07301291013136506 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 130, training loss: 0.0289 , running loss:0.0728460899554193 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 131, training loss: 0.1246 , running loss:0.07737810807302595 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 132, training loss: 0.0279 , running loss:0.07479756586253643 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 133, training loss: 0.1244 , running loss:0.07710671499371528 (0.9722222222222222, 0.7777777777777778, 0.75)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 134, training loss: 0.0284 , running loss:0.07480461541563273 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 135, training loss: 0.0764 , running loss:0.06311772707849742 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 136, training loss: 0.0758 , running loss:0.060698624886572364 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 137, training loss: 0.1818 , running loss:0.06574918236583471 (0.9583333333333334, 0.6666666666666666, 0.625)\n",
      "Epoch : 0 ,Iteration : 138, training loss: 0.0281 , running loss:0.06554217860102654 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 139, training loss: 0.2123 , running loss:0.07452770732343197 (0.9444444444444444, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 140, training loss: 0.0272 , running loss:0.07429185165092349 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.05977154671162629   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139457  ,f1-macro :  0.926360377973281\n",
      "Epoch : 0 ,Iteration : 141, training loss: 0.1126 , running loss:0.07838488724082708 (0.9722222222222222, 0.8888888888888888, 0.75)\n",
      "Epoch : 0 ,Iteration : 142, training loss: 0.0822 , running loss:0.07870798353105783 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 143, training loss: 0.1226 , running loss:0.08334805648773909 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 144, training loss: 0.0287 , running loss:0.08333681486546993 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 145, training loss: 0.2190 , running loss:0.08797524832189083 (0.9444444444444444, 0.6666666666666666, 0.625)\n",
      "Epoch : 0 ,Iteration : 146, training loss: 0.1291 , running loss:0.08829911164939404 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 147, training loss: 0.1631 , running loss:0.09260241612792015 (0.9583333333333334, 0.6666666666666666, 0.625)\n",
      "Epoch : 0 ,Iteration : 148, training loss: 0.0317 , running loss:0.09272513436153532 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 149, training loss: 0.1316 , running loss:0.09781795600429177 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 150, training loss: 0.0301 , running loss:0.09787937998771667 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 151, training loss: 0.0776 , running loss:0.09553132988512517 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 152, training loss: 0.0723 , running loss:0.09775153445079923 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 153, training loss: 0.0286 , running loss:0.09295914974063635 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 154, training loss: 0.0659 , running loss:0.09483341574668884 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 155, training loss: 0.0286 , running loss:0.09244447695091366 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 156, training loss: 0.0780 , running loss:0.0925559096969664 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 157, training loss: 0.1262 , running loss:0.0897771337069571 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 158, training loss: 0.0759 , running loss:0.0921649911440909 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 159, training loss: 0.0729 , running loss:0.08519783550873399 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 160, training loss: 0.0826 , running loss:0.08796797282993793 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Validation loss : 0.05849961642055742   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139451  ,f1-macro :  0.924731182795699\n",
      "Epoch : 0 ,Iteration : 161, training loss: 0.0756 , running loss:0.08611582592129707 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 162, training loss: 0.0694 , running loss:0.0854773387312889 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 163, training loss: 0.0263 , running loss:0.08066345863044262 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 164, training loss: 0.0269 , running loss:0.08057501474395394 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 165, training loss: 0.1289 , running loss:0.07607326293364167 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 166, training loss: 0.0790 , running loss:0.07356749111786484 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 167, training loss: 0.0252 , running loss:0.06667528357356786 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 168, training loss: 0.0722 , running loss:0.06870508044958115 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 169, training loss: 0.0254 , running loss:0.06339450553059578 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 170, training loss: 0.0786 , running loss:0.06581472540274262 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 171, training loss: 0.0249 , running loss:0.0631798442453146 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 172, training loss: 0.1274 , running loss:0.06593378745019436 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 173, training loss: 0.0239 , running loss:0.06570005295798183 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 174, training loss: 0.0746 , running loss:0.06613777903839946 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 175, training loss: 0.0236 , running loss:0.06588455224409699 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 176, training loss: 0.0813 , running loss:0.06604843186214567 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 177, training loss: 0.1296 , running loss:0.06621451349928975 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 178, training loss: 0.0719 , running loss:0.0660162870772183 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 179, training loss: 0.0730 , running loss:0.06602166341617703 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 180, training loss: 0.2227 , running loss:0.07302293190732598 (0.9444444444444444, 0.6666666666666666, 0.625)\n",
      "Validation loss : 0.05680080451333453   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139454  ,f1-macro :  0.9286412512218962\n",
      "Epoch : 0 ,Iteration : 181, training loss: 0.0737 , running loss:0.07292594397440552 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 182, training loss: 0.0689 , running loss:0.07290219450369477 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 183, training loss: 0.1275 , running loss:0.07795924441888928 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 184, training loss: 0.0227 , running loss:0.07774587506428361 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 185, training loss: 0.1789 , running loss:0.08024204233661295 (0.9583333333333334, 0.6666666666666666, 0.625)\n",
      "Epoch : 0 ,Iteration : 186, training loss: 0.1652 , running loss:0.08455165857449173 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 187, training loss: 0.0239 , running loss:0.08448394546285272 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 188, training loss: 0.0782 , running loss:0.08478054823353887 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 189, training loss: 0.0692 , running loss:0.0869727392680943 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 190, training loss: 0.0220 , running loss:0.08414660142734647 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 191, training loss: 0.0236 , running loss:0.08407931653782726 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 192, training loss: 0.0232 , running loss:0.07887299535796047 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 193, training loss: 0.1857 , running loss:0.08696354171261192 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 194, training loss: 0.2179 , running loss:0.09412901056930423 (0.9444444444444444, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 195, training loss: 0.0626 , running loss:0.09608151130378247 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 196, training loss: 0.0248 , running loss:0.09325670506805181 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 197, training loss: 0.0251 , running loss:0.0880322310142219 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 198, training loss: 0.0250 , running loss:0.08568861177191138 (1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 199, training loss: 0.0235 , running loss:0.08321371991187335 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 200, training loss: 0.0867 , running loss:0.0764173896983266 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Validation loss : 0.056175643294132   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139452  ,f1-macro :  0.9263603779732814\n",
      "Epoch : 0 ,Iteration : 201, training loss: 0.1152 , running loss:0.07849718425422907 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 202, training loss: 0.0216 , running loss:0.07613239195197821 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 203, training loss: 0.0704 , running loss:0.07327994611114264 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 204, training loss: 0.0222 , running loss:0.07325635701417924 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 205, training loss: 0.0216 , running loss:0.06539420299232006 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 206, training loss: 0.0209 , running loss:0.05817732447758317 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 207, training loss: 0.0224 , running loss:0.05810620710253715 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 208, training loss: 0.0690 , running loss:0.05764964632689953 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 209, training loss: 0.1270 , running loss:0.06053956039249897 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 210, training loss: 0.1566 , running loss:0.06727022603154183 (0.9583333333333334, 0.7777777777777778, 0.625)\n",
      "Epoch : 0 ,Iteration : 211, training loss: 0.1109 , running loss:0.07163636600598693 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 212, training loss: 0.0206 , running loss:0.07150378776714206 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 213, training loss: 0.0691 , running loss:0.06567320777103305 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 214, training loss: 0.0219 , running loss:0.05586980571970344 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 215, training loss: 0.0676 , running loss:0.056117779295891526 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 216, training loss: 0.0648 , running loss:0.05811856957152486 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 217, training loss: 0.0200 , running loss:0.05786542184650898 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 218, training loss: 0.0205 , running loss:0.057636144943535327 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 219, training loss: 0.0197 , running loss:0.0574460769072175 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 220, training loss: 0.1914 , running loss:0.06268120426684617 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Validation loss : 0.05495977320345091   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139452  ,f1-macro :  0.9273378950798311\n",
      "Epoch : 0 ,Iteration : 221, training loss: 0.1131 , running loss:0.06257134694606066 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 222, training loss: 0.0684 , running loss:0.06491129565984011 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 223, training loss: 0.0201 , running loss:0.062397340591996905 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 224, training loss: 0.0205 , running loss:0.0623140437528491 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 225, training loss: 0.0197 , running loss:0.0622199441306293 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 226, training loss: 0.0204 , running loss:0.06219531893730164 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 227, training loss: 0.0702 , running loss:0.06458271257579326 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 228, training loss: 0.0656 , running loss:0.06440907418727874 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 229, training loss: 0.1242 , running loss:0.06427139677107334 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 230, training loss: 0.0623 , running loss:0.059553293883800505 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 231, training loss: 0.0606 , running loss:0.05704049281775951 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 232, training loss: 0.0767 , running loss:0.059846580401062964 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 233, training loss: 0.0717 , running loss:0.05997362546622753 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 234, training loss: 0.0715 , running loss:0.06245306134223938 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 235, training loss: 0.0965 , running loss:0.06389951892197132 (0.9722222222222222, 0.8888888888888888, 0.75)\n",
      "Epoch : 0 ,Iteration : 236, training loss: 0.0200 , running loss:0.06165665201842785 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 237, training loss: 0.0210 , running loss:0.06170569900423288 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 238, training loss: 0.1436 , running loss:0.06786442026495934 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 239, training loss: 0.1208 , running loss:0.07291625710204244 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 240, training loss: 0.0209 , running loss:0.06438740948215127 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.0550848906887067   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139457  ,f1-macro :  0.925057021831215\n",
      "Epoch : 0 ,Iteration : 241, training loss: 0.0587 , running loss:0.06167119415476918 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 242, training loss: 0.1059 , running loss:0.06354533033445478 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 243, training loss: 0.0788 , running loss:0.06647826805710792 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 244, training loss: 0.1274 , running loss:0.07182274283841253 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 245, training loss: 0.0595 , running loss:0.0738126888871193 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 246, training loss: 0.0222 , running loss:0.07390471389517188 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 247, training loss: 0.0597 , running loss:0.07338252076879144 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 248, training loss: 0.1206 , running loss:0.07613348560407758 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 249, training loss: 0.0190 , running loss:0.07087139720097184 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 250, training loss: 0.0213 , running loss:0.06882145376875996 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 251, training loss: 0.0193 , running loss:0.06675652340054512 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 252, training loss: 0.0196 , running loss:0.0639019763097167 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 253, training loss: 0.0729 , running loss:0.06396341044455767 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 254, training loss: 0.0179 , running loss:0.0612873362377286 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 255, training loss: 0.1728 , running loss:0.06510135512799024 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 256, training loss: 0.0602 , running loss:0.06711157448589802 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 257, training loss: 0.0692 , running loss:0.06952455397695304 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 258, training loss: 0.0788 , running loss:0.06628294866532088 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 259, training loss: 0.0782 , running loss:0.0641535947099328 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 260, training loss: 0.0208 , running loss:0.06415211018174886 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.05493842685305653   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139451  ,f1-macro :  0.9250570218312161\n",
      "Epoch : 0 ,Iteration : 261, training loss: 0.0752 , running loss:0.06497716587036848 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 262, training loss: 0.0190 , running loss:0.06063320366665721 (1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 263, training loss: 0.0848 , running loss:0.06093087336048484 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 264, training loss: 0.0199 , running loss:0.05555357960984111 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 265, training loss: 0.0185 , running loss:0.05350055545568466 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 266, training loss: 0.0599 , running loss:0.05538656106218696 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 267, training loss: 0.0178 , running loss:0.05328705590218306 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 268, training loss: 0.0786 , running loss:0.051185339130461215 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 269, training loss: 0.1687 , running loss:0.05866855178028345 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 270, training loss: 0.1874 , running loss:0.0669754596427083 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 271, training loss: 0.0165 , running loss:0.0668325730599463 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 272, training loss: 0.0175 , running loss:0.06672515450045466 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 273, training loss: 0.0163 , running loss:0.06389736644923687 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 274, training loss: 0.0743 , running loss:0.06671729609370232 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 275, training loss: 0.1821 , running loss:0.06718236580491066 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 276, training loss: 0.0591 , running loss:0.06712806764990091 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 277, training loss: 0.0165 , running loss:0.06448945570737123 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 278, training loss: 0.3170 , running loss:0.07639889512211084 (0.9305555555555556, 0.5555555555555556, 0.5)\n",
      "Epoch : 0 ,Iteration : 279, training loss: 0.0168 , running loss:0.073328304477036 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 280, training loss: 0.0163 , running loss:0.0731003482826054 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.05367946930371684   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139452  ,f1-macro :  0.9263603779732814\n",
      "Epoch : 0 ,Iteration : 281, training loss: 0.0168 , running loss:0.07018035110086203 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 282, training loss: 0.1135 , running loss:0.07490598382428289 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 283, training loss: 0.2935 , running loss:0.0853421644307673 (0.9305555555555556, 0.5555555555555556, 0.5)\n",
      "Epoch : 0 ,Iteration : 284, training loss: 0.0171 , running loss:0.08520260667428374 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 285, training loss: 0.0839 , running loss:0.08847585339099169 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 286, training loss: 0.0760 , running loss:0.08928022515028715 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 287, training loss: 0.0620 , running loss:0.09149232218042017 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 288, training loss: 0.0172 , running loss:0.08842317629605531 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 289, training loss: 0.0712 , running loss:0.08355113100260496 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 290, training loss: 0.1333 , running loss:0.08084635343402624 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 291, training loss: 0.1250 , running loss:0.08627433069050312 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 292, training loss: 0.0661 , running loss:0.08870327956974507 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 293, training loss: 0.0203 , running loss:0.0888996615074575 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 294, training loss: 0.0196 , running loss:0.08616500189527869 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 295, training loss: 0.1746 , running loss:0.08579282080754638 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 296, training loss: 0.0644 , running loss:0.08605809090659022 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 297, training loss: 0.0204 , running loss:0.08625604538246989 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 298, training loss: 0.1264 , running loss:0.07672889335080982 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 299, training loss: 0.0185 , running loss:0.07681442396715284 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 300, training loss: 0.0185 , running loss:0.07692481381818653 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.05399109204122223   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139448  ,f1-macro :  0.9247311827956997\n",
      "Epoch : 0 ,Iteration : 301, training loss: 0.1253 , running loss:0.08235078994184733 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 302, training loss: 0.1896 , running loss:0.0861556587740779 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 303, training loss: 0.1857 , running loss:0.08076474498957395 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 304, training loss: 0.1327 , running loss:0.08654514811933041 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 305, training loss: 0.1311 , running loss:0.08890430405735969 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 306, training loss: 0.1452 , running loss:0.09236394949257373 (0.9722222222222222, 0.8888888888888888, 0.75)\n",
      "Epoch : 0 ,Iteration : 307, training loss: 0.0175 , running loss:0.09013952082023025 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 308, training loss: 0.1761 , running loss:0.0980841213837266 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 309, training loss: 0.1690 , running loss:0.10297152306884527 (0.9583333333333334, 0.7777777777777778, 0.625)\n",
      "Epoch : 0 ,Iteration : 310, training loss: 0.0643 , running loss:0.09952055606991053 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 311, training loss: 0.0660 , running loss:0.09656833801418543 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 312, training loss: 0.0614 , running loss:0.09633763954043388 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 313, training loss: 0.0811 , running loss:0.09937883429229259 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 314, training loss: 0.1280 , running loss:0.10479553416371346 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 315, training loss: 0.0240 , running loss:0.09726519323885441 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 316, training loss: 0.1378 , running loss:0.10093465968966484 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 317, training loss: 0.0773 , running loss:0.10377799831330776 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 318, training loss: 0.0613 , running loss:0.10052102878689766 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 319, training loss: 0.0434 , running loss:0.10176879651844502 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 320, training loss: 0.0255 , running loss:0.10211784858256578 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.055074580740238216   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139451  ,f1-macro :  0.9260345389377651\n",
      "Epoch : 0 ,Iteration : 321, training loss: 0.0226 , running loss:0.09698091140016914 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 322, training loss: 0.1263 , running loss:0.09381568571552634 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 323, training loss: 0.1334 , running loss:0.09120452171191573 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 324, training loss: 0.1239 , running loss:0.09076426057145 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 325, training loss: 0.0182 , running loss:0.08511838680133224 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 326, training loss: 0.0186 , running loss:0.07878924496471881 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 327, training loss: 0.1098 , running loss:0.08340375730767846 (0.9722222222222222, 0.7777777777777778, 0.75)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 328, training loss: 0.0626 , running loss:0.0777298198081553 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 329, training loss: 0.0165 , running loss:0.07010836685076356 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 330, training loss: 0.0799 , running loss:0.07089049341157079 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 331, training loss: 0.0646 , running loss:0.0708191824145615 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 332, training loss: 0.0638 , running loss:0.07093709548935294 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 333, training loss: 0.1342 , running loss:0.07359078777953983 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 334, training loss: 0.1116 , running loss:0.07277224836871028 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 335, training loss: 0.0179 , running loss:0.0724638618528843 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 336, training loss: 0.0734 , running loss:0.06924410201609135 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 337, training loss: 0.0171 , running loss:0.06623600283637643 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 338, training loss: 0.0765 , running loss:0.06699608722701669 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 339, training loss: 0.0746 , running loss:0.06855456670746207 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 340, training loss: 0.0673 , running loss:0.07064624018967151 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Validation loss : 0.05356100125680746   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139457  ,f1-macro :  0.9253828608667315\n",
      "Epoch : 0 ,Iteration : 341, training loss: 0.1123 , running loss:0.07512841699644923 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 342, training loss: 0.0166 , running loss:0.06964421272277832 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 343, training loss: 0.0162 , running loss:0.06378070618957281 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 344, training loss: 0.1327 , running loss:0.064223931171 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 345, training loss: 0.2569 , running loss:0.07615798767656087 (0.9444444444444444, 0.6666666666666666, 0.625)\n",
      "Epoch : 0 ,Iteration : 346, training loss: 0.0169 , running loss:0.0760733881033957 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 347, training loss: 0.1149 , running loss:0.07633075015619398 (0.9722222222222222, 0.8888888888888888, 0.75)\n",
      "Epoch : 0 ,Iteration : 348, training loss: 0.0175 , running loss:0.07407523719593882 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 349, training loss: 0.1796 , running loss:0.08222543327137828 (0.9583333333333334, 0.7777777777777778, 0.625)\n",
      "Epoch : 0 ,Iteration : 350, training loss: 0.0180 , running loss:0.0791285378858447 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 351, training loss: 0.0690 , running loss:0.0793497109785676 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 352, training loss: 0.0185 , running loss:0.077085548825562 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 353, training loss: 0.0184 , running loss:0.07130002276971936 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 354, training loss: 0.0192 , running loss:0.06667776657268405 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 355, training loss: 0.0714 , running loss:0.06935298163443804 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 356, training loss: 0.0826 , running loss:0.06981464233249426 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 357, training loss: 0.0165 , running loss:0.06978502059355378 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 358, training loss: 0.0700 , running loss:0.06946170488372445 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 359, training loss: 0.0172 , running loss:0.06659152479842305 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 360, training loss: 0.0669 , running loss:0.06657317439094186 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Validation loss : 0.053752353270142134   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139457  ,f1-macro :  0.9263603779732813\n",
      "Epoch : 0 ,Iteration : 361, training loss: 0.0158 , running loss:0.061749724112451075 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 362, training loss: 0.0722 , running loss:0.06452630469575524 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 363, training loss: 0.0157 , running loss:0.06450393274426461 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 364, training loss: 0.1121 , running loss:0.06347413174808025 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 365, training loss: 0.0155 , running loss:0.0514063885435462 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 366, training loss: 0.0158 , running loss:0.05134864915162325 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 367, training loss: 0.0156 , running loss:0.04638070836663246 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 368, training loss: 0.0594 , running loss:0.04847766552120447 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 369, training loss: 0.0145 , running loss:0.04022721541114151 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 370, training loss: 0.1327 , running loss:0.04596049054525793 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 371, training loss: 0.0630 , running loss:0.04565985999070108 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 372, training loss: 0.0143 , running loss:0.04544786876067519 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 373, training loss: 0.0638 , running loss:0.0477164164185524 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 374, training loss: 0.0705 , running loss:0.05028378460556269 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 375, training loss: 0.0152 , running loss:0.04747308362275362 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 376, training loss: 0.0803 , running loss:0.047355289570987225 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 377, training loss: 0.0153 , running loss:0.0472917047329247 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 378, training loss: 0.0149 , running loss:0.04453460774384439 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 379, training loss: 0.0815 , running loss:0.047750373696908356 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 380, training loss: 0.1094 , running loss:0.049874956673011187 (0.9722222222222222, 0.8888888888888888, 0.75)\n",
      "Validation loss : 0.05280666640219538   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139452  ,f1-macro :  0.9270120560443142\n",
      "Epoch : 0 ,Iteration : 381, training loss: 0.0142 , running loss:0.04979399056173861 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 382, training loss: 0.0141 , running loss:0.04689188892953098 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 383, training loss: 0.0656 , running loss:0.04938438297249377 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 384, training loss: 0.0770 , running loss:0.04762685433961451 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 385, training loss: 0.0644 , running loss:0.050069599272683264 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 386, training loss: 0.0145 , running loss:0.050004208879545334 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 387, training loss: 0.0142 , running loss:0.04993382161483169 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 388, training loss: 0.0781 , running loss:0.05086613809689879 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 389, training loss: 0.1957 , running loss:0.059921703906729816 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 390, training loss: 0.0657 , running loss:0.05657280390150845 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 391, training loss: 0.0139 , running loss:0.054119232669472696 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 392, training loss: 0.1643 , running loss:0.061618187883868816 (0.9583333333333334, 0.8888888888888888, 0.625)\n",
      "Epoch : 0 ,Iteration : 393, training loss: 0.0622 , running loss:0.061535325134173036 (0.9861111111111112, 0.8888888888888888, 0.875)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 394, training loss: 0.0144 , running loss:0.05872809304855764 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 395, training loss: 0.0140 , running loss:0.058669882174581287 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 396, training loss: 0.0142 , running loss:0.055365871824324134 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 397, training loss: 0.0152 , running loss:0.05536336097866297 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 398, training loss: 0.0143 , running loss:0.05533245033584535 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 399, training loss: 0.0147 , running loss:0.051990033593028784 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 400, training loss: 0.0145 , running loss:0.04724269616417587 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.05278747913064408   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139451  ,f1-macro :  0.9266862170087978\n",
      "Epoch : 0 ,Iteration : 401, training loss: 0.0143 , running loss:0.04725092747248709 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 402, training loss: 0.1745 , running loss:0.055269634863361713 (0.9583333333333334, 0.7777777777777778, 0.625)\n",
      "Epoch : 0 ,Iteration : 403, training loss: 0.0139 , running loss:0.05268842480145395 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 404, training loss: 0.0680 , running loss:0.05224060802720487 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 405, training loss: 0.1965 , running loss:0.05884760557673872 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 406, training loss: 0.0141 , running loss:0.05882945726625621 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 407, training loss: 0.0808 , running loss:0.06215971624478698 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 408, training loss: 0.0147 , running loss:0.05899041052907705 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 409, training loss: 0.0148 , running loss:0.04994574640877545 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 410, training loss: 0.2096 , running loss:0.05714098778553307 (0.9583333333333334, 0.7777777777777778, 0.625)\n",
      "Epoch : 0 ,Iteration : 411, training loss: 0.0150 , running loss:0.05719421408139169 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 412, training loss: 0.0144 , running loss:0.04969886541366577 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 413, training loss: 0.0655 , running loss:0.0498657051473856 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 414, training loss: 0.0145 , running loss:0.04987167506478727 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 415, training loss: 0.0148 , running loss:0.04991020141169429 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 416, training loss: 0.0138 , running loss:0.049889294896274805 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 417, training loss: 0.0778 , running loss:0.053020532988011834 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 418, training loss: 0.0139 , running loss:0.05299974586814642 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 419, training loss: 0.0144 , running loss:0.05298521905206144 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 420, training loss: 0.0131 , running loss:0.05291841086000204 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.052954333760609026   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139458  ,f1-macro :  0.9273378950798304\n",
      "Epoch : 0 ,Iteration : 421, training loss: 0.0628 , running loss:0.055339923966675995 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 422, training loss: 0.0129 , running loss:0.04725813465192914 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 423, training loss: 0.0791 , running loss:0.05051718885079026 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 424, training loss: 0.0126 , running loss:0.04774768808856607 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 425, training loss: 0.1394 , running loss:0.04489085534587502 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 426, training loss: 0.1117 , running loss:0.049769929703325035 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 427, training loss: 0.0123 , running loss:0.0463463599793613 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 428, training loss: 0.0117 , running loss:0.04619681262411177 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 429, training loss: 0.0124 , running loss:0.046079242043197154 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 430, training loss: 0.0120 , running loss:0.036197319626808167 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 431, training loss: 0.1330 , running loss:0.04209671500138938 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 432, training loss: 0.0614 , running loss:0.04445008216425776 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 433, training loss: 0.0696 , running loss:0.04465303821489215 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 434, training loss: 0.0117 , running loss:0.0445127512793988 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 435, training loss: 0.0117 , running loss:0.04435999682173133 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 436, training loss: 0.0116 , running loss:0.044253062829375266 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 437, training loss: 0.0119 , running loss:0.040957051003351806 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 438, training loss: 0.0127 , running loss:0.040899436734616754 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 439, training loss: 0.0718 , running loss:0.04377016695216298 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 440, training loss: 0.0118 , running loss:0.043701878283172846 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.053002112747178226   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139457  ,f1-macro :  0.9257086999022478\n",
      "Epoch : 0 ,Iteration : 441, training loss: 0.0117 , running loss:0.0411471385974437 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 442, training loss: 0.0117 , running loss:0.04108653594739735 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 443, training loss: 0.0118 , running loss:0.03771980726160109 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 444, training loss: 0.0794 , running loss:0.04105642945505679 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 445, training loss: 0.0112 , running loss:0.03464600308798253 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 446, training loss: 0.1391 , running loss:0.03601473695598543 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 447, training loss: 0.0671 , running loss:0.038752387976273894 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 448, training loss: 0.0726 , running loss:0.041798855271190406 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 449, training loss: 0.0113 , running loss:0.04174273419193923 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 450, training loss: 0.0110 , running loss:0.04169496642425656 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 451, training loss: 0.0831 , running loss:0.03919934080913663 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 452, training loss: 0.0110 , running loss:0.03667683321982622 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 453, training loss: 0.0112 , running loss:0.03375983634032309 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 454, training loss: 0.1296 , running loss:0.039654877549037335 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 455, training loss: 0.0724 , running loss:0.042690339218825105 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 456, training loss: 0.0116 , running loss:0.042691555619239804 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 457, training loss: 0.0108 , running loss:0.04263503742404282 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 458, training loss: 0.1287 , running loss:0.048432012740522626 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 459, training loss: 0.0105 , running loss:0.04536549001932144 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 460, training loss: 0.0689 , running loss:0.04822202688083053 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Validation loss : 0.053266220032360374   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139451  ,f1-macro :  0.9263603779732813\n",
      "Epoch : 0 ,Iteration : 461, training loss: 0.0841 , running loss:0.05184275577776134 (0.9861111111111112, 0.8888888888888888, 0.875)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 462, training loss: 0.0666 , running loss:0.05459145638160408 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 463, training loss: 0.0727 , running loss:0.05763871907256544 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 464, training loss: 0.0724 , running loss:0.057288037007674576 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 465, training loss: 0.0113 , running loss:0.05729524502530694 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 466, training loss: 0.2036 , running loss:0.060524211172014473 (0.9583333333333334, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 467, training loss: 0.0113 , running loss:0.05773607320152223 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 468, training loss: 0.0109 , running loss:0.05465150480158627 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 469, training loss: 0.0113 , running loss:0.05465368269942701 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 470, training loss: 0.0113 , running loss:0.05467066154815257 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 471, training loss: 0.0113 , running loss:0.05108290989883244 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 472, training loss: 0.2786 , running loss:0.06446377797983587 (0.9444444444444444, 0.5555555555555556, 0.625)\n",
      "Epoch : 0 ,Iteration : 473, training loss: 0.0111 , running loss:0.06446037059649826 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 474, training loss: 0.1365 , running loss:0.0648079908452928 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 475, training loss: 0.0105 , running loss:0.061712230741977694 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 476, training loss: 0.0117 , running loss:0.06171537018381059 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 477, training loss: 0.0117 , running loss:0.06175945322029293 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 478, training loss: 0.2497 , running loss:0.06781392167322338 (0.9444444444444444, 0.5555555555555556, 0.625)\n",
      "Epoch : 0 ,Iteration : 479, training loss: 0.0118 , running loss:0.06787823615595698 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 480, training loss: 0.0711 , running loss:0.0679869051091373 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Validation loss : 0.05298189327251876   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139447  ,f1-macro :  0.9253828608667329\n",
      "Epoch : 0 ,Iteration : 481, training loss: 0.1300 , running loss:0.0702819406054914 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 482, training loss: 0.0680 , running loss:0.07034857822582126 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 483, training loss: 0.0118 , running loss:0.06730131735093892 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 484, training loss: 0.0703 , running loss:0.06719837342388928 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 485, training loss: 0.0119 , running loss:0.0672249220777303 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 486, training loss: 0.1246 , running loss:0.0632714529056102 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 487, training loss: 0.0127 , running loss:0.06334172277711332 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 488, training loss: 0.0121 , running loss:0.06339861275628209 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 489, training loss: 0.0122 , running loss:0.0634396874345839 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 490, training loss: 0.2331 , running loss:0.07452856902964414 (0.9444444444444444, 0.6666666666666666, 0.5)\n",
      "Epoch : 0 ,Iteration : 491, training loss: 0.0136 , running loss:0.07464358825236558 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 492, training loss: 0.0619 , running loss:0.06380695793777705 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 493, training loss: 0.0762 , running loss:0.06705981977283955 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 494, training loss: 0.0139 , running loss:0.06092790011316538 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 495, training loss: 0.0148 , running loss:0.061142291454598305 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 496, training loss: 0.1309 , running loss:0.06710156565532088 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 497, training loss: 0.0147 , running loss:0.06725045242346823 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 498, training loss: 0.0137 , running loss:0.055448368098586796 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 499, training loss: 0.0821 , running loss:0.058964523347094656 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 500, training loss: 0.1330 , running loss:0.062059996323660015 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Validation loss : 0.052172101012641386   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139454  ,f1-macro :  0.9266862170087973\n",
      "Epoch : 0 ,Iteration : 501, training loss: 0.0559 , running loss:0.05835671932436526 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 502, training loss: 0.0774 , running loss:0.05882827178575099 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 503, training loss: 0.0136 , running loss:0.058920144103467464 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 504, training loss: 0.0136 , running loss:0.056083908583968875 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 505, training loss: 0.0128 , running loss:0.05613227435387671 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 506, training loss: 0.0126 , running loss:0.0505360939539969 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 507, training loss: 0.0128 , running loss:0.050541234016418454 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 508, training loss: 0.0604 , running loss:0.05296011003665626 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 509, training loss: 0.0119 , running loss:0.05294570126570761 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 510, training loss: 0.0115 , running loss:0.041864743130281566 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 511, training loss: 0.0113 , running loss:0.04174842583015561 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 512, training loss: 0.0117 , running loss:0.03924184595234692 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 513, training loss: 0.0114 , running loss:0.0359993090853095 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 514, training loss: 0.0801 , running loss:0.039309986680746076 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 515, training loss: 0.1914 , running loss:0.04814061499200761 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 516, training loss: 0.2026 , running loss:0.051725125825032595 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 517, training loss: 0.0679 , running loss:0.05438779639080167 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 518, training loss: 0.1116 , running loss:0.059282288001850245 (0.9722222222222222, 0.8888888888888888, 0.75)\n",
      "Epoch : 0 ,Iteration : 519, training loss: 0.0715 , running loss:0.05875450600869954 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 520, training loss: 0.0630 , running loss:0.05525609222240746 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Validation loss : 0.052463934491893766   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139445  ,f1-macro :  0.9266862170087985\n",
      "Epoch : 0 ,Iteration : 521, training loss: 0.1260 , running loss:0.05876194820739329 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 522, training loss: 0.0122 , running loss:0.055502748535946014 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 523, training loss: 0.0122 , running loss:0.05543103343807161 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 524, training loss: 0.0567 , running loss:0.05758846593089402 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 525, training loss: 0.0546 , running loss:0.059675266640260814 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 526, training loss: 0.1215 , running loss:0.06511865463107824 (0.9722222222222222, 0.8888888888888888, 0.75)\n",
      "Epoch : 0 ,Iteration : 527, training loss: 0.1078 , running loss:0.06986959925852716 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 528, training loss: 0.0138 , running loss:0.06753924693912268 (1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 529, training loss: 0.1190 , running loss:0.07289851079694927 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 530, training loss: 0.0178 , running loss:0.07321265316568315 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 531, training loss: 0.1582 , running loss:0.08055845848284662 (0.9583333333333334, 0.6666666666666666, 0.875)\n",
      "Epoch : 0 ,Iteration : 532, training loss: 0.0788 , running loss:0.08391272742301226 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 533, training loss: 0.0527 , running loss:0.08598196436651051 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 534, training loss: 0.0220 , running loss:0.08307783207856119 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 535, training loss: 0.0251 , running loss:0.07476015104912222 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 536, training loss: 0.0218 , running loss:0.06571921152062714 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 537, training loss: 0.0200 , running loss:0.06332436199299991 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 538, training loss: 0.0608 , running loss:0.06078692558221519 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 539, training loss: 0.0546 , running loss:0.05993853094987571 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 540, training loss: 0.0527 , running loss:0.05942237745039165 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Validation loss : 0.051825631819556184   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139454  ,f1-macro :  0.9253828608667318\n",
      "Epoch : 0 ,Iteration : 541, training loss: 0.0514 , running loss:0.05569063737057149 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 542, training loss: 0.0746 , running loss:0.05880912798456848 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 543, training loss: 0.1463 , running loss:0.06551551125012338 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 544, training loss: 0.0159 , running loss:0.06347489762119948 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 545, training loss: 0.0713 , running loss:0.06431179284118116 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 546, training loss: 0.0124 , running loss:0.05885736676864326 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 547, training loss: 0.0750 , running loss:0.057215261785313484 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 548, training loss: 0.0118 , running loss:0.05711503671482206 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 549, training loss: 0.0122 , running loss:0.05177181325852871 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 550, training loss: 0.0117 , running loss:0.051467895368114115 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 551, training loss: 0.0847 , running loss:0.04779576384462416 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 552, training loss: 0.2248 , running loss:0.0550934346858412 (0.9444444444444444, 0.6666666666666666, 0.625)\n",
      "Epoch : 0 ,Iteration : 553, training loss: 0.1586 , running loss:0.06038897805847228 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 554, training loss: 0.1112 , running loss:0.06484776600264013 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 555, training loss: 0.1215 , running loss:0.06967062144540251 (0.9722222222222222, 0.8888888888888888, 0.75)\n",
      "Epoch : 0 ,Iteration : 556, training loss: 0.0126 , running loss:0.06921396683901548 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 557, training loss: 0.0565 , running loss:0.0710389289073646 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 558, training loss: 0.1802 , running loss:0.07700605085119605 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 559, training loss: 0.0549 , running loss:0.07702201521024107 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 560, training loss: 0.0139 , running loss:0.07508229138329625 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.052058950003749986   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139454  ,f1-macro :  0.9270120560443144\n",
      "Epoch : 0 ,Iteration : 561, training loss: 0.0628 , running loss:0.07565057976171374 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 562, training loss: 0.0171 , running loss:0.07277465863153339 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 563, training loss: 0.0145 , running loss:0.0661853963509202 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 564, training loss: 0.0835 , running loss:0.06956658847630023 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 565, training loss: 0.1389 , running loss:0.07294671535491944 (0.9583333333333334, 0.8888888888888888, 0.625)\n",
      "Epoch : 0 ,Iteration : 566, training loss: 0.0167 , running loss:0.0731593208387494 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 567, training loss: 0.0150 , running loss:0.07015946111641824 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 568, training loss: 0.0984 , running loss:0.0744886030908674 (0.9722222222222222, 0.8888888888888888, 0.75)\n",
      "Epoch : 0 ,Iteration : 569, training loss: 0.0175 , running loss:0.07475375444628299 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 570, training loss: 0.0140 , running loss:0.0748669421300292 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 571, training loss: 0.0154 , running loss:0.07140044909901917 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 572, training loss: 0.0167 , running loss:0.060995173966512085 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 573, training loss: 0.0176 , running loss:0.053941944940015675 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 574, training loss: 0.0496 , running loss:0.05086038145236671 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 575, training loss: 0.0156 , running loss:0.04556277422234416 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 576, training loss: 0.0155 , running loss:0.045705672027543186 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 577, training loss: 0.0132 , running loss:0.04354136358015239 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 578, training loss: 0.1448 , running loss:0.041772344661876556 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 579, training loss: 0.0135 , running loss:0.0397048604208976 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 580, training loss: 0.1860 , running loss:0.048308926681056616 (0.9583333333333334, 0.7777777777777778, 0.75)\n",
      "Validation loss : 0.05163770455244349   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139452  ,f1-macro :  0.9276637341153476\n",
      "Epoch : 0 ,Iteration : 581, training loss: 0.0534 , running loss:0.04784190566278994 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 582, training loss: 0.1421 , running loss:0.05409137145616114 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 583, training loss: 0.0121 , running loss:0.053970268834382294 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 584, training loss: 0.0681 , running loss:0.053196921665221455 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 585, training loss: 0.0810 , running loss:0.05030280398204923 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 586, training loss: 0.1499 , running loss:0.05696476111188531 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 587, training loss: 0.0126 , running loss:0.056844431161880496 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 588, training loss: 0.0130 , running loss:0.0525715253315866 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 589, training loss: 0.1429 , running loss:0.05884142285212875 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 590, training loss: 0.0590 , running loss:0.06109441136941314 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 591, training loss: 0.2435 , running loss:0.07249756124801934 (0.9444444444444444, 0.5555555555555556, 0.625)\n",
      "Epoch : 0 ,Iteration : 592, training loss: 0.0169 , running loss:0.07250836486928165 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 593, training loss: 0.0179 , running loss:0.07252234895713627 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 594, training loss: 0.0247 , running loss:0.07127841277979315 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 595, training loss: 0.0817 , running loss:0.07458401564508677 (0.9861111111111112, 0.8888888888888888, 0.875)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 596, training loss: 0.0227 , running loss:0.07494587628170848 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 597, training loss: 0.0169 , running loss:0.07513008611276746 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 598, training loss: 0.0504 , running loss:0.07040787609294057 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 599, training loss: 0.1305 , running loss:0.07625741213560104 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 600, training loss: 0.0664 , running loss:0.07027684003114701 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Validation loss : 0.05115474818432786   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139448  ,f1-macro :  0.927989573150864\n",
      "Epoch : 0 ,Iteration : 601, training loss: 0.1043 , running loss:0.07282088324427605 (0.9722222222222222, 0.8888888888888888, 0.75)\n",
      "Epoch : 0 ,Iteration : 602, training loss: 0.0612 , running loss:0.06877579875290393 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 603, training loss: 0.0135 , running loss:0.06884433170780539 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 604, training loss: 0.0713 , running loss:0.0690055294893682 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 605, training loss: 0.0124 , running loss:0.0655736405402422 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 606, training loss: 0.0120 , running loss:0.05867561297491193 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 607, training loss: 0.1316 , running loss:0.06462813117541373 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 608, training loss: 0.0133 , running loss:0.0646469718310982 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 609, training loss: 0.0121 , running loss:0.05810955376364291 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 610, training loss: 0.0117 , running loss:0.0557421418838203 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 611, training loss: 0.0113 , running loss:0.04413339919410646 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 612, training loss: 0.0107 , running loss:0.043825670052319764 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 613, training loss: 0.0105 , running loss:0.04345825440250337 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 614, training loss: 0.1341 , running loss:0.04892899435944855 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 615, training loss: 0.0575 , running loss:0.04771969676949084 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 616, training loss: 0.0736 , running loss:0.05026472290046513 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 617, training loss: 0.0103 , running loss:0.04993541957810521 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 618, training loss: 0.0571 , running loss:0.05027306182309985 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 619, training loss: 0.1995 , running loss:0.05372387869283557 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 620, training loss: 0.0796 , running loss:0.054382437746971844 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Validation loss : 0.0511780661391504   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139447  ,f1-macro :  0.9253828608667326\n",
      "Epoch : 0 ,Iteration : 621, training loss: 0.0110 , running loss:0.049716878589242695 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 622, training loss: 0.0104 , running loss:0.047181125311180946 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 623, training loss: 0.0110 , running loss:0.047058894159272315 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 624, training loss: 0.0098 , running loss:0.04398577162064612 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 625, training loss: 0.0105 , running loss:0.043889605347067116 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 626, training loss: 0.0546 , running loss:0.04602005258202553 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 627, training loss: 0.0097 , running loss:0.03992231520824134 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 628, training loss: 0.1231 , running loss:0.04541065176017582 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 629, training loss: 0.0102 , running loss:0.04531512442044914 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 630, training loss: 0.0098 , running loss:0.04522153167054057 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 631, training loss: 0.0672 , running loss:0.048014386044815185 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 632, training loss: 0.0125 , running loss:0.04810351184569299 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 633, training loss: 0.0111 , running loss:0.048132362263277176 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 634, training loss: 0.1771 , running loss:0.05028252457268536 (0.9583333333333334, 0.6666666666666666, 0.625)\n",
      "Epoch : 0 ,Iteration : 635, training loss: 0.0105 , running loss:0.047934462688863276 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 636, training loss: 0.0093 , running loss:0.04471879377961159 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 637, training loss: 0.1210 , running loss:0.050253315502777694 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 638, training loss: 0.2427 , running loss:0.05953029659576714 (0.9444444444444444, 0.5555555555555556, 0.625)\n",
      "Epoch : 0 ,Iteration : 639, training loss: 0.0147 , running loss:0.0502898785751313 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 640, training loss: 0.0160 , running loss:0.047113522933796045 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.05065608494901412   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139454  ,f1-macro :  0.9260345389377649\n",
      "Epoch : 0 ,Iteration : 641, training loss: 0.0140 , running loss:0.04726604837924242 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 642, training loss: 0.1434 , running loss:0.05391259430907667 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 643, training loss: 0.1774 , running loss:0.06223262995481491 (0.9583333333333334, 0.6666666666666666, 0.75)\n",
      "Epoch : 0 ,Iteration : 644, training loss: 0.0216 , running loss:0.06281885402277113 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 645, training loss: 0.0642 , running loss:0.06550373821519315 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 646, training loss: 0.0700 , running loss:0.06627654968760907 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 647, training loss: 0.0196 , running loss:0.06677184207364917 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 648, training loss: 0.0539 , running loss:0.06331289252266288 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 649, training loss: 0.0130 , running loss:0.06345138363540173 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 650, training loss: 0.0531 , running loss:0.0656141072511673 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 651, training loss: 0.0131 , running loss:0.06291087409481406 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 652, training loss: 0.0906 , running loss:0.06681525991298258 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 653, training loss: 0.1169 , running loss:0.07210687212646008 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 654, training loss: 0.0120 , running loss:0.06385212447494268 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 655, training loss: 0.0890 , running loss:0.06777671785093844 (0.9722222222222222, 0.8888888888888888, 0.75)\n",
      "Epoch : 0 ,Iteration : 656, training loss: 0.0222 , running loss:0.0684230372775346 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 657, training loss: 0.0091 , running loss:0.06282829032279551 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 658, training loss: 0.1024 , running loss:0.0558152271900326 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 659, training loss: 0.0608 , running loss:0.05812157797627151 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 660, training loss: 0.0524 , running loss:0.05994146247394383 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Validation loss : 0.050849002051301016   ,acc :  0.9215542521994134  ,f1-micro :  0.9902655588139454  ,f1-macro :  0.9263603779732816\n",
      "Epoch : 0 ,Iteration : 661, training loss: 0.0783 , running loss:0.06315255938097834 (0.9861111111111112, 0.8888888888888888, 0.875)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 662, training loss: 0.0197 , running loss:0.056968702003359795 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 663, training loss: 0.0774 , running loss:0.051964371278882024 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 664, training loss: 0.0128 , running loss:0.05152514446526766 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 665, training loss: 0.0161 , running loss:0.04912395384162664 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 666, training loss: 0.1130 , running loss:0.051272002048790455 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 667, training loss: 0.0158 , running loss:0.051081802044063804 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 668, training loss: 0.0162 , running loss:0.04919721065089107 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 669, training loss: 0.0102 , running loss:0.04905754108913243 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 670, training loss: 0.1260 , running loss:0.05270370938815176 (0.9722222222222222, 0.7777777777777778, 0.875)\n",
      "Epoch : 0 ,Iteration : 671, training loss: 0.0788 , running loss:0.05599060724489391 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 672, training loss: 0.0997 , running loss:0.056447541294619444 (0.9722222222222222, 0.8888888888888888, 0.75)\n",
      "Epoch : 0 ,Iteration : 673, training loss: 0.0093 , running loss:0.05106538771651685 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 674, training loss: 0.0548 , running loss:0.0532031727489084 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 675, training loss: 0.0124 , running loss:0.049372775154188274 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 676, training loss: 0.0111 , running loss:0.048816662887111305 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 677, training loss: 0.0487 , running loss:0.050797805981710555 (0.9861111111111112, 0.8888888888888888, 0.875)\n",
      "Epoch : 0 ,Iteration : 678, training loss: 0.0088 , running loss:0.04611579901538789 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 679, training loss: 0.1399 , running loss:0.05007005794905126 (0.9722222222222222, 0.7777777777777778, 0.75)\n",
      "Epoch : 0 ,Iteration : 680, training loss: 0.0372 , running loss:0.04930968671105802 (0.9861111111111112, 0.8888888888888888, 0.875)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_73975/1592086564.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_73975/1592086564.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(base_model, train_dataloader, val_dataloader, optimizer, loss_function)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/legal_env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_73975/339400833.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/legal_env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/legal_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         )\n\u001b[0;32m--> 991\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    992\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/legal_env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/legal_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 )\n\u001b[1;32m    581\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    583\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/legal_env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/legal_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/legal_env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/legal_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         )\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/legal_env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/legal_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/legal_env/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/legal_env/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(base_model,train_dataloader,val_dataloader,optimizer,loss_function):\n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    valid_interval = 20  # Perform validation and save model every 10 iterations\n",
    "    iteration = 0\n",
    "    max_f1 = 100000000\n",
    "    stop_criterion = 2000000\n",
    "\n",
    "    running_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        base_model.train()  # Set the model to training mode\n",
    "        for curr_batch in train_dataloader:\n",
    "            \n",
    "            if iteration > stop_criterion:\n",
    "                break\n",
    "            \n",
    "            input_ids = curr_batch['input_ids'].to(device)\n",
    "            attention_mask = curr_batch['attention_mask'].to(device)\n",
    "            targets = curr_batch['multi_label'].to(device)\n",
    "\n",
    "\n",
    "\n",
    "            outputs = base_model(input_ids,attention_mask)\n",
    "            loss = loss_function(outputs.to(device), targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #print (iteration)\n",
    "            running_loss.append(loss.item())\n",
    "            if len(running_loss) > 20:\n",
    "                running_loss.pop(0)\n",
    "            print (f\"Epoch : {epoch} ,Iteration : {iteration}, training loss: {loss:.4f} , running loss:{sum(running_loss)/len(running_loss)}\",find_metrics(targets,outputs))\n",
    "            \n",
    "            # freeing up excess memory\n",
    "            del loss, outputs\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \n",
    "            # Validation and model saving\n",
    "            if iteration % valid_interval == 0:\n",
    "                base_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    total_loss = []\n",
    "                    f1_micro = []\n",
    "                    f1_macro = []\n",
    "                    f1_avg = []\n",
    "                    for val_batch in val_dataloader:\n",
    "                        val_input_ids = val_batch['input_ids'].to(device)\n",
    "                        val_attention_mask = val_batch['attention_mask'].to(device)\n",
    "                        val_targets = val_batch['multi_label'].to(device)\n",
    "\n",
    "\n",
    "                        outputs = base_model(val_input_ids,val_attention_mask)\n",
    "                        loss = loss_function(outputs.to(device), val_targets)\n",
    "                        \n",
    "                        total_loss.append(loss.item())\n",
    "                        val_out = find_metrics(val_targets,outputs)\n",
    "                        f1_micro.append(val_out[0])\n",
    "                        f1_macro.append(val_out[1])\n",
    "                        f1_avg.append(val_out[2])\n",
    "                        \n",
    "                        # emptying memory\n",
    "                        del val_out, loss, outputs\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                    avg_acc = sum(f1_avg)/len(f1_avg)\n",
    "                    avg_f1mic = sum(f1_micro)/len(f1_micro)\n",
    "                    avg_f1mac = sum(f1_macro)/len(f1_macro)\n",
    "                    avg_loss = sum(total_loss)/len(total_loss)\n",
    "                    \n",
    "                    print (f\"Validation loss : {sum(total_loss)/len(total_loss)} \", ' ,acc : ',avg_acc,\" ,f1-micro : \",avg_f1mic,\" ,f1-macro : \",avg_f1mac)\n",
    "                    if max_f1 >  (sum(total_loss)/len(total_loss)):\n",
    "                        max_f1 = avg_f1mic\n",
    "                        torch.save(base_model.state_dict(),f\"sector_dataset/nine_c/model_{iteration}.pth\")\n",
    "                    \n",
    "                    del total_loss, f1_micro, f1_macro, f1_avg\n",
    "\n",
    "                base_model.train()  # Set the model back to training mode\n",
    "            \n",
    "            iteration += 1\n",
    "    return base_model, optimizer, loss_function\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model.to(device)\n",
    "base_model, optimizer, loss_function = train(base_model,train_dataloader,val_dataloader,optimizer,loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0c143b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='sector'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing the model\n",
    "pd.DataFrame.from_dict(train_dataset)[\"sector\"].value_counts().plot.pie(autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328025cd",
   "metadata": {},
   "source": [
    "## BASELINE MODEL: SECTOR WISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6868adea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 0, training loss: 0.7329 , running loss:0.7328974008560181 (0.47500000000000003, 0.1, 0.0)\n",
      "Validation loss : 0.6665784772540118   ,acc :  0.09237536656891496  ,f1-micro :  0.6412756598240467  ,f1-macro :  0.2607647517031683\n",
      "Epoch : 0 ,Iteration : 1, training loss: 0.6576 , running loss:0.6952400803565979 (0.7, 0.1942857142857143, 0.25)\n",
      "Epoch : 0 ,Iteration : 2, training loss: 0.6358 , running loss:0.6754293441772461 (0.6, 0.3244444444444444, 0.0)\n",
      "Epoch : 0 ,Iteration : 3, training loss: 0.5636 , running loss:0.647464245557785 (0.7250000000000001, 0.3888888888888889, 0.0)\n",
      "Epoch : 0 ,Iteration : 4, training loss: 0.5958 , running loss:0.6371255993843079 (0.675, 0.5777777777777777, 0.0)\n",
      "Epoch : 0 ,Iteration : 5, training loss: 0.5562 , running loss:0.6236393849054972 (0.675, 0.16, 0.25)\n",
      "Epoch : 0 ,Iteration : 6, training loss: 0.4837 , running loss:0.6036439069679805 (0.75, 0.5888888888888889, 0.25)\n",
      "Epoch : 0 ,Iteration : 7, training loss: 0.5337 , running loss:0.5949005894362926 (0.675, 0.32, 0.0)\n",
      "Epoch : 0 ,Iteration : 8, training loss: 0.4601 , running loss:0.5799240900410546 (0.775, 0.5890909090909091, 0.375)\n",
      "Epoch : 0 ,Iteration : 9, training loss: 0.4764 , running loss:0.5695696204900742 (0.75, 0.27999999999999997, 0.125)\n",
      "Epoch : 0 ,Iteration : 10, training loss: 0.4865 , running loss:0.5620144795287739 (0.875, 0.37777777777777777, 0.5)\n",
      "Epoch : 0 ,Iteration : 11, training loss: 0.5015 , running loss:0.556970072289308 (0.775, 0.3090909090909091, 0.375)\n",
      "Epoch : 0 ,Iteration : 12, training loss: 0.5037 , running loss:0.5528725041792943 (0.7250000000000001, 0.25, 0.125)\n",
      "Epoch : 0 ,Iteration : 13, training loss: 0.4688 , running loss:0.5468668639659882 (0.8000000000000002, 0.45714285714285713, 0.125)\n",
      "Epoch : 0 ,Iteration : 14, training loss: 0.4613 , running loss:0.5411652604738871 (0.8000000000000002, 0.3, 0.25)\n",
      "Epoch : 0 ,Iteration : 15, training loss: 0.4688 , running loss:0.5366393867880106 (0.825, 0.5, 0.125)\n",
      "Epoch : 0 ,Iteration : 16, training loss: 0.4641 , running loss:0.5323713018613703 (0.7250000000000001, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 17, training loss: 0.4245 , running loss:0.5263798733552297 (0.775, 0.45, 0.125)\n",
      "Epoch : 0 ,Iteration : 18, training loss: 0.3474 , running loss:0.5169608012626046 (0.825, 0.65, 0.125)\n",
      "Epoch : 0 ,Iteration : 19, training loss: 0.4708 , running loss:0.5146537378430367 (0.7250000000000001, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 20, training loss: 0.4983 , running loss:0.5029224857687951 (0.775, 0.3, 0.25)\n",
      "Validation loss : 0.3811029201029333   ,acc :  0.6268328445747801  ,f1-micro :  0.8605571847507336  ,f1-macro :  0.6523289125341915\n",
      "Epoch : 0 ,Iteration : 21, training loss: 0.5448 , running loss:0.4972848162055016 (0.7250000000000001, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 22, training loss: 0.5142 , running loss:0.49120480567216873 (0.7250000000000001, 0.28888888888888886, 0.25)\n",
      "Epoch : 0 ,Iteration : 23, training loss: 0.5090 , running loss:0.4884770318865776 (0.7250000000000001, 0.4666666666666667, 0.125)\n",
      "Epoch : 0 ,Iteration : 24, training loss: 0.4017 , running loss:0.47877419292926787 (0.825, 0.5, 0.25)\n",
      "Epoch : 0 ,Iteration : 25, training loss: 0.4466 , running loss:0.4732946023344994 (0.8000000000000002, 0.5142857142857143, 0.25)\n",
      "Epoch : 0 ,Iteration : 26, training loss: 0.4702 , running loss:0.4726209342479706 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 27, training loss: 0.4355 , running loss:0.46771010756492615 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 28, training loss: 0.4952 , running loss:0.46946422159671786 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 29, training loss: 0.3921 , running loss:0.4652520552277565 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 30, training loss: 0.4025 , running loss:0.4610537976026535 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 31, training loss: 0.4133 , running loss:0.45664709657430647 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 32, training loss: 0.4019 , running loss:0.4515575036406517 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 33, training loss: 0.4070 , running loss:0.4484699428081512 (0.8000000000000002, 0.26666666666666666, 0.125)\n",
      "Epoch : 0 ,Iteration : 34, training loss: 0.5242 , running loss:0.45161456763744356 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 35, training loss: 0.4541 , running loss:0.4508818581700325 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 36, training loss: 0.4273 , running loss:0.44904418140649793 (0.75, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 37, training loss: 0.4141 , running loss:0.44852048307657244 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 38, training loss: 0.5317 , running loss:0.4577329993247986 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 39, training loss: 0.4695 , running loss:0.457666851580143 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 40, training loss: 0.4061 , running loss:0.45305670946836474 (0.8000000000000002, 0.2, 0.0)\n",
      "Validation loss : 0.3989653222022518   ,acc :  0.0  ,f1-micro :  0.7979472140762514  ,f1-macro :  0.4947214076246338\n",
      "Epoch : 0 ,Iteration : 41, training loss: 0.3876 , running loss:0.4451941803097725 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 42, training loss: 0.3807 , running loss:0.4385202541947365 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 43, training loss: 0.4229 , running loss:0.43421596586704253 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 44, training loss: 0.4476 , running loss:0.4365093544125557 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 45, training loss: 0.4796 , running loss:0.4381575256586075 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 46, training loss: 0.4496 , running loss:0.43712947368621824 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 47, training loss: 0.4315 , running loss:0.4369293376803398 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 48, training loss: 0.4389 , running loss:0.4341171011328697 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 49, training loss: 0.3814 , running loss:0.43357968479394915 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 50, training loss: 0.4782 , running loss:0.4373639553785324 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 51, training loss: 0.5155 , running loss:0.44247020930051806 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 52, training loss: 0.4983 , running loss:0.4472914576530457 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 53, training loss: 0.4597 , running loss:0.44992673844099046 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 54, training loss: 0.4640 , running loss:0.44691721349954605 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 55, training loss: 0.4438 , running loss:0.4464007765054703 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 56, training loss: 0.4666 , running loss:0.44836596995592115 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 57, training loss: 0.4297 , running loss:0.44915064573287966 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 58, training loss: 0.4402 , running loss:0.44457888752222063 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 59, training loss: 0.4321 , running loss:0.44271050691604613 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 60, training loss: 0.4592 , running loss:0.44536792784929274 (0.8000000000000002, 0.2, 0.0)\n",
      "Validation loss : 0.4184373371587121   ,acc :  0.0  ,f1-micro :  0.7997800586510314  ,f1-macro :  0.49149560117302143\n",
      "Epoch : 0 ,Iteration : 61, training loss: 0.4563 , running loss:0.44880295544862747 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 62, training loss: 0.4447 , running loss:0.4520017489790916 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 63, training loss: 0.4634 , running loss:0.4540270194411278 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 64, training loss: 0.4434 , running loss:0.4538175776600838 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 65, training loss: 0.4647 , running loss:0.45307164192199706 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 66, training loss: 0.4283 , running loss:0.4520072877407074 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 67, training loss: 0.3997 , running loss:0.45041907131671904 (0.8000000000000002, 0.2, 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 68, training loss: 0.3949 , running loss:0.44821417033672334 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 69, training loss: 0.4650 , running loss:0.45239667147397994 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 70, training loss: 0.4969 , running loss:0.4533301338553429 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 71, training loss: 0.4456 , running loss:0.4498357519507408 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 72, training loss: 0.4496 , running loss:0.4473974660038948 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 73, training loss: 0.4218 , running loss:0.44550176411867143 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 74, training loss: 0.4531 , running loss:0.4449566647410393 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 75, training loss: 0.4308 , running loss:0.44430653750896454 (0.75, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 76, training loss: 0.4034 , running loss:0.44114451557397844 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 77, training loss: 0.4119 , running loss:0.44025368690490724 (0.8000000000000002, 0.4666666666666667, 0.125)\n",
      "Epoch : 0 ,Iteration : 78, training loss: 0.4511 , running loss:0.4407989621162415 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 79, training loss: 0.3466 , running loss:0.4365201607346535 (0.825, 0.45714285714285713, 0.125)\n",
      "Epoch : 0 ,Iteration : 80, training loss: 0.3696 , running loss:0.4320401281118393 (0.825, 0.7, 0.25)\n",
      "Validation loss : 0.3685019274960515   ,acc :  0.006964809384164223  ,f1-micro :  0.8002932551319694  ,f1-macro :  0.49298561653400413\n",
      "Epoch : 0 ,Iteration : 81, training loss: 0.5225 , running loss:0.43535336554050447 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 82, training loss: 0.4429 , running loss:0.4352624610066414 (0.8000000000000002, 0.27999999999999997, 0.125)\n",
      "Epoch : 0 ,Iteration : 83, training loss: 0.4078 , running loss:0.4324798047542572 (0.825, 0.27999999999999997, 0.125)\n",
      "Epoch : 0 ,Iteration : 84, training loss: 0.4271 , running loss:0.43166489601135255 (0.8000000000000002, 0.5142857142857143, 0.25)\n",
      "Epoch : 0 ,Iteration : 85, training loss: 0.4049 , running loss:0.4286761999130249 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 86, training loss: 0.5154 , running loss:0.4330310136079788 (0.75, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 87, training loss: 0.4774 , running loss:0.43691644072532654 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 88, training loss: 0.4246 , running loss:0.438406340777874 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 89, training loss: 0.4250 , running loss:0.4364026695489883 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 90, training loss: 0.4026 , running loss:0.4316879719495773 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 91, training loss: 0.4261 , running loss:0.43071592748165133 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 92, training loss: 0.3519 , running loss:0.42583052068948746 (0.825, 0.4666666666666667, 0.125)\n",
      "Epoch : 0 ,Iteration : 93, training loss: 0.3849 , running loss:0.4239859089255333 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 94, training loss: 0.4180 , running loss:0.42223006337881086 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 95, training loss: 0.4265 , running loss:0.42201450914144517 (0.825, 0.27999999999999997, 0.125)\n",
      "Epoch : 0 ,Iteration : 96, training loss: 0.3256 , running loss:0.4181240499019623 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 97, training loss: 0.3992 , running loss:0.4174878135323524 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 98, training loss: 0.3555 , running loss:0.41270582675933837 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 99, training loss: 0.3668 , running loss:0.41371940821409225 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 100, training loss: 0.5601 , running loss:0.4232421085238457 (0.75, 0.4, 0.0)\n",
      "Validation loss : 0.37422606934550223   ,acc :  0.009897360703812317  ,f1-micro :  0.8004398826979512  ,f1-macro :  0.49198622166364164\n",
      "Epoch : 0 ,Iteration : 101, training loss: 0.4063 , running loss:0.41742835342884066 (0.75, 0.45714285714285713, 0.125)\n",
      "Epoch : 0 ,Iteration : 102, training loss: 0.4525 , running loss:0.41790836602449416 (0.825, 0.3333333333333333, 0.25)\n",
      "Epoch : 0 ,Iteration : 103, training loss: 0.4043 , running loss:0.4177342027425766 (0.8000000000000002, 0.5142857142857143, 0.25)\n",
      "Epoch : 0 ,Iteration : 104, training loss: 0.3716 , running loss:0.41495921462774277 (0.9, 0.5777777777777777, 0.5)\n",
      "Epoch : 0 ,Iteration : 105, training loss: 0.4818 , running loss:0.4188056319952011 (0.8000000000000002, 0.5142857142857143, 0.25)\n",
      "Epoch : 0 ,Iteration : 106, training loss: 0.4200 , running loss:0.41403292715549467 (0.825, 0.5142857142857143, 0.25)\n",
      "Epoch : 0 ,Iteration : 107, training loss: 0.4905 , running loss:0.4146869361400604 (0.75, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 108, training loss: 0.4795 , running loss:0.41743025928735733 (0.75, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 109, training loss: 0.5045 , running loss:0.4214078575372696 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 110, training loss: 0.4522 , running loss:0.4238888889551163 (0.8000000000000002, 0.4666666666666667, 0.125)\n",
      "Epoch : 0 ,Iteration : 111, training loss: 0.3538 , running loss:0.4202692091464996 (0.825, 0.45714285714285713, 0.125)\n",
      "Epoch : 0 ,Iteration : 112, training loss: 0.4540 , running loss:0.42537754476070405 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 113, training loss: 0.5144 , running loss:0.4318512424826622 (0.75, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 114, training loss: 0.4062 , running loss:0.4312592849135399 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 115, training loss: 0.4901 , running loss:0.43444201201200483 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 116, training loss: 0.4071 , running loss:0.4385159730911255 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 117, training loss: 0.4410 , running loss:0.4406082764267921 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 118, training loss: 0.4525 , running loss:0.4454582378268242 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 119, training loss: 0.4619 , running loss:0.45021006315946577 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 120, training loss: 0.4158 , running loss:0.4429969608783722 (0.8000000000000002, 0.6, 0.0)\n",
      "Validation loss : 0.4052684195230434   ,acc :  0.0  ,f1-micro :  0.800000000000005  ,f1-macro :  0.4961876832844578\n",
      "Epoch : 0 ,Iteration : 121, training loss: 0.3850 , running loss:0.44193493872880935 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 122, training loss: 0.4321 , running loss:0.4409144833683968 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 123, training loss: 0.5419 , running loss:0.4477938175201416 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 124, training loss: 0.4320 , running loss:0.45081315338611605 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 125, training loss: 0.4527 , running loss:0.44935816079378127 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 126, training loss: 0.4481 , running loss:0.45076211988925935 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 127, training loss: 0.4773 , running loss:0.45010402500629426 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 128, training loss: 0.4005 , running loss:0.4461534693837166 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 129, training loss: 0.4403 , running loss:0.44294108301401136 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 130, training loss: 0.4245 , running loss:0.4415556609630585 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 131, training loss: 0.3859 , running loss:0.44316336512565613 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 132, training loss: 0.4806 , running loss:0.44449246525764463 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 133, training loss: 0.4118 , running loss:0.439362196624279 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 134, training loss: 0.5087 , running loss:0.44449006468057634 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 135, training loss: 0.5617 , running loss:0.44806865602731705 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 136, training loss: 0.3914 , running loss:0.44728376269340514 (0.8000000000000002, 0.4, 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 137, training loss: 0.4195 , running loss:0.4462075799703598 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 138, training loss: 0.3658 , running loss:0.4418709561228752 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 139, training loss: 0.4719 , running loss:0.442372165620327 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 140, training loss: 0.4265 , running loss:0.44290619492530825 (0.8000000000000002, 0.4, 0.0)\n",
      "Validation loss : 0.37155515846968395   ,acc :  0.0  ,f1-micro :  0.800000000000005  ,f1-macro :  0.49325513196480986\n",
      "Epoch : 0 ,Iteration : 141, training loss: 0.3639 , running loss:0.44185095429420473 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 142, training loss: 0.4898 , running loss:0.4447348594665527 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 143, training loss: 0.4959 , running loss:0.4424359664320946 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 144, training loss: 0.3967 , running loss:0.44067293852567674 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 145, training loss: 0.4432 , running loss:0.440197166800499 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 146, training loss: 0.4797 , running loss:0.4417764961719513 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 147, training loss: 0.3936 , running loss:0.43759031146764754 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 148, training loss: 0.4230 , running loss:0.4387165099382401 (0.825, 0.48, 0.125)\n",
      "Epoch : 0 ,Iteration : 149, training loss: 0.4036 , running loss:0.4368850067257881 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 150, training loss: 0.4876 , running loss:0.44004025757312776 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 151, training loss: 0.4200 , running loss:0.4417462795972824 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 152, training loss: 0.4254 , running loss:0.4389850914478302 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 153, training loss: 0.3781 , running loss:0.4373016655445099 (0.825, 0.4666666666666667, 0.125)\n",
      "Epoch : 0 ,Iteration : 154, training loss: 0.3777 , running loss:0.4307501181960106 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 155, training loss: 0.4519 , running loss:0.42526227682828904 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 156, training loss: 0.4580 , running loss:0.4285917803645134 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 157, training loss: 0.4599 , running loss:0.43061177879571916 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 158, training loss: 0.4253 , running loss:0.433589780330658 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 159, training loss: 0.4350 , running loss:0.43174475580453875 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 160, training loss: 0.4628 , running loss:0.43356087803840637 (0.8000000000000002, 0.2, 0.0)\n",
      "Validation loss : 0.3875675585962111   ,acc :  0.0  ,f1-micro :  0.800000000000005  ,f1-macro :  0.49677419354838726\n",
      "Epoch : 0 ,Iteration : 161, training loss: 0.4515 , running loss:0.437938492000103 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 162, training loss: 0.4432 , running loss:0.4356109663844109 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 163, training loss: 0.3875 , running loss:0.4301911279559135 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 164, training loss: 0.4779 , running loss:0.4342490419745445 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 165, training loss: 0.4471 , running loss:0.43444283604621886 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 166, training loss: 0.4895 , running loss:0.43493517488241196 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 167, training loss: 0.4451 , running loss:0.43750822693109515 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 168, training loss: 0.4345 , running loss:0.43808275312185285 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 169, training loss: 0.4064 , running loss:0.43821988850831983 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 170, training loss: 0.4378 , running loss:0.43573048114776614 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 171, training loss: 0.4625 , running loss:0.43785217106342317 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 172, training loss: 0.4338 , running loss:0.43827072978019715 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 173, training loss: 0.4107 , running loss:0.43989729285240176 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 174, training loss: 0.4169 , running loss:0.44185472279787064 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 175, training loss: 0.3674 , running loss:0.437628410756588 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 176, training loss: 0.4126 , running loss:0.4353609666228294 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 177, training loss: 0.4004 , running loss:0.4323829561471939 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 178, training loss: 0.4105 , running loss:0.43164278715848925 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 179, training loss: 0.4150 , running loss:0.43064318150281905 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 180, training loss: 0.4423 , running loss:0.4296198770403862 (0.8000000000000002, 0.4, 0.0)\n",
      "Validation loss : 0.40410729473636997   ,acc :  0.0  ,f1-micro :  0.800000000000005  ,f1-macro :  0.49266862170088027\n",
      "Epoch : 0 ,Iteration : 181, training loss: 0.4395 , running loss:0.4290210008621216 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 182, training loss: 0.3786 , running loss:0.42578941583633423 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 183, training loss: 0.4112 , running loss:0.4269751623272896 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 184, training loss: 0.3367 , running loss:0.41991483271121977 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 185, training loss: 0.4300 , running loss:0.4190612345933914 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 186, training loss: 0.4088 , running loss:0.4150276556611061 (0.825, 0.48, 0.125)\n",
      "Epoch : 0 ,Iteration : 187, training loss: 0.3987 , running loss:0.41270698606967926 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 188, training loss: 0.4529 , running loss:0.4136262506246567 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 189, training loss: 0.3651 , running loss:0.4115620762109756 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 190, training loss: 0.5207 , running loss:0.41570542603731153 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 191, training loss: 0.5555 , running loss:0.4203553482890129 (0.7, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 192, training loss: 0.4838 , running loss:0.42285587042570116 (0.775, 0.31428571428571433, 0.25)\n",
      "Epoch : 0 ,Iteration : 193, training loss: 0.5334 , running loss:0.42899100184440614 (0.75, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 194, training loss: 0.4360 , running loss:0.42994582951068877 (0.8000000000000002, 0.26666666666666666, 0.125)\n",
      "Epoch : 0 ,Iteration : 195, training loss: 0.3868 , running loss:0.4309164747595787 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 196, training loss: 0.3922 , running loss:0.4298943042755127 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 197, training loss: 0.4182 , running loss:0.430784872174263 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 198, training loss: 0.3515 , running loss:0.4278337135910988 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 199, training loss: 0.4751 , running loss:0.43084144592285156 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 200, training loss: 0.3553 , running loss:0.4264903232455254 (0.8000000000000002, 0.6, 0.0)\n",
      "Validation loss : 0.41270817453560593   ,acc :  0.0  ,f1-micro :  0.800000000000005  ,f1-macro :  0.4967741935483876\n",
      "Epoch : 0 ,Iteration : 201, training loss: 0.4140 , running loss:0.42521587759256363 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 202, training loss: 0.4256 , running loss:0.42756816297769545 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 203, training loss: 0.4581 , running loss:0.42991221994161605 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 204, training loss: 0.3794 , running loss:0.4320501729846001 (0.8000000000000002, 0.4, 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 205, training loss: 0.4241 , running loss:0.4317564070224762 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 206, training loss: 0.4648 , running loss:0.43455655723810194 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 207, training loss: 0.4284 , running loss:0.43604459762573244 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 208, training loss: 0.3971 , running loss:0.4332550898194313 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 209, training loss: 0.4418 , running loss:0.43708901256322863 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 210, training loss: 0.4028 , running loss:0.4311955586075783 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 211, training loss: 0.4174 , running loss:0.42429336309432986 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 212, training loss: 0.4065 , running loss:0.420429439842701 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 213, training loss: 0.5499 , running loss:0.4212558701634407 (0.75, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 214, training loss: 0.3868 , running loss:0.4187956526875496 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 215, training loss: 0.4077 , running loss:0.4198420405387878 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 216, training loss: 0.3711 , running loss:0.4187865868210793 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 217, training loss: 0.4248 , running loss:0.41911746859550475 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 218, training loss: 0.4444 , running loss:0.42376449704170227 (0.75, 0.45714285714285713, 0.125)\n",
      "Epoch : 0 ,Iteration : 219, training loss: 0.4012 , running loss:0.4200666695833206 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 220, training loss: 0.3968 , running loss:0.422140434384346 (0.775, 0.4, 0.0)\n",
      "Validation loss : 0.3644597558331979   ,acc :  0.004398826979472141  ,f1-micro :  0.8003665689149608  ,f1-macro :  0.4961462551785138\n",
      "Epoch : 0 ,Iteration : 221, training loss: 0.3999 , running loss:0.42143732458353045 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 222, training loss: 0.3307 , running loss:0.4166908159852028 (0.85, 0.7142857142857143, 0.25)\n",
      "Epoch : 0 ,Iteration : 223, training loss: 0.4151 , running loss:0.4145405933260918 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 224, training loss: 0.4837 , running loss:0.41975534409284593 (0.75, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 225, training loss: 0.3994 , running loss:0.4185212954878807 (0.8000000000000002, 0.48, 0.125)\n",
      "Epoch : 0 ,Iteration : 226, training loss: 0.4532 , running loss:0.4179374173283577 (0.8000000000000002, 0.27999999999999997, 0.125)\n",
      "Epoch : 0 ,Iteration : 227, training loss: 0.4028 , running loss:0.4166543573141098 (0.8000000000000002, 0.26666666666666666, 0.125)\n",
      "Epoch : 0 ,Iteration : 228, training loss: 0.3927 , running loss:0.4164327263832092 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 229, training loss: 0.4799 , running loss:0.41833766251802446 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 230, training loss: 0.3815 , running loss:0.4172723039984703 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 231, training loss: 0.3712 , running loss:0.4149618610739708 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 232, training loss: 0.4578 , running loss:0.4175282269716263 (0.8000000000000002, 0.27999999999999997, 0.125)\n",
      "Epoch : 0 ,Iteration : 233, training loss: 0.4754 , running loss:0.41380281895399096 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 234, training loss: 0.4366 , running loss:0.4162972331047058 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 235, training loss: 0.4768 , running loss:0.41975084096193316 (0.8000000000000002, 0.48, 0.125)\n",
      "Epoch : 0 ,Iteration : 236, training loss: 0.4452 , running loss:0.4234604090452194 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 237, training loss: 0.4985 , running loss:0.4271453216671944 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 238, training loss: 0.4514 , running loss:0.42749523669481276 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 239, training loss: 0.4069 , running loss:0.4277789190411568 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 240, training loss: 0.5949 , running loss:0.43768205493688583 (0.8000000000000002, 0.4, 0.0)\n",
      "Validation loss : 0.3985666326763343   ,acc :  0.0  ,f1-micro :  0.800000000000005  ,f1-macro :  0.4844574780058656\n",
      "Epoch : 0 ,Iteration : 241, training loss: 0.3888 , running loss:0.43712488412857053 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 242, training loss: 0.4618 , running loss:0.4436821401119232 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 243, training loss: 0.4149 , running loss:0.44367441684007647 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 244, training loss: 0.3914 , running loss:0.43906044214963913 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 245, training loss: 0.4748 , running loss:0.442829529941082 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 246, training loss: 0.3868 , running loss:0.43950913548469545 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 247, training loss: 0.4144 , running loss:0.44009213149547577 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 248, training loss: 0.4858 , running loss:0.44474887251853945 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 249, training loss: 0.4220 , running loss:0.44185694456100466 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 250, training loss: 0.3854 , running loss:0.44205478876829146 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 251, training loss: 0.4216 , running loss:0.44457588642835616 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 252, training loss: 0.4029 , running loss:0.44183031767606734 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 253, training loss: 0.4214 , running loss:0.439130437374115 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 254, training loss: 0.3755 , running loss:0.43607263416051867 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 255, training loss: 0.4556 , running loss:0.43501434326171873 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 256, training loss: 0.4187 , running loss:0.43368956744670867 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 257, training loss: 0.4208 , running loss:0.4298036709427834 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 258, training loss: 0.4617 , running loss:0.4303183451294899 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 259, training loss: 0.4439 , running loss:0.43217125087976455 (0.825, 0.3, 0.125)\n",
      "Epoch : 0 ,Iteration : 260, training loss: 0.4762 , running loss:0.426238414645195 (0.8000000000000002, 0.2, 0.0)\n",
      "Validation loss : 0.3696191753110578   ,acc :  0.0  ,f1-micro :  0.800000000000005  ,f1-macro :  0.4868035190615843\n",
      "Epoch : 0 ,Iteration : 261, training loss: 0.3925 , running loss:0.4264219909906387 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 262, training loss: 0.4438 , running loss:0.4255190476775169 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 263, training loss: 0.4274 , running loss:0.4261417120695114 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 264, training loss: 0.4442 , running loss:0.4287780836224556 (0.8000000000000002, 0.27999999999999997, 0.125)\n",
      "Epoch : 0 ,Iteration : 265, training loss: 0.4105 , running loss:0.42556092888116837 (0.825, 0.5, 0.125)\n",
      "Epoch : 0 ,Iteration : 266, training loss: 0.4027 , running loss:0.4263600394129753 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 267, training loss: 0.3697 , running loss:0.42412196695804594 (0.825, 0.4666666666666667, 0.125)\n",
      "Epoch : 0 ,Iteration : 268, training loss: 0.5193 , running loss:0.4257981613278389 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 269, training loss: 0.3974 , running loss:0.424568609893322 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 270, training loss: 0.3845 , running loss:0.42452032715082166 (0.825, 0.48, 0.125)\n",
      "Epoch : 0 ,Iteration : 271, training loss: 0.4436 , running loss:0.42562144845724104 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 272, training loss: 0.4070 , running loss:0.4258267909288406 (0.775, 0.2571428571428572, 0.125)\n",
      "Epoch : 0 ,Iteration : 273, training loss: 0.4318 , running loss:0.42634738236665726 (0.75, 0.4, 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 274, training loss: 0.4716 , running loss:0.43115499168634414 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 275, training loss: 0.4092 , running loss:0.42883462458848953 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 276, training loss: 0.4268 , running loss:0.4292361125349998 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 277, training loss: 0.3883 , running loss:0.42761215567588806 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 278, training loss: 0.4561 , running loss:0.4273322731256485 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 279, training loss: 0.4429 , running loss:0.4272786855697632 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 280, training loss: 0.3641 , running loss:0.42167242914438247 (0.775, 0.4, 0.0)\n",
      "Validation loss : 0.36489840808851626   ,acc :  0.0  ,f1-micro :  0.800000000000005  ,f1-macro :  0.4897360703812319\n",
      "Epoch : 0 ,Iteration : 281, training loss: 0.4430 , running loss:0.42419646978378295 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 282, training loss: 0.3342 , running loss:0.4187185525894165 (0.775, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 283, training loss: 0.5038 , running loss:0.4225378930568695 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 284, training loss: 0.4236 , running loss:0.421510049700737 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 285, training loss: 0.4123 , running loss:0.42160017490386964 (0.75, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 286, training loss: 0.3330 , running loss:0.4181124195456505 (0.825, 0.45714285714285713, 0.125)\n",
      "Epoch : 0 ,Iteration : 287, training loss: 0.3762 , running loss:0.418439145386219 (0.8000000000000002, 0.45714285714285713, 0.125)\n",
      "Epoch : 0 ,Iteration : 288, training loss: 0.4184 , running loss:0.4133934751152992 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 289, training loss: 0.4535 , running loss:0.41619785875082016 (0.75, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 290, training loss: 0.4246 , running loss:0.4182028040289879 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 291, training loss: 0.5120 , running loss:0.4216195583343506 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 292, training loss: 0.3986 , running loss:0.4211978778243065 (0.8000000000000002, 0.45714285714285713, 0.125)\n",
      "Epoch : 0 ,Iteration : 293, training loss: 0.3947 , running loss:0.41934324651956556 (0.8000000000000002, 0.45714285714285713, 0.125)\n",
      "Epoch : 0 ,Iteration : 294, training loss: 0.3842 , running loss:0.4149690389633179 (0.825, 0.2666666666666667, 0.125)\n",
      "Epoch : 0 ,Iteration : 295, training loss: 0.4223 , running loss:0.4156236410140991 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 296, training loss: 0.4374 , running loss:0.4161545321345329 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 297, training loss: 0.4387 , running loss:0.4186747848987579 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 298, training loss: 0.4695 , running loss:0.4193443521857262 (0.775, 0.27999999999999997, 0.125)\n",
      "Epoch : 0 ,Iteration : 299, training loss: 0.4972 , running loss:0.4220623478293419 (0.75, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 300, training loss: 0.4429 , running loss:0.4260045990347862 (0.825, 0.3333333333333333, 0.25)\n",
      "Validation loss : 0.3575400868294176   ,acc :  0.0018328445747800588  ,f1-micro :  0.8002932551319697  ,f1-macro :  0.48835078899595125\n",
      "Epoch : 0 ,Iteration : 301, training loss: 0.4384 , running loss:0.42577808499336245 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 302, training loss: 0.3369 , running loss:0.4259120151400566 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 303, training loss: 0.4396 , running loss:0.4227010369300842 (0.825, 0.5142857142857143, 0.25)\n",
      "Epoch : 0 ,Iteration : 304, training loss: 0.4617 , running loss:0.42460294216871264 (0.775, 0.26666666666666666, 0.125)\n",
      "Epoch : 0 ,Iteration : 305, training loss: 0.4026 , running loss:0.42411766946315765 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 306, training loss: 0.3782 , running loss:0.42637653946876525 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 307, training loss: 0.4410 , running loss:0.42961558848619463 (0.825, 0.3, 0.125)\n",
      "Epoch : 0 ,Iteration : 308, training loss: 0.4229 , running loss:0.4298407956957817 (0.825, 0.3333333333333333, 0.125)\n",
      "Epoch : 0 ,Iteration : 309, training loss: 0.4753 , running loss:0.43092901110649107 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 310, training loss: 0.4190 , running loss:0.43065155297517776 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 311, training loss: 0.3979 , running loss:0.4249494060873985 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 312, training loss: 0.4287 , running loss:0.4264575198292732 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 313, training loss: 0.4069 , running loss:0.4270682156085968 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 314, training loss: 0.4527 , running loss:0.43049644827842715 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 315, training loss: 0.4334 , running loss:0.4310495600104332 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 316, training loss: 0.4290 , running loss:0.43063024431467056 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 317, training loss: 0.4094 , running loss:0.42916461676359174 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 318, training loss: 0.3859 , running loss:0.42498416304588316 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 319, training loss: 0.4874 , running loss:0.42449350357055665 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 320, training loss: 0.4320 , running loss:0.4239451587200165 (0.8000000000000002, 0.2, 0.0)\n",
      "Validation loss : 0.38505446666147003   ,acc :  0.0  ,f1-micro :  0.800000000000005  ,f1-macro :  0.4938416422287398\n",
      "Epoch : 0 ,Iteration : 321, training loss: 0.4613 , running loss:0.42508688271045686 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 322, training loss: 0.4987 , running loss:0.43317658454179764 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 323, training loss: 0.4047 , running loss:0.4314320147037506 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 324, training loss: 0.4785 , running loss:0.43227638602256774 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 325, training loss: 0.4559 , running loss:0.4349445566534996 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 326, training loss: 0.4092 , running loss:0.43649598956108093 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 327, training loss: 0.4409 , running loss:0.436490373313427 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 328, training loss: 0.4478 , running loss:0.4377316400408745 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 329, training loss: 0.5172 , running loss:0.4398273378610611 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 330, training loss: 0.4791 , running loss:0.4428328096866608 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 331, training loss: 0.4925 , running loss:0.4475592732429504 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 332, training loss: 0.4468 , running loss:0.448459991812706 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 333, training loss: 0.5073 , running loss:0.4534759968519211 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 334, training loss: 0.4602 , running loss:0.4538488730788231 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 335, training loss: 0.4449 , running loss:0.45442508310079577 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 336, training loss: 0.4032 , running loss:0.4531345158815384 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 337, training loss: 0.4244 , running loss:0.4538871839642525 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 338, training loss: 0.4254 , running loss:0.45586083382368087 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 339, training loss: 0.4300 , running loss:0.45299012809991834 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 340, training loss: 0.4555 , running loss:0.4541682630777359 (0.8000000000000002, 0.2, 0.0)\n",
      "Validation loss : 0.41381548594169953   ,acc :  0.0  ,f1-micro :  0.800000000000005  ,f1-macro :  0.4932551319648102\n",
      "Epoch : 0 ,Iteration : 341, training loss: 0.4446 , running loss:0.4533341392874718 (0.8000000000000002, 0.4, 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 342, training loss: 0.4160 , running loss:0.4491993010044098 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 343, training loss: 0.4271 , running loss:0.4503189042210579 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 344, training loss: 0.4001 , running loss:0.44639691412448884 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 345, training loss: 0.3813 , running loss:0.4426635652780533 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 346, training loss: 0.4623 , running loss:0.4453192338347435 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 347, training loss: 0.4449 , running loss:0.44552031755447385 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 348, training loss: 0.3628 , running loss:0.44127386957407 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 349, training loss: 0.4168 , running loss:0.4362512335181236 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 350, training loss: 0.4050 , running loss:0.4325451597571373 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 351, training loss: 0.4418 , running loss:0.43000990003347395 (0.825, 0.3, 0.125)\n",
      "Epoch : 0 ,Iteration : 352, training loss: 0.4980 , running loss:0.43257263153791425 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 353, training loss: 0.4062 , running loss:0.4275191068649292 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 354, training loss: 0.5389 , running loss:0.43145641684532166 (0.775, 0.3, 0.125)\n",
      "Epoch : 0 ,Iteration : 355, training loss: 0.4826 , running loss:0.4333407551050186 (0.775, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 356, training loss: 0.4391 , running loss:0.4351381719112396 (0.825, 0.5, 0.125)\n",
      "Epoch : 0 ,Iteration : 357, training loss: 0.3786 , running loss:0.43284798562526705 (0.825, 0.5, 0.25)\n",
      "Epoch : 0 ,Iteration : 358, training loss: 0.4602 , running loss:0.4345876306295395 (0.85, 0.36, 0.25)\n",
      "Epoch : 0 ,Iteration : 359, training loss: 0.4387 , running loss:0.4350252911448479 (0.775, 0.26666666666666666, 0.125)\n",
      "Epoch : 0 ,Iteration : 360, training loss: 0.4205 , running loss:0.4332719728350639 (0.775, 0.2, 0.0)\n",
      "Validation loss : 0.3572551388719564   ,acc :  0.0  ,f1-micro :  0.7999266862170139  ,f1-macro :  0.4926686217008805\n",
      "Epoch : 0 ,Iteration : 361, training loss: 0.3959 , running loss:0.43083915412425994 (0.825, 0.48, 0.125)\n",
      "Epoch : 0 ,Iteration : 362, training loss: 0.4203 , running loss:0.4310547932982445 (0.825, 0.5, 0.125)\n",
      "Epoch : 0 ,Iteration : 363, training loss: 0.4117 , running loss:0.4302853375673294 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 364, training loss: 0.4439 , running loss:0.432477168738842 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 365, training loss: 0.3965 , running loss:0.43323819488286974 (0.825, 0.5142857142857143, 0.25)\n",
      "Epoch : 0 ,Iteration : 366, training loss: 0.4476 , running loss:0.43250181525945663 (0.825, 0.3333333333333333, 0.125)\n",
      "Epoch : 0 ,Iteration : 367, training loss: 0.4042 , running loss:0.4304685056209564 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 368, training loss: 0.4615 , running loss:0.43540374785661695 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 369, training loss: 0.4512 , running loss:0.4371282383799553 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 370, training loss: 0.4355 , running loss:0.4386508911848068 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 371, training loss: 0.4242 , running loss:0.43777344822883607 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 372, training loss: 0.4848 , running loss:0.4371146231889725 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 373, training loss: 0.4331 , running loss:0.4384626686573029 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 374, training loss: 0.4166 , running loss:0.43234567940235136 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 375, training loss: 0.4906 , running loss:0.4327462732791901 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 376, training loss: 0.4390 , running loss:0.4327407866716385 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 377, training loss: 0.4225 , running loss:0.43493258506059645 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 378, training loss: 0.3740 , running loss:0.43062274605035783 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 379, training loss: 0.4715 , running loss:0.4322601452469826 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 380, training loss: 0.3612 , running loss:0.429298435151577 (0.8000000000000002, 0.6, 0.0)\n",
      "Validation loss : 0.3838841356664809   ,acc :  0.0  ,f1-micro :  0.800000000000005  ,f1-macro :  0.49208211143695085\n",
      "Epoch : 0 ,Iteration : 381, training loss: 0.4298 , running loss:0.43099169582128527 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 382, training loss: 0.3881 , running loss:0.4293791875243187 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 383, training loss: 0.4332 , running loss:0.43045376390218737 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 384, training loss: 0.4997 , running loss:0.4332421913743019 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 385, training loss: 0.4094 , running loss:0.433884933590889 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 386, training loss: 0.4206 , running loss:0.43253494054079056 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 387, training loss: 0.3438 , running loss:0.42951585054397584 (0.8000000000000002, 0.6, 0.0)\n",
      "Epoch : 0 ,Iteration : 388, training loss: 0.4014 , running loss:0.4265088975429535 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 389, training loss: 0.4708 , running loss:0.42748694568872453 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 390, training loss: 0.4227 , running loss:0.42684999853372574 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 391, training loss: 0.4258 , running loss:0.4269316360354424 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 392, training loss: 0.4020 , running loss:0.42278929948806765 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 393, training loss: 0.4466 , running loss:0.4234617233276367 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 394, training loss: 0.3832 , running loss:0.4217931509017944 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 395, training loss: 0.3866 , running loss:0.41659035682678225 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 396, training loss: 0.4092 , running loss:0.41509692966938017 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 397, training loss: 0.4694 , running loss:0.41744494885206224 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 398, training loss: 0.4443 , running loss:0.4209602579474449 (0.775, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 399, training loss: 0.3926 , running loss:0.4170156031847 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 400, training loss: 0.4782 , running loss:0.4228648439049721 (0.8000000000000002, 0.2, 0.0)\n",
      "Validation loss : 0.37131662644948427   ,acc :  0.0  ,f1-micro :  0.800000000000005  ,f1-macro :  0.49677419354838737\n",
      "Epoch : 0 ,Iteration : 401, training loss: 0.4618 , running loss:0.4244668498635292 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 402, training loss: 0.4611 , running loss:0.42812038362026217 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 403, training loss: 0.4971 , running loss:0.43131916522979735 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 404, training loss: 0.4160 , running loss:0.42713567018508913 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 405, training loss: 0.4956 , running loss:0.43144679069519043 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 406, training loss: 0.3825 , running loss:0.4295454815030098 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 407, training loss: 0.4147 , running loss:0.43308990597724917 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 408, training loss: 0.4239 , running loss:0.434214586019516 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 409, training loss: 0.3633 , running loss:0.42884159088134766 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 410, training loss: 0.3850 , running loss:0.4269541516900063 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 411, training loss: 0.3423 , running loss:0.42277445942163466 (0.8000000000000002, 0.6, 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 412, training loss: 0.4401 , running loss:0.42468208223581316 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 413, training loss: 0.4326 , running loss:0.42398322224617 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 414, training loss: 0.5026 , running loss:0.42995211482048035 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 415, training loss: 0.3681 , running loss:0.42902680188417436 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 416, training loss: 0.4159 , running loss:0.4293654054403305 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 417, training loss: 0.4623 , running loss:0.42900712937116625 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 418, training loss: 0.4428 , running loss:0.42893318086862564 (0.8000000000000002, 0.4, 0.0)\n",
      "Epoch : 0 ,Iteration : 419, training loss: 0.4510 , running loss:0.43185120075941086 (0.8000000000000002, 0.2, 0.0)\n",
      "Epoch : 0 ,Iteration : 420, training loss: 0.3536 , running loss:0.4256187230348587 (0.8000000000000002, 0.6, 0.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_73975/1974055776.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mbase_model1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mbase_model1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_73975/1974055776.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(base_model, train_dataloader, val_dataloader, optimizer, loss_function)\u001b[0m\n\u001b[1;32m     71\u001b[0m                         \u001b[0;31m# emptying memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                         \u001b[0;32mdel\u001b[0m \u001b[0mval_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-5\n",
    "base_model1 = BERTClassifier(num_sector)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(base_model1.parameters(), lr=learning_rate)\n",
    "\n",
    "def train(base_model,train_dataloader,val_dataloader,optimizer,loss_function):\n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    valid_interval = 20  # Perform validation and save model every 10 iterations\n",
    "    iteration = 0\n",
    "    max_f1 = 100000000\n",
    "    stop_criterion = 2000000\n",
    "\n",
    "    running_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        base_model.train()  # Set the model to training mode\n",
    "        for curr_batch in train_dataloader:\n",
    "            \n",
    "            if iteration > stop_criterion:\n",
    "                break\n",
    "            \n",
    "            input_ids = curr_batch['input_ids'].to(device)\n",
    "            attention_mask = curr_batch['attention_mask'].to(device)\n",
    "            targets = curr_batch['multi_sector'].to(device)\n",
    "\n",
    "\n",
    "\n",
    "            outputs = base_model(input_ids,attention_mask)\n",
    "            loss = loss_function(outputs.to(device), targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #print (iteration)\n",
    "            running_loss.append(loss.item())\n",
    "            if len(running_loss) > 20:\n",
    "                running_loss.pop(0)\n",
    "            print (f\"Epoch : {epoch} ,Iteration : {iteration}, training loss: {loss:.4f} , running loss:{sum(running_loss)/len(running_loss)}\",find_metrics(targets,outputs))\n",
    "            \n",
    "            # freeing up excess memory\n",
    "            del loss, outputs\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \n",
    "            # Validation and model saving\n",
    "            if iteration % valid_interval == 0:\n",
    "                base_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    total_loss = []\n",
    "                    f1_micro = []\n",
    "                    f1_macro = []\n",
    "                    f1_avg = []\n",
    "                    for val_batch in val_dataloader:\n",
    "                        val_input_ids = val_batch['input_ids'].to(device)\n",
    "                        val_attention_mask = val_batch['attention_mask'].to(device)\n",
    "                        val_targets = val_batch['multi_sector'].to(device)\n",
    "\n",
    "\n",
    "                        outputs = base_model(val_input_ids,val_attention_mask)\n",
    "                        loss = loss_function(outputs.to(device), val_targets)\n",
    "                        \n",
    "                        total_loss.append(loss.item())\n",
    "                        val_out = find_metrics(val_targets,outputs)\n",
    "                        f1_micro.append(val_out[0])\n",
    "                        f1_macro.append(val_out[1])\n",
    "                        f1_avg.append(val_out[2])\n",
    "                        \n",
    "                        # emptying memory\n",
    "                        del val_out, loss, outputs\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                    avg_acc = sum(f1_avg)/len(f1_avg)\n",
    "                    avg_f1mic = sum(f1_micro)/len(f1_micro)\n",
    "                    avg_f1mac = sum(f1_macro)/len(f1_macro)\n",
    "                    avg_loss = sum(total_loss)/len(total_loss)\n",
    "                    \n",
    "                    print (f\"Validation loss : {sum(total_loss)/len(total_loss)} \", ' ,acc : ',avg_acc,\" ,f1-micro : \",avg_f1mic,\" ,f1-macro : \",avg_f1mac)\n",
    "                    if max_f1 >  (sum(total_loss)/len(total_loss)):\n",
    "                        max_f1 = (sum(total_loss)/len(total_loss))\n",
    "                        torch.save(base_model.state_dict(),f\"sector_dataset/sector/model_{iteration}.pth\")\n",
    "                    \n",
    "                    del total_loss, f1_micro, f1_macro, f1_avg\n",
    "\n",
    "                base_model.train()  # Set the model back to training mode\n",
    "            \n",
    "            iteration += 1\n",
    "    return base_model, optimizer, loss_function\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model1.to(device)\n",
    "base_model1, optimizer, loss_function = train(base_model1,train_dataloader,val_dataloader,optimizer,loss_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f8b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "408bcacf",
   "metadata": {},
   "source": [
    "### Entailment approach classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31670db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2def = {\n",
    "    0: \"this clause stipulates that the duty to pay damages is limited or excluded, for certain kind of losses, under certain conditions. \",\n",
    "    1 : \"this clause gives provider the right to suspend and/or terminate the service and/or the contract, and sometimes details the circumstances under which the provider claims to have a right to do so.\",\n",
    "    2 : \"this clause specifies the conditions under which the service provider could amend and modify the terms of service and/or the service itself.\",\n",
    "    3 : \"this clause gives the provider a right to modify/delete user’s content, including in-app purchases, and sometimes specifies the conditions under which the service provider may do so.\",\n",
    "    4 : \"this clause stipulates that the consumer is bound by the terms of use of a specific service, simply by using the service, without even being required to mark that he or she has read and accepted them.\",\n",
    "    5 : \"this clause specifies what law will govern the contract, meaning also what law will be applied in potential adjudication of a dispute arising under the contract.\",\n",
    "    6 : \"this selection clause requires or allows the parties to resolve their disputes through an arbitration process, before the case could go to court.\",\n",
    "    7 : \"this forum selection clause requires or allows the parties to resolve their disputes through an arbitration process, before the case could go to court however, such a clause may or may not specify that arbitration should occur within a specific jurisdiction. \",\n",
    "    8 : \"this clause stipulates that the scope of consent granted to the ToS incorporates also the privacy policy,\"\n",
    "}\n",
    "\n",
    "config = {\"labs\":{\"train\":[0,3,4,6],\"val\":[1,2],\"test\":[5,7,8]},\"sector\":{\"train\":[1,2,3],\"val\":[0,4],\"test\":[0,4]}}\n",
    "\n",
    "def overlap(list1,list2):\n",
    "    for l1 in list1:\n",
    "        if l1 in list2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def convert2ent(config,dir_sent,dir_labs,dir_sec,label_true):\n",
    "    train_ent, val_ent, test_ent = {\"text\":[],\"labels\":[],\"str_labels\":[]}, {\"text\":[],\"labels\":[],\"str_labels\":[]} , {\"text\":[],\"labels\":[],\"str_labels\":[]}\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    file_sent = os.listdir(dir_sent)        \n",
    "    file_labs = os.listdir(dir_labs)\n",
    "\n",
    "    dataset_new = make_data(dir_sent,dir_labs,dir_sec, file_sent)\n",
    "    \n",
    "    for i in range(0,len(dataset_new[\"text\"])):\n",
    "        \n",
    "        sent = dataset_new[\"text\"][i]\n",
    "        str_label = (dataset_new[\"labels\"][i]).split(\" \")\n",
    "        labels = []\n",
    "        for lb in str_label:\n",
    "            if len(lb) > 1:\n",
    "                labels.append(label2ind[lb][0])\n",
    "        sector = dataset_new[\"sector\"][i]\n",
    "        \n",
    "        inter = config[\"sector\"]\n",
    "        if file_sent:\n",
    "            inter = config[\"labs\"]\n",
    "\n",
    "\n",
    "        \n",
    "        # train\n",
    "        if overlap(labels,inter[\"train\"]):\n",
    "            if len(labels) == 0:\n",
    "                continue\n",
    "            doc = nlp(sent)\n",
    "            old_string = (\" \").join([\" \".join([token.text for token in sent if not token.is_punct]) for sent in doc.sents])[:-1]\n",
    "            for j in range(0,9):\n",
    "                new_string = old_string + \" \" + ent_con[0] + \" \" + ind2def[j]\n",
    "                train_ent[\"text\"].append(new_string)\n",
    "                train_ent[\"str_labels\"].append(str(dataset_new[\"labels\"][i]))\n",
    "                if j in labels:\n",
    "                    train_ent[\"labels\"].append([0,1])\n",
    "                else:\n",
    "                    train_ent[\"labels\"].append([1,0])\n",
    "\n",
    "        # Validation\n",
    "        elif overlap(labels,inter[\"val\"]):\n",
    "            doc = nlp(sent)\n",
    "            old_string = (\" \").join([\" \".join([token.text for token in sent if not token.is_punct]) for sent in doc.sents])[:-1]\n",
    "            for j in range(0,9):\n",
    "                new_string = old_string + \" \" + ent_con[0] + \" \" + ind2def[j]\n",
    "                val_ent[\"text\"].append(new_string)\n",
    "                val_ent[\"str_labels\"].append(str(dataset_new[\"labels\"][i]))\n",
    "                if j in labels:\n",
    "                    val_ent[\"labels\"].append([0,1])\n",
    "                else:\n",
    "                    val_ent[\"labels\"].append([1,0])\n",
    "\n",
    "        # Test\n",
    "        else:\n",
    "            doc = nlp(sent)\n",
    "            old_string = (\" \").join([\" \".join([token.text for token in sent if not token.is_punct]) for sent in doc.sents])[:-1]\n",
    "            for j in range(0,9):\n",
    "                new_string = old_string + \" \" + ent_con[0] + \" \" + ind2def[j]\n",
    "                test_ent[\"text\"].append(new_string)\n",
    "                test_ent[\"str_labels\"].append(str(dataset_new[\"labels\"][i]))\n",
    "                if j in labels:\n",
    "                    test_ent[\"labels\"].append([0,1])\n",
    "                else:\n",
    "                    test_ent[\"labels\"].append([1,0])\n",
    "\n",
    "    \n",
    "    return train_ent,val_ent,test_ent\n",
    "\n",
    "train_ent_dt,val_ent_dt,test_ent_dt = convert2ent(config,\"../datasets/corpus_142_ToS/corpus/sentences\",\"../datasets/corpus_142_ToS/corpus/tags\",\"../datasets/corpus_142_ToS/corpus/lists\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5ea0827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class CustomDatasetEnt(Dataset):\n",
    "    def __init__(self, dataset,num_classes,num_sector,tokenizer):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.texts = self.dataset[\"text\"]\n",
    "        self.labels = self.dataset[\"labels\"]\n",
    "        self.num_classes = num_classes\n",
    "        self.num_sector = num_sector\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=64,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "\n",
    "        multi_label = torch.tensor(label, dtype=torch.float32)\n",
    "        return {'input_ids':input_ids, 'attention_mask':attention_mask, 'multi_label':multi_label}\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_custom = CustomDatasetEnt(train_ent_dt, num_classes,num_sector,tokenizer)\n",
    "train_dataloader = DataLoader(train_custom, batch_size=batch_size, shuffle=True)\n",
    "valid_custom = CustomDatasetEnt(val_ent_dt, num_classes,num_sector,tokenizer)\n",
    "val_dataloader = DataLoader(valid_custom, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "learning_rate = 3e-5\n",
    "base_model_ent = BERTClassifier(2)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(base_model_ent.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "335b6142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 0, training loss: 0.6229 , running loss:0.6229053139686584 (0.625, 0.6095238095238096, 0.25)\n",
      "Validation loss : 0.5270379872347332   ,acc :  0.8820600100857287  ,f1-micro :  0.8820600100857287  ,f1-macro :  0.646574202398707\n",
      "Epoch : 0 ,Iteration : 1, training loss: 0.6142 , running loss:0.6185733079910278 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 2, training loss: 0.4165 , running loss:0.5512179732322693 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 3, training loss: 0.4956 , running loss:0.5373192802071571 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 4, training loss: 0.3848 , running loss:0.5068179726600647 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 5, training loss: 0.4562 , running loss:0.49838902552922565 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 6, training loss: 0.2813 , running loss:0.4673714169434139 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 7, training loss: 0.2246 , running loss:0.43702157586812973 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 8, training loss: 0.5797 , running loss:0.4528700047069126 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 9, training loss: 0.1989 , running loss:0.4274760216474533 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 10, training loss: 0.4119 , running loss:0.42605949260971765 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 11, training loss: 0.1629 , running loss:0.40412986651062965 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 12, training loss: 0.7824 , running loss:0.43322833570150227 (0.625, 0.38461538461538464, 0.625)\n",
      "Epoch : 0 ,Iteration : 13, training loss: 0.8311 , running loss:0.4616446058665003 (0.625, 0.38461538461538464, 0.625)\n",
      "Epoch : 0 ,Iteration : 14, training loss: 0.2502 , running loss:0.4475453048944473 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 15, training loss: 0.4237 , running loss:0.4460531147196889 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 16, training loss: 0.4000 , running loss:0.44334397333509784 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 17, training loss: 0.2182 , running loss:0.43083834151426953 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 18, training loss: 0.2042 , running loss:0.41890937403628703 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 19, training loss: 0.7680 , running loss:0.43636471331119536 (0.625, 0.38461538461538464, 0.625)\n",
      "Epoch : 0 ,Iteration : 20, training loss: 0.7853 , running loss:0.44448334276676177 (0.625, 0.38461538461538464, 0.625)\n",
      "Validation loss : 0.3716724586649129   ,acc :  0.882375189107413  ,f1-micro :  0.882375189107413  ,f1-macro :  0.6563108097147388\n",
      "Epoch : 0 ,Iteration : 21, training loss: 0.3665 , running loss:0.43209835439920424 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 22, training loss: 0.1874 , running loss:0.4206435188651085 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 23, training loss: 0.1923 , running loss:0.4054794855415821 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 24, training loss: 0.5693 , running loss:0.4147034965455532 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 25, training loss: 0.5989 , running loss:0.42183694764971735 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 26, training loss: 0.3788 , running loss:0.4267152898013592 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 27, training loss: 1.0020 , running loss:0.46558812633156776 (0.5, 0.3333333333333333, 0.5)\n",
      "Epoch : 0 ,Iteration : 28, training loss: 0.3740 , running loss:0.45530601814389227 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 29, training loss: 0.2051 , running loss:0.45561386570334433 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 30, training loss: 0.5903 , running loss:0.4645322643220425 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 31, training loss: 0.3955 , running loss:0.47616423964500426 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 32, training loss: 0.1953 , running loss:0.446810095757246 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 33, training loss: 0.3875 , running loss:0.424631492048502 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 34, training loss: 0.1848 , running loss:0.4213632047176361 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 35, training loss: 0.3662 , running loss:0.418491342663765 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 36, training loss: 0.5762 , running loss:0.4273033678531647 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 37, training loss: 0.1641 , running loss:0.42459497153759 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 38, training loss: 0.1560 , running loss:0.42218493968248366 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 39, training loss: 0.6312 , running loss:0.41534355133771894 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 40, training loss: 0.3691 , running loss:0.3945334255695343 (0.875, 0.4666666666666667, 0.875)\n",
      "Validation loss : 0.36400989647081144   ,acc :  0.8820600100857287  ,f1-micro :  0.8820600100857287  ,f1-macro :  0.645910015350253\n",
      "Epoch : 0 ,Iteration : 41, training loss: 0.1429 , running loss:0.38335040807723997 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 42, training loss: 0.3925 , running loss:0.39360403567552565 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 43, training loss: 0.3649 , running loss:0.40223135575652125 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 44, training loss: 0.3655 , running loss:0.3920406199991703 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 45, training loss: 0.6141 , running loss:0.39280125573277475 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 46, training loss: 0.3723 , running loss:0.3924727104604244 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 47, training loss: 0.5997 , running loss:0.3723574511706829 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 48, training loss: 0.1242 , running loss:0.3598670370876789 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 49, training loss: 0.6107 , running loss:0.38014995828270914 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 50, training loss: 0.3766 , running loss:0.36946770921349525 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 51, training loss: 0.1341 , running loss:0.3563948817551136 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 52, training loss: 0.3527 , running loss:0.36426215767860415 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 53, training loss: 0.1378 , running loss:0.35177860483527185 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 54, training loss: 0.1331 , running loss:0.3491926610469818 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 55, training loss: 0.6564 , running loss:0.36370244771242144 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 56, training loss: 0.1228 , running loss:0.3410303987562656 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 57, training loss: 0.3740 , running loss:0.35152872651815414 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 58, training loss: 0.9121 , running loss:0.3893353834748268 (0.625, 0.38461538461538464, 0.625)\n",
      "Epoch : 0 ,Iteration : 59, training loss: 0.1180 , running loss:0.36367703787982464 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 60, training loss: 0.1217 , running loss:0.35131028294563293 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.3637341190462033   ,acc :  0.882375189107413  ,f1-micro :  0.882375189107413  ,f1-macro :  0.6512820512820474\n",
      "Epoch : 0 ,Iteration : 61, training loss: 0.3957 , running loss:0.3639488711953163 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 62, training loss: 0.1103 , running loss:0.34983884543180466 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 63, training loss: 0.3958 , running loss:0.3513865903019905 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 64, training loss: 0.3872 , running loss:0.35247149169445036 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 65, training loss: 0.3658 , running loss:0.3400559604167938 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 66, training loss: 0.3706 , running loss:0.33997122049331663 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 67, training loss: 0.1004 , running loss:0.31500618532299995 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 68, training loss: 0.3978 , running loss:0.32868353351950647 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 69, training loss: 0.6759 , running loss:0.33194092735648156 (0.75, 0.42857142857142855, 0.75)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 70, training loss: 0.3808 , running loss:0.33215186968445776 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 71, training loss: 0.3644 , running loss:0.3436671905219555 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 72, training loss: 0.6654 , running loss:0.35930528715252874 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 73, training loss: 0.6465 , running loss:0.3847395330667496 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 74, training loss: 0.3703 , running loss:0.39660064280033114 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 75, training loss: 0.3769 , running loss:0.38262468874454497 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 76, training loss: 0.5802 , running loss:0.4054955460131168 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 77, training loss: 0.5970 , running loss:0.4166438959538937 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 78, training loss: 0.1824 , running loss:0.3801595076918602 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 79, training loss: 0.4012 , running loss:0.3943185079842806 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 80, training loss: 0.2078 , running loss:0.3986214481294155 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.3731296636198003   ,acc :  0.882375189107413  ,f1-micro :  0.882375189107413  ,f1-macro :  0.6416078122583381\n",
      "Epoch : 0 ,Iteration : 81, training loss: 0.5784 , running loss:0.407760538905859 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 82, training loss: 0.3886 , running loss:0.42167876586318015 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 83, training loss: 0.6102 , running loss:0.4323943056166172 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 84, training loss: 0.5763 , running loss:0.44185260906815527 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 85, training loss: 0.3989 , running loss:0.4435055278241634 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 86, training loss: 0.3826 , running loss:0.44410520419478416 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 87, training loss: 0.1816 , running loss:0.4481619939208031 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 88, training loss: 0.5761 , running loss:0.45708084255456927 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 89, training loss: 0.7991 , running loss:0.4632421866059303 (0.625, 0.38461538461538464, 0.625)\n",
      "Epoch : 0 ,Iteration : 90, training loss: 0.6055 , running loss:0.47447581887245177 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 91, training loss: 0.1959 , running loss:0.46605191826820375 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 92, training loss: 0.1984 , running loss:0.4427016466856003 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 93, training loss: 0.5929 , running loss:0.4400235921144485 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 94, training loss: 0.1785 , running loss:0.43043320178985595 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 95, training loss: 0.3620 , running loss:0.4296894982457161 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 96, training loss: 0.3784 , running loss:0.41959664672613145 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 97, training loss: 0.5961 , running loss:0.4195494994521141 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 98, training loss: 0.3743 , running loss:0.42914077267050743 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 99, training loss: 0.3621 , running loss:0.4271836929023266 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 100, training loss: 0.6202 , running loss:0.44780615121126177 (0.75, 0.42857142857142855, 0.75)\n",
      "Validation loss : 0.36547682971979595   ,acc :  0.882375189107413  ,f1-micro :  0.882375189107413  ,f1-macro :  0.6445293486140647\n",
      "Epoch : 0 ,Iteration : 101, training loss: 0.5978 , running loss:0.4487763151526451 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 102, training loss: 0.5887 , running loss:0.45877752304077146 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 103, training loss: 0.1806 , running loss:0.4373008869588375 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 104, training loss: 0.3688 , running loss:0.4269246034324169 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 105, training loss: 0.3748 , running loss:0.42572233751416205 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 106, training loss: 0.5683 , running loss:0.43501088991761205 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 107, training loss: 0.1946 , running loss:0.435663553327322 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 108, training loss: 0.7610 , running loss:0.4449055515229702 (0.625, 0.38461538461538464, 0.625)\n",
      "Epoch : 0 ,Iteration : 109, training loss: 0.3942 , running loss:0.42466053292155265 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 110, training loss: 0.1963 , running loss:0.404199942946434 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 111, training loss: 0.3822 , running loss:0.4135146290063858 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 112, training loss: 0.3723 , running loss:0.42220832854509355 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 113, training loss: 0.1651 , running loss:0.4008161187171936 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 114, training loss: 0.8432 , running loss:0.4340497702360153 (0.625, 0.38461538461538464, 0.625)\n",
      "Epoch : 0 ,Iteration : 115, training loss: 0.1420 , running loss:0.4230487734079361 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 116, training loss: 0.3843 , running loss:0.4233452945947647 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 117, training loss: 0.1384 , running loss:0.4004598021507263 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 118, training loss: 0.3707 , running loss:0.40028262734413145 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 119, training loss: 0.3781 , running loss:0.4010839074850082 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 120, training loss: 1.1037 , running loss:0.425255811214447 (0.5, 0.3333333333333333, 0.5)\n",
      "Validation loss : 0.3624732708741605   ,acc :  0.882375189107413  ,f1-micro :  0.882375189107413  ,f1-macro :  0.6497340279488505\n",
      "Epoch : 0 ,Iteration : 121, training loss: 0.6150 , running loss:0.4261163055896759 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 122, training loss: 0.1397 , running loss:0.4036702126264572 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 123, training loss: 0.1420 , running loss:0.4017398916184902 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 124, training loss: 0.6321 , running loss:0.4149053178727627 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 125, training loss: 0.3840 , running loss:0.41536463126540185 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 126, training loss: 0.6094 , running loss:0.41741975769400597 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 127, training loss: 0.1480 , running loss:0.4150896415114403 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 128, training loss: 0.5911 , running loss:0.40659763664007187 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 129, training loss: 0.1571 , running loss:0.3947411961853504 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 130, training loss: 0.6094 , running loss:0.4153971642255783 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 131, training loss: 0.1547 , running loss:0.40402122512459754 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 132, training loss: 0.3815 , running loss:0.4044821225106716 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 133, training loss: 0.3977 , running loss:0.4161103881895542 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 134, training loss: 0.1562 , running loss:0.38176127076148986 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 135, training loss: 0.8313 , running loss:0.4162265583872795 (0.625, 0.38461538461538464, 0.625)\n",
      "Epoch : 0 ,Iteration : 136, training loss: 0.6021 , running loss:0.42711790055036547 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 137, training loss: 0.3924 , running loss:0.439821669459343 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 138, training loss: 0.1544 , running loss:0.42900545597076417 (1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 139, training loss: 0.3817 , running loss:0.429187273979187 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 140, training loss: 0.3655 , running loss:0.3922786325216293 (0.875, 0.4666666666666667, 0.875)\n",
      "Validation loss : 0.36355444586042557   ,acc :  0.8820600100857287  ,f1-micro :  0.8820600100857287  ,f1-macro :  0.6519149030495438\n",
      "Epoch : 0 ,Iteration : 141, training loss: 0.1557 , running loss:0.3693088531494141 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 142, training loss: 0.1432 , running loss:0.3694804050028324 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 143, training loss: 0.3811 , running loss:0.38143570348620415 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 144, training loss: 0.1298 , running loss:0.35632171034812926 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 145, training loss: 0.6576 , running loss:0.3699995994567871 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 146, training loss: 0.6591 , running loss:0.372483029961586 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 147, training loss: 0.1256 , running loss:0.3713610641658306 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 148, training loss: 0.6040 , running loss:0.37200419828295705 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 149, training loss: 0.1274 , running loss:0.3705173745751381 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 150, training loss: 0.1174 , running loss:0.34591488242149354 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 151, training loss: 0.3673 , running loss:0.35654696449637413 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 152, training loss: 0.6351 , running loss:0.36922349259257314 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 153, training loss: 0.6060 , running loss:0.3796419702470303 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 154, training loss: 0.1208 , running loss:0.37787081450223925 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 155, training loss: 0.1228 , running loss:0.3424455523490906 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 156, training loss: 0.3739 , running loss:0.3310357451438904 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 157, training loss: 0.3820 , running loss:0.33051513135433197 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 158, training loss: 0.3772 , running loss:0.341657917201519 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 159, training loss: 0.3808 , running loss:0.34161199182271956 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 160, training loss: 0.1230 , running loss:0.32948520630598066 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.36592468565242553   ,acc :  0.882375189107413  ,f1-micro :  0.882375189107413  ,f1-macro :  0.6489944750156512\n",
      "Epoch : 0 ,Iteration : 161, training loss: 0.3666 , running loss:0.3400307223200798 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 162, training loss: 0.1164 , running loss:0.3386930860579014 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 163, training loss: 0.3939 , running loss:0.3393310822546482 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 164, training loss: 0.1178 , running loss:0.3387286067008972 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 165, training loss: 0.1111 , running loss:0.31140279322862624 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 166, training loss: 0.3747 , running loss:0.2971822887659073 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 167, training loss: 0.6476 , running loss:0.32328508496284486 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 168, training loss: 0.3782 , running loss:0.311997777223587 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 169, training loss: 0.9545 , running loss:0.35335419327020645 (0.625, 0.38461538461538464, 0.625)\n",
      "Epoch : 0 ,Iteration : 170, training loss: 0.1046 , running loss:0.3527147315442562 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 171, training loss: 1.1849 , running loss:0.39359175488352777 (0.5, 0.3333333333333333, 0.5)\n",
      "Epoch : 0 ,Iteration : 172, training loss: 0.3664 , running loss:0.3801579885184765 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 173, training loss: 0.4023 , running loss:0.3699738927185535 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 174, training loss: 0.1350 , running loss:0.3706852003931999 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 175, training loss: 0.3688 , running loss:0.3829865396022797 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 176, training loss: 0.3968 , running loss:0.3841303825378418 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 177, training loss: 0.1543 , running loss:0.37274626195430755 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 178, training loss: 0.3697 , running loss:0.37236813753843306 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 179, training loss: 0.6340 , running loss:0.385025592148304 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 180, training loss: 0.1553 , running loss:0.3866440564393997 (1.0, 1.0, 1.0)\n",
      "Validation loss : 0.36605684828920554   ,acc :  0.8820600100857287  ,f1-micro :  0.8820600100857287  ,f1-macro :  0.6541758795919118\n",
      "Epoch : 0 ,Iteration : 181, training loss: 0.6087 , running loss:0.39875284731388094 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 182, training loss: 0.1484 , running loss:0.4003516063094139 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 183, training loss: 0.5690 , running loss:0.4091085985302925 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 184, training loss: 0.1451 , running loss:0.41047286093235014 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 185, training loss: 0.1464 , running loss:0.4122380629181862 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 186, training loss: 0.3791 , running loss:0.41245699524879453 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 187, training loss: 0.5981 , running loss:0.4099819451570511 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 188, training loss: 0.3799 , running loss:0.41006356924772264 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 189, training loss: 0.6191 , running loss:0.3932934060692787 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 190, training loss: 0.6423 , running loss:0.42017916664481164 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 191, training loss: 0.6126 , running loss:0.39156359657645223 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 192, training loss: 0.5733 , running loss:0.40191120728850366 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 193, training loss: 0.5842 , running loss:0.4110059715807438 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 194, training loss: 0.3672 , running loss:0.4226162299513817 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 195, training loss: 0.5844 , running loss:0.43339624255895615 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 196, training loss: 0.3838 , running loss:0.43274780064821244 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 197, training loss: 0.7545 , running loss:0.4627582967281342 (0.625, 0.38461538461538464, 0.625)\n",
      "Epoch : 0 ,Iteration : 198, training loss: 0.4131 , running loss:0.46493096351623536 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 199, training loss: 0.5411 , running loss:0.46028695106506345 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 200, training loss: 0.5582 , running loss:0.48043124228715894 (0.75, 0.42857142857142855, 0.75)\n",
      "Validation loss : 0.4350676073519437   ,acc :  0.882375189107413  ,f1-micro :  0.882375189107413  ,f1-macro :  0.6527605526092628\n",
      "Epoch : 0 ,Iteration : 201, training loss: 0.5254 , running loss:0.4762631461024284 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 202, training loss: 0.3064 , running loss:0.48416317999362946 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 203, training loss: 0.2906 , running loss:0.4702430933713913 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 204, training loss: 0.5350 , running loss:0.4897406563162804 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 205, training loss: 0.2307 , running loss:0.49395678341388705 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 206, training loss: 0.1911 , running loss:0.48455822840332985 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 207, training loss: 0.5622 , running loss:0.482760176807642 (0.75, 0.42857142857142855, 0.75)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 208, training loss: 0.1674 , running loss:0.4721354991197586 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 209, training loss: 0.3740 , running loss:0.4598839521408081 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 210, training loss: 0.1234 , running loss:0.43393935933709143 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 211, training loss: 0.6020 , running loss:0.4334122337400913 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 212, training loss: 0.3612 , running loss:0.42280619516968726 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 213, training loss: 0.3861 , running loss:0.4128973789513111 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 214, training loss: 0.3784 , running loss:0.4134572960436344 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 215, training loss: 0.3783 , running loss:0.40314824804663657 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 216, training loss: 0.1054 , running loss:0.38922516219317915 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 217, training loss: 0.3935 , running loss:0.371170898899436 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 218, training loss: 0.3867 , running loss:0.36984715722501277 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 219, training loss: 0.3955 , running loss:0.3625691797584295 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 220, training loss: 0.6560 , running loss:0.3674565847963095 (0.75, 0.42857142857142855, 0.75)\n",
      "Validation loss : 0.36605802389163294   ,acc :  0.8820600100857287  ,f1-micro :  0.8820600100857287  ,f1-macro :  0.6599657528553102\n",
      "Epoch : 0 ,Iteration : 221, training loss: 0.1069 , running loss:0.34653161056339743 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 222, training loss: 0.4066 , running loss:0.3515424568206072 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 223, training loss: 0.8988 , running loss:0.3819539088755846 (0.625, 0.38461538461538464, 0.625)\n",
      "Epoch : 0 ,Iteration : 224, training loss: 0.3857 , running loss:0.3744893927127123 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 225, training loss: 0.1338 , running loss:0.3696468774229288 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 226, training loss: 0.6036 , running loss:0.39027078412473204 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 227, training loss: 0.3690 , running loss:0.3806137632578611 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 228, training loss: 0.8615 , running loss:0.41531833447515965 (0.625, 0.38461538461538464, 0.625)\n",
      "Epoch : 0 ,Iteration : 229, training loss: 0.4136 , running loss:0.41729822643101216 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 230, training loss: 0.3697 , running loss:0.4296125050634146 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 231, training loss: 0.1988 , running loss:0.40945326425135137 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 232, training loss: 0.2008 , running loss:0.4014314983040094 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 233, training loss: 0.1910 , running loss:0.3916761253029108 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 234, training loss: 0.1688 , running loss:0.38119634129107 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 235, training loss: 0.1468 , running loss:0.3696235630661249 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 236, training loss: 0.3801 , running loss:0.38336149603128433 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 237, training loss: 0.6245 , running loss:0.3949110895395279 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 238, training loss: 0.6308 , running loss:0.4071164458990097 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 239, training loss: 0.3898 , running loss:0.4068320393562317 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 240, training loss: 0.8973 , running loss:0.4188975363969803 (0.625, 0.38461538461538464, 0.625)\n",
      "Validation loss : 0.3644670892884439   ,acc :  0.8820600100857287  ,f1-micro :  0.8820600100857287  ,f1-macro :  0.645125323491431\n",
      "Epoch : 0 ,Iteration : 241, training loss: 0.8910 , running loss:0.4581053353846073 (0.625, 0.38461538461538464, 0.625)\n",
      "Epoch : 0 ,Iteration : 242, training loss: 0.1281 , running loss:0.4441773444414139 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 243, training loss: 0.3803 , running loss:0.41824886947870255 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 244, training loss: 0.6126 , running loss:0.4295907184481621 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 245, training loss: 0.3694 , running loss:0.4413709074258804 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 246, training loss: 0.3889 , running loss:0.4306385263800621 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 247, training loss: 0.1645 , running loss:0.42041048109531404 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 248, training loss: 0.1729 , running loss:0.38598395586013795 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 249, training loss: 0.5900 , running loss:0.39480382949113846 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 250, training loss: 0.6054 , running loss:0.40659033954143525 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 251, training loss: 0.3745 , running loss:0.4153717465698719 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 252, training loss: 0.3856 , running loss:0.42461496517062186 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 253, training loss: 0.1684 , running loss:0.4234852783381939 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 254, training loss: 0.3908 , running loss:0.4345837377011776 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 255, training loss: 0.5834 , running loss:0.45641367211937905 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 256, training loss: 0.3787 , running loss:0.4563413970172405 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 257, training loss: 0.3680 , running loss:0.44352066293358805 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 258, training loss: 0.3699 , running loss:0.43047894909977913 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 259, training loss: 0.1515 , running loss:0.41856196746230123 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 260, training loss: 0.3861 , running loss:0.39300227388739584 (0.875, 0.4666666666666667, 0.875)\n",
      "Validation loss : 0.3652105907629189   ,acc :  0.882375189107413  ,f1-micro :  0.882375189107413  ,f1-macro :  0.6526015588042777\n",
      "Epoch : 0 ,Iteration : 261, training loss: 0.6041 , running loss:0.3786547236144543 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 262, training loss: 0.3762 , running loss:0.39106172919273374 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 263, training loss: 0.3939 , running loss:0.3917434498667717 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 264, training loss: 0.1433 , running loss:0.36827712431550025 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 265, training loss: 0.1335 , running loss:0.3564824916422367 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 266, training loss: 0.1338 , running loss:0.3437235429883003 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 267, training loss: 0.3937 , running loss:0.3551851138472557 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 268, training loss: 0.3762 , running loss:0.36534814834594725 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 269, training loss: 0.3769 , running loss:0.3546889781951904 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 270, training loss: 0.3826 , running loss:0.3435487329959869 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 271, training loss: 0.1195 , running loss:0.33079916834831236 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 272, training loss: 0.3904 , running loss:0.3310368418693542 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 273, training loss: 0.1010 , running loss:0.32767039760947225 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 274, training loss: 0.3767 , running loss:0.3269700311124325 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 275, training loss: 0.1008 , running loss:0.3028400555253029 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 276, training loss: 0.1003 , running loss:0.2889215301722288 (1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 ,Iteration : 277, training loss: 0.6622 , running loss:0.30363073013722897 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 278, training loss: 0.3913 , running loss:0.30469867698848246 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 279, training loss: 0.6660 , running loss:0.33042409978806975 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 280, training loss: 0.3865 , running loss:0.33044647090137 (0.875, 0.4666666666666667, 0.875)\n",
      "Validation loss : 0.37034143145659687   ,acc :  0.882375189107413  ,f1-micro :  0.882375189107413  ,f1-macro :  0.6541925044194296\n",
      "Epoch : 0 ,Iteration : 281, training loss: 0.0939 , running loss:0.30493923015892505 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 282, training loss: 0.3920 , running loss:0.3057286236435175 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 283, training loss: 0.0942 , running loss:0.2907435491681099 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 284, training loss: 0.6509 , running loss:0.31612608954310417 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 285, training loss: 0.0950 , running loss:0.31420017071068285 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 286, training loss: 0.3600 , running loss:0.32551105730235574 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 287, training loss: 0.3986 , running loss:0.3257576864212751 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 288, training loss: 0.6773 , running loss:0.34081115387380123 (0.75, 0.42857142857142855, 0.75)\n",
      "Epoch : 0 ,Iteration : 289, training loss: 0.3806 , running loss:0.34099939130246637 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 290, training loss: 0.3959 , running loss:0.34166499339044093 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 291, training loss: 0.1080 , running loss:0.3410901688039303 (1.0, 1.0, 1.0)\n",
      "Epoch : 0 ,Iteration : 292, training loss: 0.3905 , running loss:0.34109723940491676 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 293, training loss: 0.3727 , running loss:0.35468280911445615 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 294, training loss: 0.3832 , running loss:0.35500397384166715 (0.875, 0.4666666666666667, 0.875)\n",
      "Epoch : 0 ,Iteration : 295, training loss: 0.1032 , running loss:0.35512460693717 (1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_73975/3572055129.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mbase_model_ent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mbase_model_ent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_ent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_73975/3572055129.py\u001b[0m in \u001b[0;36mtrain_ent\u001b[0;34m(base_model, train_dataloader, val_dataloader, optimizer, loss_function)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/legal_env/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/legal_env/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_ent(base_model,train_dataloader,val_dataloader,optimizer,loss_function):\n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    valid_interval = 20  # Perform validation and save model every 10 iterations\n",
    "    iteration = 0\n",
    "    max_f1 = 100000000\n",
    "    stop_criterion = 2000000\n",
    "\n",
    "    running_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        base_model.train()  # Set the model to training mode\n",
    "        for curr_batch in train_dataloader:\n",
    "            \n",
    "            if iteration > stop_criterion:\n",
    "                break\n",
    "            \n",
    "            input_ids = curr_batch['input_ids'].to(device)\n",
    "            attention_mask = curr_batch['attention_mask'].to(device)\n",
    "            targets = curr_batch['multi_label'].to(device)\n",
    "\n",
    "\n",
    "\n",
    "            outputs = base_model(input_ids,attention_mask)\n",
    "            loss = loss_function(outputs.to(device), targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #print (iteration)\n",
    "            running_loss.append(loss.item())\n",
    "            if len(running_loss) > 20:\n",
    "                running_loss.pop(0)\n",
    "            print (f\"Epoch : {epoch} ,Iteration : {iteration}, training loss: {loss:.4f} , running loss:{sum(running_loss)/len(running_loss)}\",find_metrics(targets,outputs))\n",
    "            \n",
    "            # freeing up excess memory\n",
    "            del loss, outputs\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            \n",
    "            # Validation and model saving\n",
    "            if iteration % valid_interval == 0:\n",
    "                base_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    total_loss = []\n",
    "                    f1_micro = []\n",
    "                    f1_macro = []\n",
    "                    f1_avg = []\n",
    "                    for val_batch in val_dataloader:\n",
    "                        val_input_ids = val_batch['input_ids'].to(device)\n",
    "                        val_attention_mask = val_batch['attention_mask'].to(device)\n",
    "                        val_targets = val_batch['multi_label'].to(device)\n",
    "\n",
    "\n",
    "                        outputs = base_model(val_input_ids,val_attention_mask)\n",
    "                        loss = loss_function(outputs.to(device), val_targets)\n",
    "                        \n",
    "                        total_loss.append(loss.item())\n",
    "                        val_out = find_metrics(val_targets,outputs)\n",
    "                        f1_micro.append(val_out[0])\n",
    "                        f1_macro.append(val_out[1])\n",
    "                        f1_avg.append(val_out[2])\n",
    "                        \n",
    "                        # emptying memory\n",
    "                        del val_out, loss, outputs\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                    avg_acc = sum(f1_avg)/len(f1_avg)\n",
    "                    avg_f1mic = sum(f1_micro)/len(f1_micro)\n",
    "                    avg_f1mac = sum(f1_macro)/len(f1_macro)\n",
    "                    avg_loss = sum(total_loss)/len(total_loss)\n",
    "                    \n",
    "                    print (f\"Validation loss : {sum(total_loss)/len(total_loss)} \", ' ,acc : ',avg_acc,\" ,f1-micro : \",avg_f1mic,\" ,f1-macro : \",avg_f1mac)\n",
    "                    if max_f1 >  (sum(total_loss)/len(total_loss)):\n",
    "                        max_f1 = (sum(total_loss)/len(total_loss))\n",
    "                        torch.save(base_model.state_dict(),f\"model_ent_2/nine_c_ent/model_{iteration}.pth\")\n",
    "                    \n",
    "                    del total_loss, f1_micro, f1_macro, f1_avg\n",
    "\n",
    "                base_model.train()  # Set the model back to training mode\n",
    "            \n",
    "            iteration += 1\n",
    "    return base_model, optimizer, loss_function\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model_ent.to(device)\n",
    "base_model_ent, optimizer, loss_function = train_ent(base_model_ent,train_dataloader,val_dataloader,optimizer,loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f781899",
   "metadata": {},
   "source": [
    "## Sector based entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66108085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49623ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
